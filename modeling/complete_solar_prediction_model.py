import torch
import torch.nn as nn
import torch.nn.functional as F
import math
from typing import Optional, Dict, Any, Tuple


class PaddingAwareEncoder(nn.Module):
    """
    íŒ¨ë”©ì„ ì¸ì‹í•˜ëŠ” Historical Encoder
    ì‹œê°„ ìˆœì„œë¥¼ ë³´ì¡´í•˜ë©´ì„œ ëª¨ë“  54ê°œ features ì²˜ë¦¬
    """
    def __init__(self, input_dim: int = 54, d_model: int = 256, nhead: int = 8, num_layers: int = 4):
        super().__init__()
        
        self.d_model = d_model
        
        # Input projection
        self.input_projection = nn.Linear(input_dim, d_model)
        
        # Positional encoding
        self.pos_encoding = PositionalEncoding(d_model, max_len=48)
        
        # Transformer encoder layers
        encoder_layer = nn.TransformerEncoderLayer(
            d_model=d_model,
            nhead=nhead,
            dim_feedforward=d_model * 4,
            dropout=0.1,
            activation='gelu',
            batch_first=True
        )
        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)
        
        # Layer normalization
        self.layer_norm = nn.LayerNorm(d_model)
        
    def forward(self, x: torch.Tensor, padding_mask: Optional[torch.Tensor] = None) -> torch.Tensor:
        """
        Args:
            x: [batch, seq_len, 54] - historical data
            padding_mask: [batch, seq_len] - True for padded positions
            
        Returns:
            encoded: [batch, seq_len, d_model] - ì‹œê°„ ì •ë³´ ë³´ì¡´í•˜ë©° ì¸ì½”ë”©
        """
        # Input projection
        x = self.input_projection(x)  # [batch, seq_len, d_model]
        
        # Scale by sqrt(d_model) as in original transformer
        x = x * math.sqrt(self.d_model)
        
        # Add positional encoding
        x = self.pos_encoding(x)
        
        # Transformer encoding with padding mask
        encoded = self.transformer(x, src_key_padding_mask=padding_mask)
        
        # Layer normalization
        encoded = self.layer_norm(encoded)
        
        return encoded


class PositionalEncoding(nn.Module):
    """í‘œì¤€ positional encoding"""
    def __init__(self, d_model: int, max_len: int = 1000):
        super().__init__()
        
        pe = torch.zeros(max_len, d_model)
        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)
        div_term = torch.exp(torch.arange(0, d_model, 2).float() * 
                           (-math.log(10000.0) / d_model))
        
        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)
        pe = pe.unsqueeze(0)  # [1, max_len, d_model]
        
        self.register_buffer('pe', pe)
        
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        seq_len = x.size(1)
        return x + self.pe[:, :seq_len, :]


class HourlyAttentionFusion(nn.Module):
    """
    24ì‹œê°„ ê° ì‹œê°„ë³„ë¡œ historical sequenceì—ì„œ ê´€ë ¨ ì •ë³´ë¥¼ ì„ íƒì ìœ¼ë¡œ ì¶”ì¶œ
    """
    def __init__(self, d_model: int = 256, nhead: int = 8):
        super().__init__()
        
        self.d_model = d_model
        
        # 24ì‹œê°„ ê°ê°ì— ëŒ€í•œ í•™ìŠµ ê°€ëŠ¥í•œ ì¿¼ë¦¬ ë²¡í„°
        self.hour_queries = nn.Parameter(torch.randn(24, d_model) / math.sqrt(d_model))
        
        # Cross attention: ê° ì˜ˆì¸¡ ì‹œê°„ â†’ historical sequence
        self.cross_attention = nn.MultiheadAttention(
            embed_dim=d_model,
            num_heads=nhead,
            dropout=0.1,
            batch_first=True
        )
        
        # Context refinement
        self.context_refine = nn.Sequential(
            nn.Linear(d_model, d_model),
            nn.GELU(),
            nn.Dropout(0.1),
            nn.Linear(d_model, d_model),
            nn.LayerNorm(d_model)
        )
        
    def forward(self, historical_encoded: torch.Tensor, 
                padding_mask: Optional[torch.Tensor] = None) -> torch.Tensor:
        """
        Args:
            historical_encoded: [batch, seq_len, d_model] - ì¸ì½”ë”©ëœ historical sequence
            padding_mask: [batch, seq_len] - íŒ¨ë”© ë§ˆìŠ¤í¬
            
        Returns:
            hourly_contexts: [batch, 24, d_model] - ê° ì‹œê°„ë³„ context
        """
        batch_size = historical_encoded.size(0)
        
        # 24ì‹œê°„ ì¿¼ë¦¬ë¥¼ ë°°ì¹˜ í¬ê¸°ë§Œí¼ ë³µì œ
        queries = self.hour_queries.unsqueeze(0).expand(batch_size, -1, -1)  # [batch, 24, d_model]
        
        # Cross attention: ê° ì˜ˆì¸¡ ì‹œê°„ì´ historical sequenceì—ì„œ ê´€ë ¨ ì •ë³´ ì¶”ì¶œ
        hourly_contexts, attention_weights = self.cross_attention(
            query=queries,                    # [batch, 24, d_model] - 24ì‹œê°„ ì¿¼ë¦¬
            key=historical_encoded,           # [batch, seq_len, d_model] - historical
            value=historical_encoded,         # [batch, seq_len, d_model] - historical
            key_padding_mask=padding_mask     # [batch, seq_len] - íŒ¨ë”© ë§ˆìŠ¤í¬
        )
        
        # Context ì •ì œ
        hourly_contexts = self.context_refine(hourly_contexts)  # [batch, 24, d_model]
        
        return hourly_contexts


class PatchMixerBlock(nn.Module):
    """PatchTSMixerì˜ í•µì‹¬ MLP Mixer ë¸”ë¡"""
    def __init__(self, dim: int, expansion_factor: int = 4):
        super().__init__()
        
        self.norm1 = nn.LayerNorm(dim)
        self.norm2 = nn.LayerNorm(dim)
        
        # Feature mixing MLP
        self.feature_mlp = nn.Sequential(
            nn.Linear(dim, dim * expansion_factor),
            nn.GELU(),
            nn.Dropout(0.1),
            nn.Linear(dim * expansion_factor, dim),
            nn.Dropout(0.1)
        )
        
        # Residual scaling
        self.gamma1 = nn.Parameter(torch.ones(1))
        self.gamma2 = nn.Parameter(torch.ones(1))
        
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """
        Args:
            x: [batch, seq_len, dim]
        Returns:
            x: [batch, seq_len, dim]
        """
        # Feature mixing with residual connection
        residual = x
        x = self.norm1(x)
        x = residual + self.gamma1 * self.feature_mlp(x)
        
        # Final normalization
        x = self.norm2(x)
        
        return x


class ChannelMixer(nn.Module):
    """íŠ¹ì„± ê°„ ìƒí˜¸ì‘ìš© (Channel mixing)"""
    def __init__(self, dim: int):
        super().__init__()
        
        self.norm = nn.LayerNorm(dim)
        self.channel_mlp = nn.Sequential(
            nn.Linear(dim, dim * 2),
            nn.GELU(),
            nn.Dropout(0.1),
            nn.Linear(dim * 2, dim),
            nn.Dropout(0.1)
        )
        
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """
        Args:
            x: [batch, 24, dim] - ê° ì‹œê°„ë³„ë¡œ íŠ¹ì„± mixing
        """
        return self.channel_mlp(self.norm(x))


class TemporalMixer(nn.Module):
    """ì‹œê°„ ê°„ ìƒí˜¸ì‘ìš© (Temporal mixing)"""
    def __init__(self, seq_len: int = 24):
        super().__init__()
        
        self.norm = nn.LayerNorm(seq_len)
        self.temporal_mlp = nn.Sequential(
            nn.Linear(seq_len, seq_len * 2),
            nn.GELU(),
            nn.Dropout(0.1),
            nn.Linear(seq_len * 2, seq_len),
            nn.Dropout(0.1)
        )
        
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """
        Args:
            x: [batch, 24, dim]
        Returns:
            x: [batch, 24, dim]
        """
        # [batch, 24, dim] â†’ [batch, dim, 24] â†’ mixing â†’ [batch, dim, 24] â†’ [batch, 24, dim]
        x_t = x.transpose(1, 2)  # [batch, dim, 24]
        x_t_mixed = self.temporal_mlp(self.norm(x_t))  # [batch, dim, 24]
        return x_t_mixed.transpose(1, 2)  # [batch, 24, dim]


class OptimalPatchTSMixerFusion(nn.Module):
    """
    PatchTSMixer ê¸°ë°˜ ìµœì¢… ì˜ˆì¸¡ ëª¨ë“ˆ
    Historical context + Weather + Plant meta â†’ 24ì‹œê°„ ë°œì „ëŸ‰ ì˜ˆì¸¡
    """
    def __init__(self, historical_dim: int = 256, weather_dim: int = 5, plant_dim: int = 7):
        super().__init__()
        
        # ëª¨ë‹¬ë¦¬í‹°ë³„ projection
        self.weather_proj = nn.Sequential(
            nn.Linear(weather_dim, 32),
            nn.GELU(),
            nn.LayerNorm(32)
        )
        
        self.plant_proj = nn.Sequential(
            nn.Linear(plant_dim, 32),
            nn.GELU(),
            nn.LayerNorm(32)
        )
        
        # ìœµí•©ëœ íŠ¹ì„± ì°¨ì›
        self.fused_dim = historical_dim + 32 + 32  # 320
        
        # PatchTSMixer ìŠ¤íƒ€ì¼ ë¸”ë¡ë“¤
        self.mixer_blocks = nn.ModuleList([
            PatchMixerBlock(self.fused_dim) for _ in range(3)
        ])
        
        # Channel mixing (íŠ¹ì„± ê°„ ìƒí˜¸ì‘ìš©)
        self.channel_mixer = ChannelMixer(self.fused_dim)
        
        # Temporal mixing (24ì‹œê°„ ê°„ ìƒí˜¸ì‘ìš©)  
        self.temporal_mixer = TemporalMixer(24)
        
        # ì¤‘ê°„ projection
        self.mid_projection = nn.Sequential(
            nn.Linear(self.fused_dim, 128),
            nn.GELU(),
            nn.LayerNorm(128),
            nn.Dropout(0.1)
        )
        
        # Final prediction head
        self.prediction_head = nn.Sequential(
            nn.Linear(128, 64),
            nn.GELU(),
            nn.Dropout(0.1),
            nn.Linear(64, 32),
            nn.GELU(),
            nn.Linear(32, 1)
        )
        
    def forward(self, hourly_historical: torch.Tensor, weather_data: torch.Tensor, 
                plant_meta: torch.Tensor) -> torch.Tensor:
        """
        Args:
            hourly_historical: [batch, 24, 256] - ì‹œê°„ë³„ historical context
            weather_data: [batch, 24, 5] - ê¸°ìƒ ë°ì´í„°
            plant_meta: [batch, 7] - ë°œì „ì†Œ ë©”íƒ€
            
        Returns:
            predictions: [batch, 24] - 24ì‹œê°„ ë°œì „ëŸ‰ ì˜ˆì¸¡
        """
        batch_size = hourly_historical.size(0)
        
        # 1. ëª¨ë‹¬ë¦¬í‹° ìœµí•©
        weather_embed = self.weather_proj(weather_data)  # [batch, 24, 32]
        plant_embed = self.plant_proj(plant_meta)  # [batch, 32]
        plant_expanded = plant_embed.unsqueeze(1).expand(-1, 24, -1)  # [batch, 24, 32]
        
        # ëª¨ë“  ì •ë³´ ê²°í•©
        fused_features = torch.cat([
            hourly_historical,  # [batch, 24, 256] - ì‹œê°„ë³„ historical context
            weather_embed,      # [batch, 24, 32] - ì‹œê°„ë³„ ê¸°ìƒ ì •ë³´
            plant_expanded      # [batch, 24, 32] - ë°œì „ì†Œ íŠ¹ì„±
        ], dim=-1)  # [batch, 24, 320]
        
        # 2. PatchTSMixer ìŠ¤íƒ€ì¼ ì²˜ë¦¬
        x = fused_features
        
        # ì—¬ëŸ¬ Mixer ë¸”ë¡ í†µê³¼
        for mixer_block in self.mixer_blocks:
            x = mixer_block(x) + x  # Residual connection -> ì¤‘ë³µì¸ë“¯
            #x = mixer_block(x)
            
        # 3. Channel mixing (íŠ¹ì„± ê°„ ìƒí˜¸ì‘ìš©)
        x = x + self.channel_mixer(x)  # Residual connection
        
        # 4. Temporal mixing (ì‹œê°„ ê°„ ìƒí˜¸ì‘ìš©)
        x = x + self.temporal_mixer(x)  # Residual connection
        
        # 5. ì¤‘ê°„ projection
        x = self.mid_projection(x)  # [batch, 24, 128]
        
        # 6. ìµœì¢… ì˜ˆì¸¡
        predictions = self.prediction_head(x).squeeze(-1)  # [batch, 24]
        
        return predictions


class SolarPowerPredictor(nn.Module):
    """
    ìµœì¢… íƒœì–‘ê´‘ ë°œì „ëŸ‰ ì˜ˆì¸¡ ëª¨ë¸
    
    Architecture:
    1. Historical Encoder (Transformer) - ì‹œê°„ ì •ë³´ ë³´ì¡´
    2. Hourly Attention - ê° ì˜ˆì¸¡ ì‹œê°„ë³„ context ì¶”ì¶œ  
    3. PatchTSMixer Fusion - ìµœì¢… ì˜ˆì¸¡
    """
    
    def __init__(
        self,
        historical_features: int = 54,
        weather_features: int = 5,
        plant_features: int = 7,
        d_model: int = 256,
        nhead: int = 8,
        num_encoder_layers: int = 4,
        device: str = "cuda" if torch.cuda.is_available() else "cpu"
    ):
        super().__init__()
        
        self.device = device
        self.d_model = d_model
        
        # 1. Historical Encoder (ì‹œê°„ ì •ë³´ ë³´ì¡´)
        self.historical_encoder = PaddingAwareEncoder(
            input_dim=historical_features,
            d_model=d_model,
            nhead=nhead,
            num_layers=num_encoder_layers
        )
        
        # 2. ì‹œê°„ë³„ Attention (ê° ì˜ˆì¸¡ ì‹œê°„ë³„ context)
        self.hourly_attention = HourlyAttentionFusion(
            d_model=d_model,
            nhead=nhead
        )
        
        # 3. PatchTSMixer ê¸°ë°˜ Final Prediction
        self.final_predictor = OptimalPatchTSMixerFusion(
            historical_dim=d_model,
            weather_dim=weather_features,
            plant_dim=plant_features
        )
        
        # 4. ì¶œë ¥ ì²˜ë¦¬
        self.output_activation = nn.ReLU()
        
        # ëª¨ë¸ ì´ˆê¸°í™”
        self._initialize_weights()
        
    def _initialize_weights(self):
        """ê°€ì¤‘ì¹˜ ì´ˆê¸°í™”"""
        for module in self.modules():
            if isinstance(module, nn.Linear):
                nn.init.xavier_uniform_(module.weight)
                if module.bias is not None:
                    nn.init.zeros_(module.bias)
            elif isinstance(module, nn.LayerNorm):
                nn.init.ones_(module.weight)
                nn.init.zeros_(module.bias)
        
    def forward(
        self,
        historical_data: torch.Tensor,
        weather_data: torch.Tensor,
        plant_meta: torch.Tensor,
        padding_mask: Optional[torch.Tensor] = None,
        return_attention: bool = False
    ) -> Dict[str, torch.Tensor]:
        """
        Forward pass
        
        Args:
            historical_data: [batch, seq_len, 54] - ì´í‹€ê°„ historical ë°ì´í„° (seq_len=48)
            weather_data: [batch, 24, 5] - ê¸°ìƒ ì˜ˆë³´  
            plant_meta: [batch, 10] - ë°œì „ì†Œ ë©”íƒ€ë°ì´í„° (10ê°œ íŠ¹ì„±)
            padding_mask: [batch, seq_len] - íŒ¨ë”© ë§ˆìŠ¤í¬
            return_attention: attention weights ë°˜í™˜ ì—¬ë¶€
            
        Returns:
            Dict containing predictions and intermediate outputs
        """
        
        # 1. Historical encoding (ì‹œê°„ ì •ë³´ ë³´ì¡´)
        historical_encoded = self.historical_encoder(historical_data, padding_mask)
        # [batch, seq_len, d_model] - ëª¨ë“  ì‹œê°„ ì •ë³´ ìœ ì§€!
        
        # 2. 24ì‹œê°„ ê°ê°ì— ëŒ€í•œ selective attention
        hourly_contexts = self.hourly_attention(historical_encoded, padding_mask)
        # [batch, 24, d_model] - ê° ì˜ˆì¸¡ ì‹œê°„ë³„ ìµœì  context
        
        # 3. PatchTSMixer ê¸°ë°˜ ìµœì¢… ì˜ˆì¸¡
        predictions = self.final_predictor(hourly_contexts, weather_data, plant_meta)
        # [batch, 24] - ì‹œê³„ì—´ ì˜ˆì¸¡ ì „ë¬¸ ëª¨ë¸ë¡œ ì²˜ë¦¬
        
        # 4. í›„ì²˜ë¦¬ (ë°œì „ëŸ‰ì€ í•­ìƒ ì–‘ìˆ˜)
        predictions = self.output_activation(predictions)
        
        outputs = {
            'predictions': predictions,
            'hourly_contexts': hourly_contexts,
            'historical_encoded': historical_encoded
        }
        
        return outputs
        
    def predict(
        self, 
        historical_data: torch.Tensor, 
        weather_data: torch.Tensor,
        plant_meta: torch.Tensor, 
        padding_mask: Optional[torch.Tensor] = None
    ) -> torch.Tensor:
        """ì¶”ë¡ ìš© í•¨ìˆ˜"""
        self.eval()
        with torch.no_grad():
            outputs = self.forward(historical_data, weather_data, plant_meta, padding_mask)
            return outputs['predictions']
    
    def get_model_info(self) -> Dict[str, Any]:
        """ëª¨ë¸ ì •ë³´ ë°˜í™˜"""
        total_params = sum(p.numel() for p in self.parameters())
        trainable_params = sum(p.numel() for p in self.parameters() if p.requires_grad)
        
        return {
            'model_name': 'SolarPowerPredictor',
            'total_parameters': total_params,
            'trainable_parameters': trainable_params,
            'model_size_mb': total_params * 4 / 1024 / 1024,  # float32 ê¸°ì¤€
            'preserves_temporal_info': True,
            'handles_padding': True,
            'uses_all_features': True,
            'historical_features': 54,
            'd_model': self.d_model,
            'device': self.device,
            'architecture': 'Transformer + PatchTSMixer Fusion'
        }


def create_solar_prediction_model(
    historical_features: int = 54,
    weather_features: int = 5,
    plant_features: int = 7,
    d_model: int = 256,
    nhead: int = 8,
    num_encoder_layers: int = 4,

    device: str = "auto"
) -> SolarPowerPredictor:
    """
    íƒœì–‘ê´‘ ë°œì „ëŸ‰ ì˜ˆì¸¡ ëª¨ë¸ ìƒì„±
    
    Args:
        historical_features: Historical ë°ì´í„° íŠ¹ì„± ìˆ˜ (ê¸°ë³¸ 54)
        weather_features: Weather ë°ì´í„° íŠ¹ì„± ìˆ˜ (ê¸°ë³¸ 5)  
        plant_features: Plant meta ë°ì´í„° íŠ¹ì„± ìˆ˜ (ê¸°ë³¸ 7)
        d_model: Transformer ëª¨ë¸ ì°¨ì› (ê¸°ë³¸ 256)
        device: ë””ë°”ì´ìŠ¤ ì„¤ì •
        
    Returns:
        SolarPowerPredictor ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤
    """
    if device == "auto":
        device = "cuda" if torch.cuda.is_available() else "cpu"
    
    model = SolarPowerPredictor(
        historical_features=historical_features,
        weather_features=weather_features,
        plant_features=plant_features,
        d_model=d_model,
        nhead=nhead,
        num_encoder_layers=num_encoder_layers,
        device=device
    )
    
    model = model.to(device)
    
    print("ğŸŒ Solar Power Prediction Model Created! ğŸŒ")
    print("=" * 50)
    model_info = model.get_model_info()
    for key, value in model_info.items():
        print(f"{key}: {value}")
    print("=" * 50)
    
    return model

'''
# ì‚¬ìš© ì˜ˆì‹œ ë° í…ŒìŠ¤íŠ¸

# ëª¨ë¸ ìƒì„±
print("Creating Solar Power Prediction Model...")
model = create_solar_prediction_model(device="cuda" if torch.cuda.is_available() else "cpu")

# í…ŒìŠ¤íŠ¸ ë°ì´í„° ìƒì„±
batch_size = 8
seq_len = 48    # 4ì¼ * 24ì‹œê°„
device = model.device

print(f"\nGenerating test data on device: {device}")

# ì‹¤ì œ ë°ì´í„° í¬ê¸°ì™€ ë™ì¼í•œ ë”ë¯¸ ë°ì´í„°
historical_data = torch.randn(batch_size, seq_len, 54).to(device)
weather_data = torch.randn(batch_size, 24, 5).to(device)
plant_meta = torch.randn(batch_size, 7).to(device)

# íŒ¨ë”© ë§ˆìŠ¤í¬ (14ì‹œ/20ì‹œ ì‹œë‚˜ë¦¬ì˜¤ ì‹œë®¬ë ˆì´ì…˜)
padding_mask = torch.zeros(batch_size, seq_len, dtype=torch.bool).to(device)
# ì˜ˆì‹œ: ì ˆë°˜ì€ 14ì‹œ ì‹œë‚˜ë¦¬ì˜¤, ì ˆë°˜ì€ 20ì‹œ ì‹œë‚˜ë¦¬ì˜¤
padding_mask[:batch_size//2, -10:] = True  # 14ì‹œ ì‹œë‚˜ë¦¬ì˜¤ (10ì‹œê°„ íŒ¨ë”©)
padding_mask[batch_size//2:, -4:] = True   # 20ì‹œ ì‹œë‚˜ë¦¬ì˜¤ (4ì‹œê°„ íŒ¨ë”©)

print(f"Test data shapes:")
print(f"  Historical: {historical_data.shape}")
print(f"  Weather: {weather_data.shape}")
print(f"  Plant meta: {plant_meta.shape}")
print(f"  Padding mask: {padding_mask.shape}")

# Forward pass í…ŒìŠ¤íŠ¸
print(f"\nTesting forward pass...")

model.eval()
with torch.no_grad():
    outputs = model(historical_data, weather_data, plant_meta, padding_mask)

predictions = outputs['predictions']
hourly_contexts = outputs['hourly_contexts']

print(f"âœ… Forward pass successful!")
print(f"Predictions shape: {predictions.shape}")
print(f"Hourly contexts shape: {hourly_contexts.shape}")
print(f"Sample predictions (first sample): {predictions[0][:6].tolist()}")

# ì¶”ë¡  í…ŒìŠ¤íŠ¸
print(f"\nTesting inference...")
predictions_infer = model.predict(historical_data, weather_data, plant_meta, padding_mask)
print(f"âœ… Inference successful!")
print(f"Inference predictions shape: {predictions_infer.shape}")

# ê²°ê³¼ ì¼ì¹˜ í™•ì¸
assert torch.allclose(predictions, predictions_infer, atol=1e-6)
print(f"âœ… Forward and inference results match!")

print(f"\nğŸ‰ All tests passed! Model is ready for training! ğŸ‰")

# ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì²´í¬
if torch.cuda.is_available():
    print(f"GPU memory allocated: {torch.cuda.memory_allocated(device) / 1024**2:.1f} MB")
    print(f"GPU memory cached: {torch.cuda.memory_reserved(device) / 1024**2:.1f} MB")
'''
import torch
import torch.nn as nn
import torch.nn.functional as F
from typing import Optional, Dict, Any


class SimpleCNNEncoder(nn.Module):
    """
    CNN ê¸°ë°˜ Historical Encoder
    Transformerë³´ë‹¤ ê°€ë³ê³  íŒ¨ë”©ë„ ìì—°ìŠ¤ëŸ½ê²Œ ì²˜ë¦¬
    """
    def __init__(self, input_dim: int = 54, hidden_dim: int = 128):
        super().__init__()
        
        self.input_projection = nn.Linear(input_dim, hidden_dim)
        
        # 1D CNN layers for temporal patterns
        self.conv_layers = nn.ModuleList([
            nn.Conv1d(hidden_dim, hidden_dim, kernel_size=3, padding=1),
            nn.Conv1d(hidden_dim, hidden_dim, kernel_size=5, padding=2),
            nn.Conv1d(hidden_dim, hidden_dim, kernel_size=7, padding=3),
        ])
        
        self.layer_norms = nn.ModuleList([
            nn.LayerNorm(hidden_dim) for _ in range(3)
        ])
        
        self.dropout = nn.Dropout(0.1)
        
    def forward(self, x: torch.Tensor, padding_mask: Optional[torch.Tensor] = None) -> torch.Tensor:
        """
        Args:
            x: [batch, seq_len, 54]
            padding_mask: [batch, seq_len] - True for padded positions
        Returns:
            encoded: [batch, seq_len, hidden_dim]
        """
        # Input projection
        x = self.input_projection(x)  # [batch, seq_len, hidden_dim]
        
        # Apply padding mask if provided
        if padding_mask is not None:
            x = x.masked_fill(padding_mask.unsqueeze(-1), 0.0)
        
        # CNN processing
        x_conv = x.transpose(1, 2)  # [batch, hidden_dim, seq_len] for conv1d
        
        for conv, norm in zip(self.conv_layers, self.layer_norms):
            # Convolution
            conv_out = conv(x_conv)  # [batch, hidden_dim, seq_len]
            conv_out = F.gelu(conv_out)
            
            # Residual connection + layer norm
            x_conv = x_conv + self.dropout(conv_out)
            x_conv = norm(x_conv.transpose(1, 2)).transpose(1, 2)  # Normalize over hidden_dim
        
        encoded = x_conv.transpose(1, 2)  # [batch, seq_len, hidden_dim]
        
        # Apply padding mask again
        if padding_mask is not None:
            encoded = encoded.masked_fill(padding_mask.unsqueeze(-1), 0.0)
        
        return encoded


class SimpleAttentionPooling(nn.Module):
    """
    ê°„ë‹¨í•œ Attentionìœ¼ë¡œ 24ì‹œê°„ context ìƒì„±
    ê° ì˜ˆì¸¡ ì‹œê°„ë³„ë¡œ historical sequenceì—ì„œ ì¤‘ìš”í•œ ì •ë³´ ì¶”ì¶œ
    """
    def __init__(self, hidden_dim: int = 128):
        super().__init__()
        
        self.hidden_dim = hidden_dim
        
        # 24ì‹œê°„ ê°ê°ì— ëŒ€í•œ ì¿¼ë¦¬ ë²¡í„°
        self.hour_queries = nn.Parameter(torch.randn(24, hidden_dim) * 0.1)
        
        # Simple attention
        self.query_proj = nn.Linear(hidden_dim, hidden_dim)
        self.key_proj = nn.Linear(hidden_dim, hidden_dim)
        self.value_proj = nn.Linear(hidden_dim, hidden_dim)
        
        self.scale = hidden_dim ** -0.5
        self.dropout = nn.Dropout(0.1)
        
    def forward(self, encoded_hist: torch.Tensor, 
                padding_mask: Optional[torch.Tensor] = None) -> torch.Tensor:
        """
        Args:
            encoded_hist: [batch, seq_len, hidden_dim]
            padding_mask: [batch, seq_len]
        Returns:
            hourly_contexts: [batch, 24, hidden_dim]
        """
        batch_size = encoded_hist.size(0)
        
        # 24ì‹œê°„ ì¿¼ë¦¬ í™•ì¥
        queries = self.hour_queries.unsqueeze(0).expand(batch_size, -1, -1)  # [batch, 24, hidden_dim]
        
        # Q, K, V projection
        Q = self.query_proj(queries)  # [batch, 24, hidden_dim]
        K = self.key_proj(encoded_hist)  # [batch, seq_len, hidden_dim]
        V = self.value_proj(encoded_hist)  # [batch, seq_len, hidden_dim]
        
        # Attention scores
        scores = torch.matmul(Q, K.transpose(-2, -1)) * self.scale  # [batch, 24, seq_len]
        
        # Apply padding mask
        if padding_mask is not None:
            scores = scores.masked_fill(padding_mask.unsqueeze(1), float('-inf'))
        
        # Attention weights
        attn_weights = F.softmax(scores, dim=-1)  # [batch, 24, seq_len]
        attn_weights = self.dropout(attn_weights)
        
        # Weighted sum
        hourly_contexts = torch.matmul(attn_weights, V)  # [batch, 24, hidden_dim]
        
        return hourly_contexts


class SimplePredictionHead(nn.Module):
    """
    ê°„ë‹¨í•œ MLP ê¸°ë°˜ ì˜ˆì¸¡ í—¤ë“œ
    Historical context + Weather + Plant meta â†’ 24ì‹œê°„ ì˜ˆì¸¡
    """
    def __init__(self, historical_dim: int = 128, weather_dim: int = 5, plant_dim: int = 10):
        super().__init__()
        
        # Weatherì™€ Plant meta ì²˜ë¦¬
        self.weather_encoder = nn.Sequential(
            nn.Linear(weather_dim, 32),
            nn.GELU(),
            nn.LayerNorm(32),
            nn.Dropout(0.1)
        )
        
        self.plant_encoder = nn.Sequential(
            nn.Linear(plant_dim, 32),
            nn.GELU(),
            nn.LayerNorm(32)
        )
        
        # ìœµí•©ëœ íŠ¹ì„± ì²˜ë¦¬
        fused_dim = historical_dim + 32 + 32  # 192
        
        # ê°„ë‹¨í•œ MLP
        self.prediction_mlp = nn.Sequential(
            nn.Linear(fused_dim, 128),
            nn.GELU(),
            nn.LayerNorm(128),
            nn.Dropout(0.1),
            
            nn.Linear(128, 64),
            nn.GELU(),
            nn.Dropout(0.1),
            
            nn.Linear(64, 32),
            nn.GELU(),
            
            nn.Linear(32, 1)  # ê° ì‹œê°„ë³„ë¡œ 1ê°œ ê°’ ì˜ˆì¸¡
        )
        
    def forward(self, hourly_contexts: torch.Tensor, 
                weather_data: torch.Tensor, plant_meta: torch.Tensor) -> torch.Tensor:
        """
        Args:
            hourly_contexts: [batch, 24, hidden_dim]
            weather_data: [batch, 24, weather_dim]
            plant_meta: [batch, plant_dim]
        Returns:
            predictions: [batch, 24]
        """
        batch_size = hourly_contexts.size(0)
        
        # Weather encoding
        weather_encoded = self.weather_encoder(weather_data)  # [batch, 24, 32]
        
        # Plant meta encoding and expansion
        plant_encoded = self.plant_encoder(plant_meta)  # [batch, 32]
        plant_expanded = plant_encoded.unsqueeze(1).expand(-1, 24, -1)  # [batch, 24, 32]
        
        # Feature fusion
        fused_features = torch.cat([
            hourly_contexts,    # [batch, 24, hidden_dim]
            weather_encoded,    # [batch, 24, 32]
            plant_expanded      # [batch, 24, 32]
        ], dim=-1)  # [batch, 24, fused_dim]
        
        # Prediction for each hour
        predictions = self.prediction_mlp(fused_features).squeeze(-1)  # [batch, 24]
        
        return predictions


class SimpleSolarPredictor(nn.Module):
    """
    ê°„ë‹¨í•œ íƒœì–‘ê´‘ ë°œì „ëŸ‰ ì˜ˆì¸¡ ëª¨ë¸
    
    Architecture:
    1. CNN Encoder - ì‹œê³„ì—´ íŒ¨í„´ ì¶”ì¶œ
    2. Simple Attention - 24ì‹œê°„ë³„ context ìƒì„±
    3. MLP Prediction - ìµœì¢… ì˜ˆì¸¡
    """
    
    def __init__(
        self,
        historical_features: int = 54,
        weather_features: int = 5,
        plant_features: int = 10,
        hidden_dim: int = 128,
        device: str = "cuda" if torch.cuda.is_available() else "cpu"
    ):
        super().__init__()
        
        self.device = device
        self.hidden_dim = hidden_dim
        
        # 1. CNN-based Historical Encoder
        self.historical_encoder = SimpleCNNEncoder(
            input_dim=historical_features,
            hidden_dim=hidden_dim
        )
        
        # 2. Simple Attention Pooling
        self.attention_pooling = SimpleAttentionPooling(
            hidden_dim=hidden_dim
        )
        
        # 3. Simple Prediction Head
        self.prediction_head = SimplePredictionHead(
            historical_dim=hidden_dim,
            weather_dim=weather_features,
            plant_dim=plant_features
        )
        
        # 4. Output activation
        self.output_activation = nn.ReLU()
        
        # Initialize weights
        self._initialize_weights()
        
    def _initialize_weights(self):
        """ê°€ì¤‘ì¹˜ ì´ˆê¸°í™”"""
        for module in self.modules():
            if isinstance(module, nn.Linear):
                nn.init.xavier_uniform_(module.weight, gain=0.1)  # ì‘ì€ gainìœ¼ë¡œ ì•ˆì •ì  ì´ˆê¸°í™”
                if module.bias is not None:
                    nn.init.zeros_(module.bias)
            elif isinstance(module, nn.LayerNorm):
                nn.init.ones_(module.weight)
                nn.init.zeros_(module.bias)
            elif isinstance(module, nn.Conv1d):
                nn.init.kaiming_normal_(module.weight, mode='fan_out', nonlinearity='relu')
                if module.bias is not None:
                    nn.init.zeros_(module.bias)
        
    def forward(
        self,
        historical_data: torch.Tensor,
        weather_data: torch.Tensor,
        plant_meta: torch.Tensor,
        padding_mask: Optional[torch.Tensor] = None
    ) -> Dict[str, torch.Tensor]:
        """
        Forward pass
        
        Args:
            historical_data: [batch, seq_len, 54]
            weather_data: [batch, 24, 5]
            plant_meta: [batch, 10] -> will use first 7 features
            padding_mask: [batch, seq_len]
            
        Returns:
            Dict containing predictions and intermediate outputs
        """
        
        # Plant metaì—ì„œ ì²« 7ê°œ íŠ¹ì„±ë§Œ ì‚¬ìš©
        #plant_meta_used = plant_meta[:, :7]
        
        # 1. Historical encoding with CNN
        encoded_hist = self.historical_encoder(historical_data, padding_mask)
        # [batch, seq_len, hidden_dim]
        
        # 2. 24ì‹œê°„ë³„ context ìƒì„±
        hourly_contexts = self.attention_pooling(encoded_hist, padding_mask)
        # [batch, 24, hidden_dim]
        
        # 3. ìµœì¢… ì˜ˆì¸¡
        predictions = self.prediction_head(hourly_contexts, weather_data, plant_meta)
        # [batch, 24]
        
        # 4. í›„ì²˜ë¦¬ (ë°œì „ëŸ‰ì€ í•­ìƒ ì–‘ìˆ˜)
        predictions = self.output_activation(predictions)
        
        outputs = {
            'predictions': predictions,
            'hourly_contexts': hourly_contexts,
            'encoded_hist': encoded_hist
        }
        
        return outputs
        
    def predict(
        self, 
        historical_data: torch.Tensor, 
        weather_data: torch.Tensor,
        plant_meta: torch.Tensor, 
        padding_mask: Optional[torch.Tensor] = None
    ) -> torch.Tensor:
        """ì¶”ë¡ ìš© í•¨ìˆ˜"""
        self.eval()
        with torch.no_grad():
            outputs = self.forward(historical_data, weather_data, plant_meta, padding_mask)
            return outputs['predictions']
    
    def get_model_info(self) -> Dict[str, Any]:
        """ëª¨ë¸ ì •ë³´ ë°˜í™˜"""
        total_params = sum(p.numel() for p in self.parameters())
        trainable_params = sum(p.numel() for p in self.parameters() if p.requires_grad)
        
        return {
            'model_name': 'SimpleSolarPredictor',
            'total_parameters': total_params,
            'trainable_parameters': trainable_params,
            'model_size_mb': total_params * 4 / 1024 / 1024,  # float32 ê¸°ì¤€
            'hidden_dim': self.hidden_dim,
            'device': self.device,
            'architecture': 'CNN + Simple Attention + MLP'
        }


def create_simple_solar_model(
    historical_features: int = 54,
    weather_features: int = 5,
    plant_features: int = 10,
    hidden_dim: int = 128,
    device: str = "auto"
) -> SimpleSolarPredictor:
    """
    ê°„ë‹¨í•œ íƒœì–‘ê´‘ ë°œì „ëŸ‰ ì˜ˆì¸¡ ëª¨ë¸ ìƒì„±
    
    Args:
        historical_features: Historical ë°ì´í„° íŠ¹ì„± ìˆ˜ (ê¸°ë³¸ 54)
        weather_features: Weather ë°ì´í„° íŠ¹ì„± ìˆ˜ (ê¸°ë³¸ 5)  
        plant_features: Plant meta ë°ì´í„° íŠ¹ì„± ìˆ˜ (ê¸°ë³¸ 7)
        hidden_dim: ëª¨ë¸ íˆë“  ì°¨ì› (ê¸°ë³¸ 128)
        device: ë””ë°”ì´ìŠ¤ ì„¤ì •
        
    Returns:
        SimpleSolarPredictor ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤
    """
    if device == "auto":
        device = "cuda" if torch.cuda.is_available() else "cpu"
    
    model = SimpleSolarPredictor(
        historical_features=historical_features,
        weather_features=weather_features,
        plant_features=plant_features,
        hidden_dim=hidden_dim,
        device=device
    )
    
    model = model.to(device)
    
    print("ğŸŒ Simple Solar Power Prediction Model Created! ğŸŒ")
    print("=" * 55)
    model_info = model.get_model_info()
    for key, value in model_info.items():
        print(f"{key}: {value}")
    print("=" * 55)
    
    return model

'''
# ===== í…ŒìŠ¤íŠ¸ ì½”ë“œ =====
if __name__ == "__main__":
    print("Creating Simple Solar Power Prediction Model...")
    model = create_simple_solar_model(hidden_dim=64, device="cuda" if torch.cuda.is_available() else "cpu")
    
    # í…ŒìŠ¤íŠ¸ ë°ì´í„° ìƒì„±
    batch_size = 4
    seq_len = 48
    device = model.device
    
    print(f"\nGenerating test data on device: {device}")
    
    historical_data = torch.randn(batch_size, seq_len, 54).to(device)
    weather_data = torch.randn(batch_size, 24, 5).to(device)
    plant_meta = torch.randn(batch_size, 10).to(device)  # 10ê°œ íŠ¹ì„± ì¤‘ 7ê°œë§Œ ì‚¬ìš©
    
    # íŒ¨ë”© ë§ˆìŠ¤í¬
    padding_mask = torch.zeros(batch_size, seq_len, dtype=torch.bool).to(device)
    padding_mask[:2, -10:] = True  # ì¼ë¶€ ìƒ˜í”Œì— íŒ¨ë”©
    
    print(f"Test data shapes:")
    print(f"  Historical: {historical_data.shape}")
    print(f"  Weather: {weather_data.shape}")
    print(f"  Plant meta: {plant_meta.shape}")
    
    # Forward pass í…ŒìŠ¤íŠ¸
    print(f"\nTesting forward pass...")
    model.eval()
    
    with torch.no_grad():
        outputs = model(historical_data, weather_data, plant_meta, padding_mask)
    
    predictions = outputs['predictions']
    print(f"âœ… Forward pass successful!")
    print(f"Predictions shape: {predictions.shape}")
    print(f"Sample predictions: {predictions[0][:6].cpu().numpy()}")
    
    # ì¶”ë¡  í…ŒìŠ¤íŠ¸
    predictions_infer = model.predict(historical_data, weather_data, plant_meta, padding_mask)
    print(f"âœ… Inference successful!")
    
    # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰
    if torch.cuda.is_available():
        print(f"\nGPU memory allocated: {torch.cuda.memory_allocated(device) / 1024**2:.1f} MB")
    
    print(f"\nğŸ‰ Simple model tests passed! ğŸ‰")
'''
from datetime import datetime 

import torch
import torch.nn as nn
import torch.nn.functional as F
import math
from typing import Optional, Dict, Any, Tuple

import os
import yaml
import json
import random
import numpy as np
import pandas as pd
from torch.utils.data import Dataset, DataLoader
import wandb
from typing import Dict, List, Tuple, Optional
import warnings
warnings.filterwarnings('ignore')

# ëª¨ë¸ import (ìœ„ì—ì„œ ì •ì˜í•œ ëª¨ë¸)
#from complete_solar_prediction_model import create_solar_prediction_model, SolarPowerPredictor

def create_model_by_name(model_name: str, cfg: Dict, device: str):
    """ëª¨ë¸ëª…ì— ë”°ë¼ ëª¨ë¸ ìƒì„±"""
    if model_name.lower() == "transformer":
        from complete_solar_prediction_model import create_solar_prediction_model
        return create_solar_prediction_model(
            historical_features=cfg["model"]["historical_features"],
            weather_features=cfg["model"]["weather_features"],
            plant_features=cfg["model"]["plant_features"],
            d_model=cfg["model"]["d_model"],
            nhead=cfg["model"]["nhead"],
            num_encoder_layers=cfg["model"]["num_encoder_layers"],
            device=device
        )
    elif model_name.lower() == "simple":
        from simple_solar_model import create_simple_solar_model
        return create_simple_solar_model(
            historical_features=cfg["model"]["historical_features"],
            weather_features=cfg["model"]["weather_features"],
            plant_features=cfg["model"]["plant_features"],
            hidden_dim=cfg["model"]["d_model"],  # d_modelì„ hidden_dimìœ¼ë¡œ ì‚¬ìš©
            device=device
        )
    else:
        raise ValueError(f"Unknown model name: {model_name}. Use 'transformer' or 'simple'")

# â”€â”€â”€ 0) ì„¤ì • ë¡œë“œ ë° W&B ì¸ì¦ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def load_config(config_path: str = "config.yaml") -> Dict:
    """ì„¤ì • íŒŒì¼ ë¡œë“œ"""
    with open(config_path, "r", encoding="utf-8") as f:
        cfg = yaml.safe_load(f)
    return cfg


def setup_wandb(cfg: Dict):
    """W&B ì„¤ì • ë° ë¡œê·¸ì¸"""
    os.environ["WANDB_API_KEY"] = cfg["wandb"]["api_key"]
    os.environ["WANDB_ENTITY"] = cfg["wandb"]["entity"]
    os.environ["WANDB_PROJECT"] = cfg["wandb"]["project"]
    wandb.login(key=os.environ["WANDB_API_KEY"], relogin=True)


def set_seed(seed: int):
    """ì‹œë“œ ì„¤ì •"""
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False


# â”€â”€â”€ 1) ë°ì´í„°ì…‹ í´ë˜ìŠ¤ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
class SolarDataset(Dataset):
    """íƒœì–‘ê´‘ ë°œì „ëŸ‰ ì˜ˆì¸¡ì„ ìœ„í•œ ë°ì´í„°ì…‹"""
    
    def __init__(
        self, 
        data_dir: str, 
        scenario: str = "14ì‹œ", 
        split: str = "train",
        device: str = "cuda:4"
    ):
        self.data_dir = data_dir
        self.scenario = scenario
        self.split = split
        self.device = device
        
        # ë°ì´í„° ë¡œë“œ
        self._load_data()
        self._load_metadata()
        
        print(f"Loaded {split} dataset:")
        print(f"  Samples: {len(self.X_historical)}")
        print(f"  Historical shape: {self.X_historical.shape}")
        print(f"  Forecast shape: {self.X_forecast.shape}")
        print(f"  Meta shape: {self.X_meta.shape}")
        print(f"  Target shape: {self.y.shape}")
        
    def _load_data(self):
        """ë°ì´í„° íŒŒì¼ ë¡œë“œ"""
        split_dir = os.path.join(self.data_dir, self.scenario, self.split)

        # ë°ì´í„° ë¡œë“œ ë° float32, torch.tensorë¡œ ë³€í™˜í•˜ì—¬ ë©”ëª¨ë¦¬ì— ìœ ì§€
        self.X_historical = torch.tensor(np.load(os.path.join(split_dir, 'X_historical.npy'), allow_pickle=True), dtype=torch.float32)
        self.X_forecast = torch.tensor(np.load(os.path.join(split_dir, 'X_forecast.npy'), allow_pickle=True), dtype=torch.float32)
        self.X_meta = torch.tensor(np.load(os.path.join(split_dir, 'X_meta.npy'), allow_pickle=True), dtype=torch.float32)
        self.y = torch.tensor(np.load(os.path.join(split_dir, 'y.npy'), allow_pickle=True), dtype=torch.float32)
        self.y_original = torch.tensor(np.load(os.path.join(split_dir, 'y_original.npy'), allow_pickle=True), dtype=torch.float32)
        self.plants = np.load(os.path.join(split_dir, 'plants.npy'), allow_pickle=True) # plantsëŠ” ë¬¸ìì—´ì´ë¯€ë¡œ numpy ìœ ì§€
        self.padding_mask = torch.tensor(np.load(os.path.join(split_dir, 'padding_mask.npy'), allow_pickle=True), dtype=torch.bool)

        # ì—¬ê¸°ì„œ self.X_historical.to(self.device) ë“±ì„ í•˜ë©´ GPU ë©”ëª¨ë¦¬ë„ ì´ˆê¸°í™” ì‹œì ì— ì‚¬ìš©
        # í•˜ì§€ë§Œ DataLoaderê°€ ì•Œì•„ì„œ GPUë¡œ ì˜®ê²¨ì£¼ë¯€ë¡œ __getitem__ì—ì„œëŠ” .to(device) ì œê±°
        print(f"  Data loaded into tensors on CPU.")

        
    def _load_metadata(self):
        """ë°œì „ì†Œ ë©”íƒ€ë°ì´í„° ë¡œë“œ"""
        try:
            self.meta_df = pd.read_csv('/home/local/soominchung_991007/EF/code/tmp/data/solar_energy/meta_data.csv')
            print(f"Metadata loaded: {len(self.meta_df)} plants")
        except:
            print("Warning: meta_data.csv not found. Capacity info will be unavailable.")
            self.meta_df = None
    
    def get_plant_capacity(self, plant_name: str) -> float:
        """ë°œì „ì†Œ ìš©ëŸ‰ ì¡°íšŒ (kW ë‹¨ìœ„)"""
        if self.meta_df is None:
            return 1000.0  # ê¸°ë³¸ê°’
        
        try:
            capacity_mw = self.meta_df[self.meta_df['name'] == plant_name]['ìš©ëŸ‰(MW)'].iloc[0]
            return float(capacity_mw) * 1000  # MW to kW
        except:
            return 1000.0  # ê¸°ë³¸ê°’
    
    def denormalize_predictions(self, normalized_y: np.ndarray, plant_names: np.ndarray) -> np.ndarray:
        """ì •ê·œí™”ëœ ì˜ˆì¸¡ê°’ì„ ì›ë³¸ ìŠ¤ì¼€ì¼ë¡œ ë³µì›"""
        denormalized = np.zeros_like(normalized_y)
        
        for i, plant_name in enumerate(plant_names):
            capacity_kw = self.get_plant_capacity(plant_name)
            # ì •ê·œí™”ê°€ ìš©ëŸ‰ ê¸°ë°˜ì´ë¼ë©´: ì´ìš©ë¥  * ìš©ëŸ‰ = ë°œì „ëŸ‰
            denormalized[i] = normalized_y[i] * capacity_kw
            
        return denormalized
    
    def __len__(self):
        return len(self.X_historical)
    
    def __getitem__(self, idx):
        return {
            'historical': self.X_historical[idx], # .to(self.device) ì œê±°
            'forecast': self.X_forecast[idx],   # .to(self.device) ì œê±°
            'meta': self.X_meta[idx],         # .to(self.device) ì œê±°
            'target': self.y[idx],
            'target_original': self.y_original[idx],
            'plant_name': self.plants[idx],
            'padding_mask': self.padding_mask[idx]
        }        
        '''
        return {
            'historical': torch.tensor(self.X_historical[idx], dtype=torch.float32),
            'forecast': torch.tensor(self.X_forecast[idx], dtype=torch.float32),
            'meta': torch.tensor(self.X_meta[idx], dtype=torch.float32),
            'target': torch.tensor(self.y[idx], dtype=torch.float32),  # ì •ê·œí™”ëœ ê°’
            'target_original': torch.tensor(self.y_original[idx], dtype=torch.float32),  # ì›ë³¸ ê°’
            'plant_name': self.plants[idx],
            'padding_mask': torch.tensor(self.padding_mask[idx], dtype=torch.bool)
        }
        '''


# â”€â”€â”€ 2) ë°ì´í„°ë¡œë” ìƒì„± â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def create_dataloaders(cfg: Dict) -> Tuple[DataLoader, DataLoader, DataLoader, DataLoader]:
    """ë°ì´í„°ë¡œë” ìƒì„±"""
    data_dir = cfg["data"]["root_dir"]
    batch_size = cfg["training"]["batch_size"]
    device = cfg["training"]["device"]
    
    # 14ì‹œì™€ 20ì‹œ ì‹œë‚˜ë¦¬ì˜¤ ë°ì´í„°ì…‹ì„ í†µí•©
    datasets = {}
    
    for scenario in ["14ì‹œ", "20ì‹œ"]:
        for split in ["train", "valid", "test", "external_test"]:
            key = f"{scenario}_{split}"
            datasets[key] = SolarDataset(
                data_dir=data_dir,
                scenario=scenario,
                split=split,
                device=device
            )
    
    # 14ì‹œì™€ 20ì‹œ ë°ì´í„°ë¥¼ ê° splitë³„ë¡œ ê²°í•©
    def combine_datasets(dataset1, dataset2):
        """ë‘ ë°ì´í„°ì…‹ì„ ê²°í•©"""
        combined_data = {}
        
        # ëª¨ë“  ë°ì´í„° ê²°í•©
        for key in ['X_historical', 'X_forecast', 'X_meta', 'y', 'y_original', 'plants', 'padding_mask']:
            data1 = getattr(dataset1, key)
            data2 = getattr(dataset2, key)
            combined_data[key] = np.concatenate([data1, data2], axis=0)
        
        # ìƒˆë¡œìš´ ë°ì´í„°ì…‹ ê°ì²´ ìƒì„±
        combined_dataset = SolarDataset.__new__(SolarDataset)
        combined_dataset.data_dir = dataset1.data_dir
        combined_dataset.scenario = "combined"
        combined_dataset.split = dataset1.split
        combined_dataset.device = dataset1.device
        combined_dataset.meta_df = dataset1.meta_df
        
        # ê²°í•©ëœ ë°ì´í„° í• ë‹¹
        for key, value in combined_data.items():
            setattr(combined_dataset, key, value)
        
        return combined_dataset
    
    # ë°ì´í„°ì…‹ ê²°í•©
    train_dataset = combine_datasets(datasets["14ì‹œ_train"], datasets["20ì‹œ_train"])
    valid_dataset = combine_datasets(datasets["14ì‹œ_valid"], datasets["20ì‹œ_valid"])
    test_dataset = combine_datasets(datasets["14ì‹œ_test"], datasets["20ì‹œ_test"])
    external_test_dataset = combine_datasets(datasets["14ì‹œ_external_test"], datasets["20ì‹œ_external_test"])
    
    # ë°ì´í„°ë¡œë” ìƒì„±
    train_loader = DataLoader(
        train_dataset, 
        batch_size=batch_size, 
        shuffle=True, 
        num_workers=0,
        pin_memory=False
    )
    
    valid_loader = DataLoader(
        valid_dataset, 
        batch_size=batch_size, 
        shuffle=False, 
        num_workers=0,
        pin_memory=False
    )
    
    test_loader = DataLoader(
        test_dataset, 
        batch_size=batch_size, 
        shuffle=False, 
        num_workers=0,
        pin_memory=False
    )
    
    external_test_loader = DataLoader(
        external_test_dataset, 
        batch_size=batch_size, 
        shuffle=False, 
        num_workers=0,
        pin_memory=False
    )
    
    return train_loader, valid_loader, test_loader, external_test_loader


# â”€â”€â”€ 3) ì†ì‹¤ í•¨ìˆ˜ ì •ì˜ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
class SolarPredictionLoss(nn.Module):
    """íƒœì–‘ê´‘ ë°œì „ëŸ‰ ì˜ˆì¸¡ì„ ìœ„í•œ ì»¤ìŠ¤í…€ ì†ì‹¤ í•¨ìˆ˜"""
    
    def __init__(self, loss_type: str = "MSELoss", use_capacity_weighting: bool = True, 
                 use_denormalized_loss: bool = True, per_weight: float = 0.5):
        super().__init__()
        self.loss_type = loss_type
        self.use_capacity_weighting = use_capacity_weighting
        self.use_denormalized_loss = use_denormalized_loss
        self.per_weight = per_weight  
        
        # ê¸°ë³¸ ì†ì‹¤ í•¨ìˆ˜
        if loss_type == "MSELoss":
            self.base_loss = nn.MSELoss(reduction='none')
        elif loss_type == "L1Loss":
            self.base_loss = nn.L1Loss(reduction='none')
        elif loss_type == "SmoothL1Loss":
            self.base_loss = nn.SmoothL1Loss(reduction='none')
        elif loss_type == "PredictionErrorRate":
            self.base_loss = None  # ì»¤ìŠ¤í…€ ì†ì‹¤ í•¨ìˆ˜ ì‚¬ìš©
        elif loss_type == "MSE_PER_Weighted":  # ğŸ‘ˆ ì¶”ê°€
            self.base_loss = nn.MSELoss(reduction='none')
        else:
            self.base_loss = nn.MSELoss(reduction='none')
    
    def forward(self, predictions: torch.Tensor, targets: torch.Tensor, 
                plant_capacities: torch.Tensor) -> torch.Tensor:
        """
        Args:
            predictions: [batch, 24] - ì •ê·œí™”ëœ ì˜ˆì¸¡ê°’ (ì´ìš©ë¥ , 0~1)
            targets: [batch, 24] - ì •ê·œí™”ëœ ì‹¤ì œê°’ (ì´ìš©ë¥ , 0~1)
            plant_capacities: [batch] - ë°œì „ì†Œ ìš©ëŸ‰ (kW)
        """
        if self.loss_type == "PredictionErrorRate":
            return self._prediction_error_rate_loss(predictions, targets, plant_capacities)
        elif self.loss_type == "MSE_PER_Weighted":  # ğŸ‘ˆ ì¶”ê°€
            return self._mse_per_weighted_loss(predictions, targets, plant_capacities)
        
        if self.use_denormalized_loss:
            # ì •ê·œí™”ëœ ê°’ì„ ì‹¤ì œ ë°œì „ëŸ‰(kW)ìœ¼ë¡œ ì—­ì •ê·œí™”
            capacity_expanded = plant_capacities.unsqueeze(1)  # [batch, 1]
            pred_power = predictions * capacity_expanded  # [batch, 24] - ì‹¤ì œ ë°œì „ëŸ‰ (kW)
            target_power = targets * capacity_expanded    # [batch, 24] - ì‹¤ì œ ë°œì „ëŸ‰ (kW)
            
            # ì‹¤ì œ ë°œì „ëŸ‰ìœ¼ë¡œ ì†ì‹¤ ê³„ì‚°
            loss = self.base_loss(pred_power, target_power)  # [batch, 24]

            # ìš©ëŸ‰ìœ¼ë¡œ ì •ê·œí™”í•˜ì—¬ ìŠ¤ì¼€ì¼ ì¡°ì •
            #loss = loss / (capacity_expanded ** 2)  # ì œê³±ìœ¼ë¡œ ë‚˜ëˆ„ì–´ ìŠ¤ì¼€ì¼ ê°ì†Œ            
            loss = loss / (capacity_expanded) 
            
            # ìš©ëŸ‰ ê¸°ë°˜ ê°€ì¤‘ì¹˜ (ì„ íƒì‚¬í•­)
            if self.use_capacity_weighting:
                # í° ë°œì „ì†Œì¼ìˆ˜ë¡ ë†’ì€ ê°€ì¤‘ì¹˜, í•˜ì§€ë§Œ ì´ë¯¸ ì—­ì •ê·œí™”í–ˆìœ¼ë¯€ë¡œ ì•½í•˜ê²Œ ì ìš©
                capacity_weights = torch.sqrt(plant_capacities / plant_capacities.mean())
                capacity_weights = capacity_weights.unsqueeze(1)  # [batch, 1]
                loss = loss * capacity_weights
            
        else:
            # ê¸°ì¡´ ë°©ì‹: ì •ê·œí™”ëœ ê°’ìœ¼ë¡œ ì†ì‹¤ ê³„ì‚°
            loss = self.base_loss(predictions, targets)  # [batch, 24]
            
            if self.use_capacity_weighting:
                # ìš©ëŸ‰ ê¸°ë°˜ ê°€ì¤‘ì¹˜ (í° ë°œì „ì†Œì¼ìˆ˜ë¡ ë†’ì€ ê°€ì¤‘ì¹˜)
                capacity_weights = plant_capacities / plant_capacities.mean()
                capacity_weights = capacity_weights.unsqueeze(1)  # [batch, 1]
                loss = loss * capacity_weights
        
        # ì‹œê°„ì¶• í‰ê·  í›„ ë°°ì¹˜ í‰ê· 
        return loss.mean()
    
    def _prediction_error_rate_loss(self, predictions: torch.Tensor, targets: torch.Tensor, 
                                   plant_capacities: torch.Tensor, threshold_ratio: float = 0.1) -> torch.Tensor:
        """ì¬ìƒì—ë„ˆì§€ ë°œì „ëŸ‰ ì˜ˆì¸¡ì œë„ ì˜¤ì°¨ìœ¨ ê¸°ë°˜ ì†ì‹¤ í•¨ìˆ˜"""
        # ì´ìš©ë¥ ì„ ì‹¤ì œ ë°œì „ëŸ‰ìœ¼ë¡œ ë³€í™˜
        capacity_expanded = plant_capacities.unsqueeze(1)  # [batch, 1]
        pred_power = predictions * capacity_expanded  # [batch, 24]
        target_power = targets * capacity_expanded    # [batch, 24]
        
        # ì„ê³„ê°’ ì´ìƒì¸ ì‹œê°„ëŒ€ë§Œ ë§ˆìŠ¤í‚¹
        threshold_power = plant_capacities * threshold_ratio
        threshold_expanded = threshold_power.unsqueeze(1)  # [batch, 1]
        mask = (target_power >= threshold_expanded).float()  # [batch, 24]
        
        # ì ˆëŒ€ ì˜¤ì°¨ ê³„ì‚°
        abs_error = torch.abs(target_power - pred_power)  # [batch, 24]
        
        # ë§ˆìŠ¤í‚¹ëœ ì˜¤ì°¨ í•©ì‚°
        masked_error_sum = (abs_error * mask).sum(dim=1)  # [batch]
        
        # ì˜¤ì°¨ìœ¨ ê³„ì‚° (%) - 0ìœ¼ë¡œ ë‚˜ëˆ„ê¸° ë°©ì§€
        error_rate = (masked_error_sum / (plant_capacities + 1e-8)) * 100.0  # [batch]
        
        return error_rate.mean()

    def _mse_per_weighted_loss(self, predictions: torch.Tensor, targets: torch.Tensor, 
                            plant_capacities: torch.Tensor) -> torch.Tensor:
        """MSE + PER ê°€ì¤‘í•© ì†ì‹¤"""
        # MSE ì†ì‹¤ ê³„ì‚°
        if self.use_denormalized_loss:
            capacity_expanded = plant_capacities.unsqueeze(1)
            pred_power = predictions * capacity_expanded
            target_power = targets * capacity_expanded
            mse_loss = self.base_loss(pred_power, target_power)
            mse_loss = mse_loss / capacity_expanded  # ìŠ¤ì¼€ì¼ ì¡°ì •
            mse_loss = mse_loss.mean()
        else:
            mse_loss = self.base_loss(predictions, targets).mean()
        
        # PER ì†ì‹¤ ê³„ì‚°
        per_loss = self._prediction_error_rate_loss(predictions, targets, plant_capacities)
        
        # ê°€ì¤‘í•© (PERì„ 100ìœ¼ë¡œ ë‚˜ëˆ„ì–´ ìŠ¤ì¼€ì¼ ë§ì¶¤)
        weighted_loss = (1 - self.per_weight) * mse_loss + self.per_weight * (per_loss / 100.0)
        
        return weighted_loss

def print_sample_predictions(model, train_loader, valid_loader, device, epoch, num_samples=3):
    """ìƒ˜í”Œ ì˜ˆì¸¡ê°’ê³¼ ì‹¤ì œê°’ ì¶œë ¥"""
    model.eval()
    
    print(f"\nğŸ“Š Sample Predictions - Epoch {epoch}")
    print("=" * 60)
    
    # Train ìƒ˜í”Œ
    with torch.no_grad():
        train_batch = next(iter(train_loader))
        
        historical = train_batch['historical'].to(device)[:num_samples]
        forecast = train_batch['forecast'].to(device)[:num_samples]
        meta = train_batch['meta'].to(device)[:num_samples]
        target = train_batch['target'].to(device)[:num_samples]
        padding_mask = train_batch['padding_mask'].to(device)[:num_samples]
        plant_names = train_batch['plant_name'][:num_samples]
        
        # ìš©ëŸ‰ ì •ë³´
        capacities = []
        for plant_name in plant_names:
            capacity = train_loader.dataset.get_plant_capacity(plant_name)
            capacities.append(capacity)
        capacities = torch.tensor(capacities, device=device, dtype=torch.float32)
        
        # ì˜ˆì¸¡
        outputs = model(historical, forecast, meta, padding_mask)
        predictions = outputs['predictions']
        
        if epoch % 5 == 0 or epoch == 1:
            print("ğŸš‚ TRAIN Samples:")
            for i in range(num_samples):
                pred_power = predictions[i] * capacities[i]
                target_power = target[i] * capacities[i]
                
                # ëª‡ ê°œ ì‹œê°„ëŒ€ë§Œ ì¶œë ¥ (0, 6, 12, 18ì‹œ)
                sample_hours = [0, 6, 12, 18]
                
                print(f"  Plant: {plant_names[i][:15]:15s} | Capacity: {capacities[i].item():6.0f}kW")
                for hour in sample_hours:
                    pred_val = pred_power[hour].item()
                    target_val = target_power[hour].item()
                    error = abs(pred_val - target_val)
                    print(f"    Hour {hour:2d}: Pred={pred_val:6.1f}kW, True={target_val:6.1f}kW, Error={error:6.1f}kW")
        
    # Valid ìƒ˜í”Œ
    with torch.no_grad():
        valid_batch = next(iter(valid_loader))
        
        historical = valid_batch['historical'].to(device)[:num_samples]
        forecast = valid_batch['forecast'].to(device)[:num_samples]
        meta = valid_batch['meta'].to(device)[:num_samples]
        target = valid_batch['target'].to(device)[:num_samples]
        padding_mask = valid_batch['padding_mask'].to(device)[:num_samples]
        plant_names = valid_batch['plant_name'][:num_samples]
        
        # ìš©ëŸ‰ ì •ë³´
        capacities = []
        for plant_name in plant_names:
            capacity = valid_loader.dataset.get_plant_capacity(plant_name)
            capacities.append(capacity)
        capacities = torch.tensor(capacities, device=device, dtype=torch.float32)
        
        # ì˜ˆì¸¡
        outputs = model(historical, forecast, meta, padding_mask)
        predictions = outputs['predictions']
        
        if epoch % 5 == 0 or epoch == 1:
            print("\nâœ… VALID Samples:")
            for i in range(num_samples):
                pred_power = predictions[i] * capacities[i]
                target_power = target[i] * capacities[i]
                
                sample_hours = [0, 6, 12, 18]
                
                print(f"  Plant: {plant_names[i][:15]:15s} | Capacity: {capacities[i].item():6.0f}kW")
                for hour in sample_hours:
                    pred_val = pred_power[hour].item()
                    target_val = target_power[hour].item()
                    error = abs(pred_val - target_val)
                    print(f"    Hour {hour:2d}: Pred={pred_val:6.1f}kW, True={target_val:6.1f}kW, Error={error:6.1f}kW")
        
    print("=" * 60)

# â”€â”€â”€ 4) í‰ê°€ ë©”íŠ¸ë¦­ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def calculate_metrics(predictions: np.ndarray, targets: np.ndarray, 
                     plant_capacities: np.ndarray) -> Dict[str, float]:
    """í‰ê°€ ë©”íŠ¸ë¦­ ê³„ì‚°"""
    # ì´ìš©ë¥ ì„ ì‹¤ì œ ë°œì „ëŸ‰ìœ¼ë¡œ ë³€í™˜
    pred_power = predictions * plant_capacities[:, np.newaxis]
    target_power = targets * plant_capacities[:, np.newaxis]
    
    # ê¸°ë³¸ ë©”íŠ¸ë¦­
    mse = np.mean((pred_power - target_power) ** 2)
    mae = np.mean(np.abs(pred_power - target_power))
    rmse = np.sqrt(mse)
    
    # MAPE (0ì´ ì•„ë‹Œ ê°’ë“¤ì— ëŒ€í•´ì„œë§Œ)
    non_zero_mask = target_power != 0
    mape = np.mean(np.abs((target_power[non_zero_mask] - pred_power[non_zero_mask]) / target_power[non_zero_mask])) * 100
    
    # ì˜ˆì¸¡ ì •í™•ë„ (10% ì˜¤ì°¨ ë‚´)
    relative_error = np.abs((target_power - pred_power) / (target_power + 1e-8))
    accuracy_10 = np.mean(relative_error <= 0.1) * 100
    
    # ë°œì „ëŸ‰ ì˜ˆì¸¡ì œë„ ì˜¤ì°¨ìœ¨ (10% ì´ìƒ êµ¬ê°„)
    threshold_power = plant_capacities * 0.1
    mask = target_power >= threshold_power[:, np.newaxis]
    
    if mask.any():
        masked_error = np.abs(target_power - pred_power) * mask
        error_sum = masked_error.sum(axis=1)
        error_rate = (error_sum / plant_capacities) * 100
        prediction_error_rate = error_rate.mean()
    else:
        prediction_error_rate = 0.0
    
    return {
        'MSE': mse,
        'MAE': mae,
        'RMSE': rmse,
        'MAPE': mape,
        'Accuracy@10%': accuracy_10,
        'PredictionErrorRate': prediction_error_rate
    }

def save_predictions(model, data_loader: DataLoader, 
                    device: str, result_dir: str, split_name: str) -> Dict[str, float]:
    """ì˜ˆì¸¡ê°’ ì €ì¥ ë° ë©”íŠ¸ë¦­ ê³„ì‚°"""
    model.eval()
    
    all_predictions = []
    all_targets = []
    all_targets_original = []
    all_capacities = []
    all_plant_names = []
    
    # ì˜ˆì¸¡ ìˆ˜í–‰
    with torch.no_grad():
        for batch in data_loader:
            historical = batch['historical'].to(device)
            forecast = batch['forecast'].to(device)
            meta = batch['meta'].to(device)
            target = batch['target'].to(device)
            target_original = batch['target_original'].to(device)
            padding_mask = batch['padding_mask'].to(device)
            plant_names = batch['plant_name']
            
            # ë°œì „ì†Œ ìš©ëŸ‰ ì •ë³´
            capacities = []
            for plant_name in plant_names:
                capacity = data_loader.dataset.get_plant_capacity(plant_name)
                capacities.append(capacity)
            capacities = torch.tensor(capacities, device=device, dtype=torch.float32)
            
            # ì˜ˆì¸¡
            outputs = model(historical, forecast, meta, padding_mask)
            predictions = outputs['predictions']
            
            # ë°ì´í„° ìˆ˜ì§‘
            all_predictions.append(predictions.cpu().numpy())
            all_targets.append(target.cpu().numpy())
            all_targets_original.append(target_original.cpu().numpy())
            all_capacities.append(capacities.cpu().numpy())
            all_plant_names.extend(plant_names)
    
    # ë°°ì—´ ê²°í•©
    predictions = np.concatenate(all_predictions, axis=0)
    targets = np.concatenate(all_targets, axis=0)
    targets_original = np.concatenate(all_targets_original, axis=0)
    capacities = np.concatenate(all_capacities, axis=0)
    
    # ì—­ì •ê·œí™”ëœ ì˜ˆì¸¡ê°’ ê³„ì‚°
    predictions_denormalized = predictions * capacities[:, np.newaxis]
    
    # ì˜ˆì¸¡ê°’ ì €ì¥
    np.save(os.path.join(result_dir, f'{split_name}_predictions_normalized.npy'), predictions)
    np.save(os.path.join(result_dir, f'{split_name}_predictions_denormalized.npy'), predictions_denormalized)
    np.save(os.path.join(result_dir, f'{split_name}_targets_normalized.npy'), targets)
    np.save(os.path.join(result_dir, f'{split_name}_targets_original.npy'), targets_original)
    np.save(os.path.join(result_dir, f'{split_name}_capacities.npy'), capacities)
    np.save(os.path.join(result_dir, f'{split_name}_plant_names.npy'), np.array(all_plant_names))
    
    # ë©”íŠ¸ë¦­ ê³„ì‚°
    metrics = calculate_metrics(predictions, targets, capacities)
    
    print(f"âœ… {split_name} predictions saved to {result_dir}")
    print(f"   - Shape: {predictions.shape}")
    print(f"   - Metrics: {metrics}")
    
    return metrics

# â”€â”€â”€ 5) í›ˆë ¨ í•¨ìˆ˜ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def train_epoch(model, train_loader: DataLoader, 
                optimizer: torch.optim.Optimizer, criterion: SolarPredictionLoss, device: str) -> float:
    """í•œ ì—í¬í¬ í›ˆë ¨"""
    model.train()
    total_loss = 0.0
    num_batches = 0
    
    for batch in train_loader:
        # ë°ì´í„° ì´ë™
        historical = batch['historical'].to(device)
        forecast = batch['forecast'].to(device)
        meta = batch['meta'].to(device)
        target = batch['target'].to(device)  # ì •ê·œí™”ëœ ê°’ ì‚¬ìš©
        padding_mask = batch['padding_mask'].to(device)
        plant_names = batch['plant_name']
        
        # ë°œì „ì†Œ ìš©ëŸ‰ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
        capacities = []
        for plant_name in plant_names:
            capacity = train_loader.dataset.get_plant_capacity(plant_name)
            capacities.append(capacity)
        capacities = torch.tensor(capacities, device=device, dtype=torch.float32)
        
        # Forward pass
        optimizer.zero_grad()
        
        outputs = model(historical, forecast, meta, padding_mask)
        predictions = outputs['predictions']
        
        # ì†ì‹¤ ê³„ì‚° - ì´ì œ ëª¨ë“  ê²½ìš°ì— capacityë¥¼ ì „ë‹¬
        loss = criterion(predictions, target, capacities)
        
        # NaN ì²´í¬
        if torch.isnan(loss):
            print(f"NaN loss detected! Skipping batch.")
            continue
        
        # Backward pass
        loss.backward()
        
        # Gradient clipping
        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
        
        optimizer.step()
        
        total_loss += loss.item()
        num_batches += 1
    
    if num_batches == 0:
        return 0.0
    
    return total_loss / num_batches


def validate_epoch(model, valid_loader: DataLoader, 
                  criterion: SolarPredictionLoss, device: str) -> Tuple[float, Dict[str, float]]:
    """ê²€ì¦"""
    model.eval()
    total_loss = 0.0
    all_predictions = []
    all_targets = []
    all_capacities = []
    
    with torch.no_grad():
        for batch in valid_loader:
            # ë°ì´í„° ì´ë™
            historical = batch['historical'].to(device)
            forecast = batch['forecast'].to(device)
            meta = batch['meta'].to(device)
            target = batch['target'].to(device)  # ì •ê·œí™”ëœ ê°’ ì‚¬ìš©
            padding_mask = batch['padding_mask'].to(device)
            plant_names = batch['plant_name']
            
            # ë°œì „ì†Œ ìš©ëŸ‰ ì •ë³´
            capacities = []
            for plant_name in plant_names:
                capacity = valid_loader.dataset.get_plant_capacity(plant_name)
                capacities.append(capacity)
            capacities = torch.tensor(capacities, device=device, dtype=torch.float32)
            
            # Forward pass
            outputs = model(historical, forecast, meta, padding_mask)
            predictions = outputs['predictions']
            
            # ì†ì‹¤ ê³„ì‚° - ì¼ê´€ì„± ìˆê²Œ capacity ì „ë‹¬
            loss = criterion(predictions, target, capacities)
            
            # NaN ì²´í¬
            if not torch.isnan(loss):
                total_loss += loss.item()
                
                # ë©”íŠ¸ë¦­ ê³„ì‚°ì„ ìœ„í•´ ë°ì´í„° ìˆ˜ì§‘
                all_predictions.append(predictions.cpu().numpy())
                all_targets.append(target.cpu().numpy())
                all_capacities.append(capacities.cpu().numpy())
    
    if len(all_predictions) == 0:
        return float('inf'), {}
    
    # ë©”íŠ¸ë¦­ ê³„ì‚°
    predictions = np.concatenate(all_predictions, axis=0)
    targets = np.concatenate(all_targets, axis=0)
    capacities = np.concatenate(all_capacities, axis=0)
    
    metrics = calculate_metrics(predictions, targets, capacities)
    
    return total_loss / len(valid_loader), metrics


# â”€â”€â”€ 6) ë©”ì¸ í›ˆë ¨ í•¨ìˆ˜ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def train_solar_prediction_model(cfg: Dict):
    """íƒœì–‘ê´‘ ë°œì „ëŸ‰ ì˜ˆì¸¡ ëª¨ë¸ í›ˆë ¨"""

    # ê²°ê³¼ ì €ì¥ ë””ë ‰í† ë¦¬ ìƒì„±
    result_dir = create_result_directory(cfg)    
    
    # ì‹œë“œ ì„¤ì •
    set_seed(cfg["training"]["seed"])
    
    # ë””ë°”ì´ìŠ¤ ì„¤ì •
    device = cfg["training"]["device"]
    print(f"Using device: {device}")
    
    # ë°ì´í„°ë¡œë” ìƒì„±
    print("Creating dataloaders...")
    train_loader, valid_loader, test_loader, external_test_loader = create_dataloaders(cfg)
    
    # ëª¨ë¸ ìƒì„±
    model_name = cfg["model"]["name"]
    print(f"Creating {model_name} model...")
    model = create_model_by_name(model_name, cfg, device)


    # ëª¨ë¸ ê°€ì¤‘ì¹˜ ì´ˆê¸°í™” ê°œì„ 
    def init_weights(m):
        if isinstance(m, nn.Linear):
            torch.nn.init.xavier_uniform_(m.weight, gain=0.1)  # ë” ì‘ì€ gain
            if m.bias is not None:
                torch.nn.init.constant_(m.bias, 0)
        elif isinstance(m, nn.LayerNorm):
            torch.nn.init.constant_(m.bias, 0)
            torch.nn.init.constant_(m.weight, 1.0)
    
    model.apply(init_weights)
    print("Applied improved weight initialization")        

    # ì˜µí‹°ë§ˆì´ì € ìƒì„±
    optimizer_class = getattr(torch.optim, cfg["training"]["optimizer"])
    optimizer = optimizer_class(
        model.parameters(), 
        lr=cfg["training"]["learning_rate"],
        **cfg["training"]["optimizer_params"]
    )
    
    # ìŠ¤ì¼€ì¤„ëŸ¬ ìƒì„±
    scheduler_class = getattr(torch.optim.lr_scheduler, cfg["training"]["scheduler"])
    scheduler = scheduler_class(optimizer, **cfg["training"]["scheduler_params"])
    
    # ì†ì‹¤ í•¨ìˆ˜ ìƒì„± - í†µí•©ëœ ë°©ì‹ìœ¼ë¡œ ìˆ˜ì •
    criterion = SolarPredictionLoss(
        loss_type=cfg["training"]["loss"],
        use_capacity_weighting=cfg["training"].get("use_capacity_weighting", True),
        use_denormalized_loss=cfg["training"].get("use_denormalized_loss", True)
    )
    
    # W&B ì´ˆê¸°í™”
    model_name = cfg["model"]["name"]
    run_name = (f"Solar{model_name.capitalize()}_"
            f"{cfg['model']['d_model']}d_"
            f"{cfg['training']['batch_size']}batch_"
            f"{cfg['training']['learning_rate']:.0e}_"
            f"{cfg['training']['loss']}_"
            f"decay{cfg['training']['optimizer_params']['weight_decay']}")

    wandb.init(
        project=cfg["wandb"]["project"],
        entity=cfg["wandb"]["entity"],
        name=run_name,
        config=cfg
    )
    
    # ëª¨ë¸ êµ¬ì¡° ë¡œê¹…
    wandb.watch(model, log="all", log_freq=100)
    
    # ì²« ë²ˆì§¸ ë°°ì¹˜ë¡œ ëª¨ë¸ ì¶œë ¥ í™•ì¸
    print("Checking model output...")
    with torch.no_grad():
        sample_batch = next(iter(train_loader))
        historical = sample_batch['historical'].to(device)[:2]  # ì‘ì€ ë°°ì¹˜ë¡œ í…ŒìŠ¤íŠ¸
        forecast = sample_batch['forecast'].to(device)[:2]
        meta = sample_batch['meta'].to(device)[:2]
        target = sample_batch['target'].to(device)[:2]
        padding_mask = sample_batch['padding_mask'].to(device)[:2]
        
        outputs = model(historical, forecast, meta, padding_mask)
        predictions = outputs['predictions']
        
        print(f"Sample predictions shape: {predictions.shape}")
        print(f"Sample targets shape: {target.shape}")
        print(f"Sample predictions range: [{predictions.min():.4f}, {predictions.max():.4f}]")
        print(f"Sample targets range: [{target.min():.4f}, {target.max():.4f}]")

        # ì—­ì •ê·œí™”ëœ ê°’ë„ í™•ì¸
        capacities = []
        for plant_name in sample_batch['plant_name'][:2]:
            capacity = train_loader.dataset.get_plant_capacity(plant_name)
            capacities.append(capacity)
        capacities = torch.tensor(capacities, device=device, dtype=torch.float32)
        
        pred_power = predictions * capacities.unsqueeze(1)
        target_power = target * capacities.unsqueeze(1)
        print(f"Sample pred_power range: [{pred_power.min():.2f}, {pred_power.max():.2f}] kW")
        print(f"Sample target_power range: [{target_power.min():.2f}, {target_power.max():.2f}] kW")
        print(f"Sample capacities: {capacities.cpu().numpy()}")
        
        # ì†ì‹¤ í•¨ìˆ˜ í…ŒìŠ¤íŠ¸
        test_loss = criterion(predictions, target, capacities)
        print(f"Sample loss: {test_loss.item():.4f}")        
    
    # í›ˆë ¨ ë£¨í”„
    best_val_loss = float('inf')
    patience_counter = 0
    best_metrics = None
    
    print("Starting training...")
    
    for epoch in range(1, cfg["training"]["epochs"] + 1):
        # í›ˆë ¨
        train_loss = train_epoch(model, train_loader, optimizer, criterion, device)
        
        # ê²€ì¦
        val_loss, val_metrics = validate_epoch(model, valid_loader, criterion, device)
        
        print_sample_predictions(model, train_loader, valid_loader, device, epoch, num_samples=3)

        # ìŠ¤ì¼€ì¤„ëŸ¬ ìŠ¤í…
        if cfg["training"]["scheduler"] == "ReduceLROnPlateau":
            scheduler.step(val_loss)
        else:
            scheduler.step()
        
        # ë¡œê¹…
        current_lr = optimizer.param_groups[0]['lr']
        log_data = {
            "epoch": epoch,
            "train_loss": train_loss,
            "val_loss": val_loss,
            "learning_rate": current_lr
        }
        
        if val_metrics:
            log_data.update({f"val_{k}": v for k, v in val_metrics.items()})
        
        wandb.log(log_data)
        
        # ì¶œë ¥
        if epoch % 5 == 0 or epoch == 1:
            if val_metrics:
                print(f"Epoch {epoch:3d} | Train Loss: {train_loss:.4f} | "
                      f"Val Loss: {val_loss:.4f} | Val RMSE: {val_metrics.get('RMSE', 0):.2f} | "
                      f"Val ErrorRate: {val_metrics.get('PredictionErrorRate', 0):.2f}% | "
                      f"LR: {current_lr:.2e}")
            else:
                print(f"Epoch {epoch:3d} | Train Loss: {train_loss:.4f} | "
                      f"Val Loss: {val_loss:.4f} | LR: {current_lr:.2e}")
        
        # Early stopping ë° ëª¨ë¸ ì €ì¥
        if val_loss < best_val_loss:
            best_val_loss = val_loss
            best_metrics = val_metrics.copy() if val_metrics else {}
            patience_counter = 0
            
            # ëª¨ë¸ ì €ì¥
            model_path = os.path.join(result_dir, f"best_solar_model_{run_name}.pth")
            torch.save({
                'epoch': epoch,
                'model_state_dict': model.state_dict(),
                'optimizer_state_dict': optimizer.state_dict(),
                'scheduler_state_dict': scheduler.state_dict(),
                'val_loss': val_loss,
                'val_metrics': val_metrics,
                'config': cfg
            }, model_path)
            
            print(f"ğŸ’¾ Best model saved at epoch {epoch} with val_loss: {val_loss:.4f}")
        else:
            patience_counter += 1
            
            if (cfg["training"]["early_stopping"]["use"] and 
                patience_counter >= cfg["training"]["early_stopping"]["patience"]):
                print(f"â° Early stopping at epoch {epoch}")
                break
    
    # ìµœì¢… í…ŒìŠ¤íŠ¸
    print("\nğŸ§ª Evaluating on test set...")
    model_path = os.path.join(result_dir, f"best_solar_model_{run_name}.pth")
    model.load_state_dict(torch.load(model_path,weights_only=False)['model_state_dict'])

    # í…ŒìŠ¤íŠ¸ ì˜ˆì¸¡ê°’ ì €ì¥ ë° ë©”íŠ¸ë¦­ ê³„ì‚°
    valid_metrics = save_predictions(model, valid_loader, device, result_dir, "valid")
    test_metrics = save_predictions(model, test_loader, device, result_dir, "test")
    external_test_metrics = save_predictions(model, external_test_loader, device, result_dir, "external_test")

    # ì†ì‹¤ê°’ ê³„ì‚°
    test_loss, test_metrics = validate_epoch(model, test_loader, criterion, device)
    external_test_loss, external_test_metrics = validate_epoch(model, external_test_loader, criterion, device)

    # ì„¤ì • íŒŒì¼ë„ ì €ì¥
    config_save_path = os.path.join(result_dir, "config.yaml")
    with open(config_save_path, "w", encoding="utf-8") as f:
        yaml.dump(cfg, f, default_flow_style=False, allow_unicode=True)
    print(f"âœ… Config saved to {config_save_path}")    
    
    # ìµœì¢… ê²°ê³¼ ë¡œê¹…
    final_results = {
        "final_test_loss": test_loss,
        "final_external_test_loss": external_test_loss,
    }
    
    if test_metrics:
        final_results.update({f"test_{k}": v for k, v in test_metrics.items()})
    if external_test_metrics:
        final_results.update({f"external_test_{k}": v for k, v in external_test_metrics.items()})
    
    wandb.log(final_results)
    
    print(f"\nğŸ‰ Training completed!")
    print(f"Best validation loss: {best_val_loss:.4f}")
    print(f"Test metrics: {test_metrics}")
    print(f"External test metrics: {external_test_metrics}")
    
    wandb.finish()
    
    return model, best_metrics, test_metrics, external_test_metrics


# â”€â”€â”€ 7) ì„¤ì • íŒŒì¼ ìƒì„± â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def create_result_directory(cfg: Dict) -> str:
    """ê²°ê³¼ ì €ì¥ ë””ë ‰í† ë¦¬ ìƒì„±"""
    current_time_str = datetime.now().strftime("%m%d_%H%M%S") 

    # ë””ë ‰í† ë¦¬ ì´ë¦„ ìƒì„±
    dir_name = (f"{cfg['model']['name']}_"
                f"{cfg['model']['d_model']}d_model_"
                f"{cfg['model']['num_encoder_layers']}n_layers_"
                f"{cfg['training']['learning_rate']}lr_"
                f"{cfg['training']['batch_size']}batch_"
                f"{cfg['training']['loss']}_"
                f"{current_time_str}"
                )
    
    result_dir = os.path.join("./result", dir_name)
    os.makedirs(result_dir, exist_ok=True)
    
    print(f"Results will be saved to: {result_dir}")
    return result_dir

def create_config_file():
    """ì„¤ì • íŒŒì¼ ìƒì„±"""
    config = {
        "data": {
            "root_dir": "/home/local/soominchung_991007/EF/code/tmp/data/normalized_forecast_modeling_data",
            "meta_path": "/home/local/soominchung_991007/EF/code/tmp/data/solar_energy/meta_data.csv"
        },
        "model": {
            "name": "simple", # "name": "simple",  # ğŸ‘ˆ ì—¬ê¸° ì¶”ê°€: "transformer" ë˜ëŠ” "simple"
            "historical_features": 54,
            "weather_features": 5,
            "plant_features": 10,
            "d_model": 64, #256
            "nhead": 4,
            "num_encoder_layers": 1
        },
        "training": {
            "seed": 42,
            "device": "cuda:6",
            "learning_rate": 5e-5,
            "batch_size": 64,
            "epochs": 100,
            "optimizer": "AdamW",
            "optimizer_params": {
                "weight_decay": 0.1,
                "eps": 1e-8
            },
            "scheduler": "ReduceLROnPlateau",
            "scheduler_params": {
                "mode": "min",
                "factor": 0.3,
                "patience": 10,
                "min_lr": 1e-6
            },
            "loss": "MSELoss",  # or "L1Loss", "SmoothL1Loss", "PredictionErrorRate"
            "use_capacity_weighting": False,
            "use_denormalized_loss": True,  # ì—­ì •ê·œí™” ì†ì‹¤ ì‚¬ìš©
            "early_stopping": {
                "use": True,
                "patience": 10
            }
        },
        "wandb": {
            "api_key": "a8af4997e63da6343549fc1212570a2d1c274303",
            "entity": "soomin200-seoul-national-university",
            "project": "solar_prediction_multimodal"
        }
    }
    
    with open("solar_config.yaml", "w", encoding="utf-8") as f:
        yaml.dump(config, f, default_flow_style=False, allow_unicode=True)
    
    print("Created solar_config.yaml")


# â”€â”€â”€ 8) ë©”ì¸ ì‹¤í–‰ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

'''
## í•˜ë‚˜ë§Œ ëŒë¦´ ë•Œ
# ì„¤ì • íŒŒì¼ ìƒì„± (ì²˜ìŒ ì‹¤í–‰ ì‹œ)
#if not os.path.exists("solar_config.yaml"):
create_config_file()

# ì„¤ì • ë¡œë“œ
cfg = load_config("solar_config.yaml")

# W&B ì„¤ì •
setup_wandb(cfg)

# í›ˆë ¨ ì‹¤í–‰
model, best_metrics, test_metrics, external_test_metrics = train_solar_prediction_model(cfg)

print("\nğŸŒ Solar Power Prediction Training Completed! ğŸŒ")
'''

# ===== ì—¬ëŸ¬ ì‹¤í—˜ ìë™ ì‹¤í–‰ =====

# ê¸°ë³¸ ì„¤ì • ìƒì„±
create_config_file()
base_config = load_config("solar_config.yaml")
'''
experiments = [
    # ===== Simple ëª¨ë¸ (ê¸°ë³¸ ì„±ëŠ¥ í™•ì¸) =====
    # (ì‹¤í—˜ëª…, model_name, learning_rate, batch_size, d_model, loss, weight_decay)
    ("simple_baseline", "simple", 1e-4, 32, 64, "MSELoss", 0.1),
    ("simple_policy_per", "simple", 1e-4, 32, 64, "PredictionErrorRate", 0.1),
    ("simple_combined", "simple", 1e-4, 32, 64, "MSE_PER_Weighted", 0.1),
    
    # ===== Transformer vs Simple ì§ì ‘ ë¹„êµ (ë™ì¼ ì¡°ê±´) =====
    ("transformer_baseline", "transformer", 5e-5, 32, 64, "MSELoss", 0.1),
    ("transformer_policy_per", "transformer", 5e-5, 32, 64, "PredictionErrorRate", 0.1),
    ("transformer_combined", "transformer", 5e-5, 32, 64, "MSE_PER_Weighted", 0.1),
    
    # ===== í•™ìŠµë¥  ì‹¤í—˜ (ê° ëª¨ë¸ë³„ ìµœì í™”) =====
    ("simple_higher_lr", "simple", 2e-4, 32, 64, "MSE_PER_Weighted", 0.1),
    ("transformer_higher_lr", "transformer", 1e-4, 32, 64, "MSE_PER_Weighted", 0.1),
    
    # ===== ì‘ì€ ëª¨ë¸ í¬ê¸° ì‹¤í—˜ =====
    ("simple_tiny", "simple", 1e-4, 32, 32, "MSE_PER_Weighted", 0.05),
    ("transformer_tiny", "transformer", 5e-5, 32, 32, "MSE_PER_Weighted", 0.05),
    
    # ===== ì¤‘ê°„ í¬ê¸° ëª¨ë¸ (ì¢‹ì€ ê²°ê³¼ ë‚˜ì˜¤ë©´) =====
    ("simple_medium", "simple", 1e-4, 32, 128, "MSE_PER_Weighted", 0.05),
    ("transformer_medium", "transformer", 5e-5, 32, 128, "MSE_PER_Weighted", 0.05),
    
    # ===== ë°°ì¹˜ í¬ê¸° ì‹¤í—˜ =====
    ("simple_large_batch", "simple", 5e-5, 64, 64, "MSE_PER_Weighted", 0.1),
    ("transformer_large_batch", "transformer", 3e-5, 64, 64, "MSE_PER_Weighted", 0.1),
    
    # ===== ìµœì¢… ë„ì „ (í° ëª¨ë¸) =====
    ("simple_large", "simple", 8e-5, 16, 256, "MSE_PER_Weighted", 0.03),
    ("transformer_large", "transformer", 3e-5, 16, 256, "MSE_PER_Weighted", 0.03),
]
'''

experiments = [
    # === nhead ì‹¤í—˜ (layers=2 ê³ ì •) ===
    ("transformer_256d_2head_2layer_mse", "transformer", 3e-5, 16, 256, "MSELoss", 0.03, 2, 2),
    ("transformer_256d_2head_2layer_weighted", "transformer", 3e-5, 16, 256, "MSE_PER_Weighted", 0.03, 2, 2),
    
    ("transformer_256d_4head_2layer_mse", "transformer", 3e-5, 16, 256, "MSELoss", 0.03, 4, 2),
    ("transformer_256d_4head_2layer_weighted", "transformer", 3e-5, 16, 256, "MSE_PER_Weighted", 0.03, 4, 2),
    
    ("transformer_256d_6head_2layer_mse", "transformer", 3e-5, 16, 256, "MSELoss", 0.03, 6, 2),
    
    #####
    ("transformer_256d_6head_2layer_weighted", "transformer", 3e-5, 16, 256, "MSE_PER_Weighted", 0.03, 6, 2),
    
    # === num_encoder_layers ì‹¤í—˜ (nhead=4 ê³ ì •) ===
    ("transformer_256d_4head_1layer_mse", "transformer", 3e-5, 16, 256, "MSELoss", 0.03, 4, 1),
    ("transformer_256d_4head_1layer_weighted", "transformer", 3e-5, 16, 256, "MSE_PER_Weighted", 0.03, 4, 1),
    
    ("transformer_256d_4head_3layer_mse", "transformer", 3e-5, 16, 256, "MSELoss", 0.03, 4, 3),
    ("transformer_256d_4head_3layer_weighted", "transformer", 3e-5, 16, 256, "MSE_PER_Weighted", 0.03, 4, 3),
]
import sys

# ëª…ë ¹í–‰ ì¸ìë¡œ ì‹¤í—˜ ê·¸ë£¹ê³¼ GPU ë°›ê¸°
'''exp_group = int(sys.argv[1]) if len(sys.argv) > 1 else 1
gpu_id = int(sys.argv[2]) if len(sys.argv) > 2 else 6

# base_configì˜ GPU ì„¤ì • ì—…ë°ì´íŠ¸
base_config["training"]["device"] = f"cuda:{gpu_id}"

# ì „ì²´ ì‹¤í—˜ì„ ë°˜ìœ¼ë¡œ ë‚˜ëˆ„ê¸°
total_experiments = len(experiments)
mid_point = total_experiments // 2

if exp_group == 1:
    experiments = experiments[:mid_point]  # ì²« ë²ˆì§¸ ì ˆë°˜
    print(f"ğŸš€ Running experiments 1-{mid_point} (Group 1) on GPU {gpu_id}")
else:
    experiments = experiments[mid_point:]  # ë‘ ë²ˆì§¸ ì ˆë°˜
    print(f"ğŸš€ Running experiments {mid_point+1}-{total_experiments} (Group 2) on GPU {gpu_id}")
    
'''
# ëª…ë ¹í–‰ ì¸ìë¡œ GPUë§Œ ë°›ê¸° (ê·¸ë£¹ ë¶„í•  ì œê±°)
gpu_id = int(sys.argv[1]) if len(sys.argv) > 1 else 6

# base_configì˜ GPU ì„¤ì • ì—…ë°ì´íŠ¸
base_config["training"]["device"] = f"cuda:{gpu_id}"

# ê·¸ë£¹ ë¶„í•  ì½”ë“œ ì „ì²´ ì£¼ì„ì²˜ë¦¬ ë˜ëŠ” ì‚­ì œ
# exp_group = int(sys.argv[1]) if len(sys.argv) > 1 else 1
# total_experiments = len(experiments)
# mid_point = total_experiments // 2
# if exp_group == 1:
#     experiments = experiments[:mid_point]
# else:
#     experiments = experiments[mid_point:]

# ì „ì²´ ì‹¤í—˜ ì‹¤í–‰
print(f"ğŸš€ Running all {len(experiments)} experiments on GPU {gpu_id}")

print(f"ğŸš€ Starting {len(experiments)} experiments...")
print(f"ğŸ“‹ ì‹¤í—˜ êµ¬ì„±:")
print(f"  - Simple ëª¨ë¸: {sum(1 for exp in experiments if exp[1] == 'simple')}ê°œ")
print(f"  - Transformer ëª¨ë¸: {sum(1 for exp in experiments if exp[1] == 'transformer')}ê°œ")
print(f"  - ì†ì‹¤ í•¨ìˆ˜ë³„: MSE({sum(1 for exp in experiments if exp[4] == 'MSELoss')}), PER({sum(1 for exp in experiments if exp[4] == 'PredictionErrorRate')}), Combined({sum(1 for exp in experiments if exp[4] == 'MSE_PER_Weighted')})")

#for i, (exp_name, model_name, lr, batch, d_model, loss, decay) in enumerate(experiments, 1):
for i, (exp_name, model_name, lr, batch, d_model, loss, decay, nhead, num_layers) in enumerate(experiments, 1):
    print(f"\n{'='*60}")
    print(f"ğŸš€ Experiment {i}/{len(experiments)}: {exp_name}")
    print(f"â° Time: {datetime.now().strftime('%H:%M:%S')}")
    print(f"ğŸ¤– Model: {model_name}")
    print(f"âš™ï¸  LR: {lr}, Batch: {batch}, d_model: {d_model}")
    print(f"ğŸ“Š Loss: {loss}, Weight_decay: {decay}")
    print(f"{'='*60}")
    
    try:
        # ì„¤ì • ë³µì‚¬ ë° ì—…ë°ì´íŠ¸
        config = base_config.copy()
        config["model"]["name"] = model_name  # ğŸ‘ˆ ëª¨ë¸ëª… ì„¤ì •
        config["training"]["learning_rate"] = lr
        config["training"]["batch_size"] = batch
        config["model"]["d_model"] = d_model
        config["training"]["loss"] = loss
        config["training"]["optimizer_params"]["weight_decay"] = decay
        config["wandb"]["project"] = f"solar_prediction_comparison"
        config["experiment_name"] = exp_name
        config["model"]["nhead"] = nhead
        config["model"]["num_encoder_layers"] = num_layers
                
        # Transformer ëª¨ë¸ íŠ¹ë³„ ì„¤ì •
        if model_name == "transformer":
            config["model"]["num_encoder_layers"] = 2  # TransformerëŠ” layer 2ê°œ
        else:
            config["model"]["num_encoder_layers"] = 1  # Simpleì€ ì˜ë¯¸ì—†ì§€ë§Œ ìœ ì§€
        
        # ì„¤ì • íŒŒì¼ ì €ì¥
        config_path = f"solar_config_{exp_name}.yaml"
        with open(config_path, "w", encoding="utf-8") as f:
            yaml.dump(config, f, default_flow_style=False, allow_unicode=True)
        
        # ì‹¤í—˜ ì‹¤í–‰
        cfg = load_config(config_path)
        setup_wandb(cfg)
        model, best_metrics, test_metrics, external_test_metrics = train_solar_prediction_model(cfg)
        
        print(f"âœ… Experiment {exp_name} SUCCESS!")
        print(f"ğŸ“Š Best Val Metrics: {best_metrics}")
        print(f"ğŸ“Š Test PredictionErrorRate: {test_metrics.get('PredictionErrorRate', 'N/A'):.1f}%")
        
        # ê°„ë‹¨í•œ ì„±ëŠ¥ ê¸°ë¡ (ë‚˜ì¤‘ì— ë¹„êµìš©)
        with open("experiment_results.txt", "a") as f:
            val_per = best_metrics.get('PredictionErrorRate', 999) if best_metrics else 999
            test_per = test_metrics.get('PredictionErrorRate', 999) if test_metrics else 999
            f.write(f"{exp_name},{model_name},{d_model},{loss},{val_per:.1f},{test_per:.1f}\n")
        
    except Exception as e:
        print(f"âŒ Experiment {exp_name} FAILED: {e}")
        import traceback
        traceback.print_exc()
        
        # ì‹¤íŒ¨ ê¸°ë¡
        with open("experiment_results.txt", "a") as f:
            f.write(f"{exp_name},{model_name},{d_model},{loss},FAILED,FAILED\n")
    
    finally:
        # GPU ë©”ëª¨ë¦¬ ì •ë¦¬
        import torch
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
        
        # ë§ˆì§€ë§‰ ì‹¤í—˜ì´ ì•„ë‹ˆë©´ ëŒ€ê¸° (ì‹œê°„ ì¡°ì •)
        if i < len(experiments):
            wait_time = 20 if d_model <= 64 else 30  # ì‘ì€ ëª¨ë¸ì€ ì§§ê²Œ ëŒ€ê¸°
            print(f"ğŸ˜´ Waiting {wait_time} seconds for GPU cooldown...")
            import time
            time.sleep(wait_time)

print(f"\nğŸ† All {len(experiments)} experiments completed!")

# ê²°ê³¼ ìš”ì•½ ì¶œë ¥
try:
    print(f"\nğŸ“Š ì‹¤í—˜ ê²°ê³¼ ìš”ì•½:")
    with open("experiment_results.txt", "r") as f:
        lines = f.readlines()
        for line in lines[-len(experiments):]:  # ë°©ê¸ˆ ì‹¤í–‰í•œ ì‹¤í—˜ë“¤ë§Œ
            parts = line.strip().split(",")
            if len(parts) >= 6:
                exp_name, model_name, d_model, loss, val_per, test_per = parts[:6]
                print(f"  {exp_name:20s} | {model_name:11s} | d{d_model:3s} | {loss:15s} | Val: {val_per:>6s}% | Test: {test_per:>6s}%")
except:
    print("ê²°ê³¼ ìš”ì•½ íŒŒì¼ ì½ê¸° ì‹¤íŒ¨")

print("\nğŸŒ Solar Power Prediction Training Completed! ğŸŒ")
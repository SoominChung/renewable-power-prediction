{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30b73af6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample file: 삼척소내_2.parquet\n",
      "Columns in 삼척소내_2.parquet: Index(['date', '호기', '총량(kw)', '평균(kw)', '최대(kw)', '최소(kw)', '최대(시간별_kw)',\n",
      "       '최소(시간별_kw)', 'value', 'time', 'temperature', 'humidity', 'rn', 'ws',\n",
      "       'wd', 'pv', 'pa', 'ps', 'ss', 'icsr', 'dc10Tca', 'dc10LmcsCa', 'lcsCh',\n",
      "       'vs', 'ts', 'sunrise', 'sunset', 'SO2', 'CO', 'O3', 'NO2', 'PM10',\n",
      "       'PM25', '미세먼지', '초미세먼지'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd \n",
    "\n",
    "META_DATA_PATH = '../data/solar_energy/meta_data.csv' \n",
    "final_data_path = '../data/concat_data'\n",
    "meta_data = pd.read_csv(META_DATA_PATH)\n",
    "files = os.listdir(final_data_path)\n",
    "sample = files[2]\n",
    "print(f\"Sample file: {sample}\")\n",
    "file = pd.read_parquet(os.path.join(final_data_path, sample))\n",
    "columns = file.columns\n",
    "print(f\"Columns in {sample}: {columns}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "b7960838",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting plant information...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 46/46 [00:00<00:00, 60.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 46 plants\n",
      "Processing each plant's data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 46/46 [00:05<00:00,  7.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 46 plants\n",
      "Feature columns (40): ['총량(kw)', '평균(kw)', '최대(kw)', '최소(kw)', '최대(시간별_kw)', '최소(시간별_kw)', 'temperature', 'humidity', 'rn', 'ws']...\n",
      "Target column: value\n",
      "Plant 익산 다송리 has insufficient data: 0 days\n",
      "Prepared time series data for 45 plants\n",
      "Total samples: 36068\n",
      "Plant 부산운동장: 1825 samples\n",
      "  X shape: (24, 40), y shape: (24,)\n",
      "Plant 화촌주민참여형: 517 samples\n",
      "  X shape: (24, 40), y shape: (24,)\n",
      "Plant 삼척소내_2: 882 samples\n",
      "  X shape: (24, 40), y shape: (24,)\n",
      "Plant 영월본부: 1460 samples\n",
      "  X shape: (24, 40), y shape: (24,)\n",
      "Plant 하동변전소: 517 samples\n",
      "  X shape: (24, 40), y shape: (24,)\n",
      "Plant 신인천 북측부지: 756 samples\n",
      "  X shape: (24, 40), y shape: (24,)\n",
      "Plant 삼척소내_3: 882 samples\n",
      "  X shape: (24, 40), y shape: (24,)\n",
      "Plant 와산리: 107 samples\n",
      "  X shape: (24, 40), y shape: (24,)\n",
      "Plant 하동본부_1: 517 samples\n",
      "  X shape: (24, 40), y shape: (24,)\n",
      "Plant 하동정수장: 517 samples\n",
      "  X shape: (24, 40), y shape: (24,)\n",
      "Plant 인천수산정수장: 882 samples\n",
      "  X shape: (24, 40), y shape: (24,)\n",
      "Plant 삼척소내_1: 882 samples\n",
      "  X shape: (24, 40), y shape: (24,)\n",
      "Plant 부산복합자재창고: 2921 samples\n",
      "  X shape: (24, 40), y shape: (24,)\n",
      "Plant 신인천해수구취수구: 882 samples\n",
      "  X shape: (24, 40), y shape: (24,)\n",
      "Plant 하동본부_3: 517 samples\n",
      "  X shape: (24, 40), y shape: (24,)\n",
      "Plant 하동하수처리장: 517 samples\n",
      "  X shape: (24, 40), y shape: (24,)\n",
      "Plant 영월철도부지: 1460 samples\n",
      "  X shape: (24, 40), y shape: (24,)\n",
      "Plant 신인천본관주차장: 369 samples\n",
      "  X shape: (24, 40), y shape: (24,)\n",
      "Plant 행원소수력: 333 samples\n",
      "  X shape: (24, 40), y shape: (24,)\n",
      "Plant 하동본부_2: 517 samples\n",
      "  X shape: (24, 40), y shape: (24,)\n",
      "Plant 하동보건소: 517 samples\n",
      "  X shape: (24, 40), y shape: (24,)\n",
      "Plant 세화리: 333 samples\n",
      "  X shape: (24, 40), y shape: (24,)\n",
      "Plant 이천D(백사면B): 690 samples\n",
      "  X shape: (24, 40), y shape: (24,)\n",
      "Plant 위미2리: 333 samples\n",
      "  X shape: (24, 40), y shape: (24,)\n",
      "Plant 남제주소내: 333 samples\n",
      "  X shape: (24, 40), y shape: (24,)\n",
      "Plant 송당리: 333 samples\n",
      "  X shape: (24, 40), y shape: (24,)\n",
      "Plant 신인천전망대: 882 samples\n",
      "  X shape: (24, 40), y shape: (24,)\n",
      "Plant 부산신항: 838 samples\n",
      "  X shape: (24, 40), y shape: (24,)\n",
      "Plant 신풍리: 333 samples\n",
      "  X shape: (24, 40), y shape: (24,)\n",
      "Plant 하동본부_6: 517 samples\n",
      "  X shape: (24, 40), y shape: (24,)\n",
      "Plant 삼척소내_4: 882 samples\n",
      "  X shape: (24, 40), y shape: (24,)\n",
      "Plant 부산역선상주차장: 868 samples\n",
      "  X shape: (24, 40), y shape: (24,)\n",
      "Plant 부산본부_1: 2920 samples\n",
      "  X shape: (24, 40), y shape: (24,)\n",
      "Plant 무릉리: 333 samples\n",
      "  X shape: (24, 40), y shape: (24,)\n",
      "Plant 이천시 백사면A: 495 samples\n",
      "  X shape: (24, 40), y shape: (24,)\n",
      "Plant 감우리: 513 samples\n",
      "  X shape: (24, 40), y shape: (24,)\n",
      "Plant 하동공설운동장: 517 samples\n",
      "  X shape: (24, 40), y shape: (24,)\n",
      "Plant 신인천 1_2단계 주차장: 756 samples\n",
      "  X shape: (24, 40), y shape: (24,)\n",
      "Plant 하동본부_4: 517 samples\n",
      "  X shape: (24, 40), y shape: (24,)\n",
      "Plant 부산수처리장: 1825 samples\n",
      "  X shape: (24, 40), y shape: (24,)\n",
      "Plant 신인천 주차장: 882 samples\n",
      "  X shape: (24, 40), y shape: (24,)\n",
      "Plant 신인천소내: 882 samples\n",
      "  X shape: (24, 40), y shape: (24,)\n",
      "Plant 용수리: 333 samples\n",
      "  X shape: (24, 40), y shape: (24,)\n",
      "Plant 하동본부_5: 517 samples\n",
      "  X shape: (24, 40), y shape: (24,)\n",
      "Plant 부산본부_2: 1459 samples\n",
      "  X shape: (24, 40), y shape: (24,)\n",
      "External test plants: ['와산리', '행원소수력', '세화리', '위미2리', '남제주소내', '송당리', '신풍리', '무릉리', '용수리']\n",
      "Regular split plants: ['신인천본관주차장', '이천시 백사면A', '감우리', '화촌주민참여형', '하동변전소', '하동본부_1', '하동정수장', '하동본부_3', '하동하수처리장', '하동본부_2', '하동보건소', '하동본부_6', '하동공설운동장', '하동본부_4', '하동본부_5', '이천D(백사면B)', '신인천 북측부지', '신인천 1_2단계 주차장', '부산신항', '부산역선상주차장', '삼척소내_2', '삼척소내_3', '인천수산정수장', '삼척소내_1', '신인천해수구취수구', '신인천전망대', '삼척소내_4', '신인천 주차장', '신인천소내', '영월본부', '영월철도부지', '부산본부_2', '부산운동장', '부산수처리장', '부산복합자재창고', '부산본부_1']\n",
      "\n",
      "TRAIN SET:\n",
      "  Samples: 23285\n",
      "  X shape: (23285, 24, 40)\n",
      "  y shape: (23285, 24)\n",
      "  Plants: 36\n",
      "  Plant distribution: {'신인천본관주차장': 258, '이천시 백사면A': 346, '감우리': 359, '화촌주민참여형': 361, '하동변전소': 361, '하동본부_1': 361, '하동정수장': 361, '하동본부_3': 361, '하동하수처리장': 361, '하동본부_2': 361, '하동보건소': 361, '하동본부_6': 361, '하동공설운동장': 361, '하동본부_4': 361, '하동본부_5': 361, '이천D(백사면B)': 482, '신인천 북측부지': 529, '신인천 1_2단계 주차장': 529, '부산신항': 586, '부산역선상주차장': 607, '삼척소내_2': 617, '삼척소내_3': 617, '인천수산정수장': 617, '삼척소내_1': 617, '신인천해수구취수구': 617, '신인천전망대': 617, '삼척소내_4': 617, '신인천 주차장': 617, '신인천소내': 617, '영월본부': 1021, '영월철도부지': 1021, '부산본부_2': 1021, '부산운동장': 1277, '부산수처리장': 1277, '부산복합자재창고': 2044, '부산본부_1': 2043}\n",
      "\n",
      "VALID SET:\n",
      "  Samples: 5000\n",
      "  X shape: (5000, 24, 40)\n",
      "  y shape: (5000, 24)\n",
      "  Plants: 36\n",
      "  Plant distribution: {'신인천본관주차장': 55, '이천시 백사면A': 74, '감우리': 77, '화촌주민참여형': 78, '하동변전소': 78, '하동본부_1': 78, '하동정수장': 78, '하동본부_3': 78, '하동하수처리장': 78, '하동본부_2': 78, '하동보건소': 78, '하동본부_6': 78, '하동공설운동장': 78, '하동본부_4': 78, '하동본부_5': 78, '이천D(백사면B)': 104, '신인천 북측부지': 113, '신인천 1_2단계 주차장': 113, '부산신항': 126, '부산역선상주차장': 130, '삼척소내_2': 132, '삼척소내_3': 132, '인천수산정수장': 132, '삼척소내_1': 132, '신인천해수구취수구': 132, '신인천전망대': 132, '삼척소내_4': 132, '신인천 주차장': 132, '신인천소내': 132, '영월본부': 220, '영월철도부지': 220, '부산본부_2': 219, '부산운동장': 274, '부산수처리장': 274, '부산복합자재창고': 438, '부산본부_1': 439}\n",
      "\n",
      "TEST SET:\n",
      "  Samples: 5012\n",
      "  X shape: (5012, 24, 40)\n",
      "  y shape: (5012, 24)\n",
      "  Plants: 36\n",
      "  Plant distribution: {'신인천본관주차장': 56, '이천시 백사면A': 75, '감우리': 77, '화촌주민참여형': 78, '하동변전소': 78, '하동본부_1': 78, '하동정수장': 78, '하동본부_3': 78, '하동하수처리장': 78, '하동본부_2': 78, '하동보건소': 78, '하동본부_6': 78, '하동공설운동장': 78, '하동본부_4': 78, '하동본부_5': 78, '이천D(백사면B)': 104, '신인천 북측부지': 114, '신인천 1_2단계 주차장': 114, '부산신항': 126, '부산역선상주차장': 131, '삼척소내_2': 133, '삼척소내_3': 133, '인천수산정수장': 133, '삼척소내_1': 133, '신인천해수구취수구': 133, '신인천전망대': 133, '삼척소내_4': 133, '신인천 주차장': 133, '신인천소내': 133, '영월본부': 219, '영월철도부지': 219, '부산본부_2': 219, '부산운동장': 274, '부산수처리장': 274, '부산복합자재창고': 439, '부산본부_1': 438}\n",
      "\n",
      "EXTERNAL_TEST SET:\n",
      "  Samples: 2771\n",
      "  X shape: (2771, 24, 40)\n",
      "  y shape: (2771, 24)\n",
      "  Plants: 9\n",
      "  Plant distribution: {'와산리': 107, '행원소수력': 333, '세화리': 333, '위미2리': 333, '남제주소내': 333, '송당리': 333, '신풍리': 333, '무릉리': 333, '용수리': 333}\n",
      "\n",
      "Time series data saved to ../data/time_series\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "import datetime\n",
    "from tqdm import tqdm\n",
    "\n",
    "def load_and_preprocess_data(meta_data_path, data_dir):\n",
    "    \"\"\"\n",
    "    Load and preprocess all solar plant data files\n",
    "    \n",
    "    Args:\n",
    "        meta_data_path: Path to metadata CSV\n",
    "        data_dir: Directory containing parquet files\n",
    "    \n",
    "    Returns:\n",
    "        processed_data: Dictionary of preprocessed dataframes by plant\n",
    "        feature_cols: List of feature columns\n",
    "        plants_info: Dictionary with information about each plant\n",
    "    \"\"\"\n",
    "    # Load metadata\n",
    "    meta_data = pd.read_csv(meta_data_path)\n",
    "    \n",
    "    # Get all files\n",
    "    files = os.listdir(data_dir)\n",
    "    \n",
    "    # Initialize dictionaries to store processed data\n",
    "    processed_data = {}\n",
    "    plants_info = {}\n",
    "    \n",
    "    # Define column types\n",
    "    time_cols = ['date', 'time', 'sunrise', 'sunset']\n",
    "    \n",
    "    # Weather/environment columns (numerical)\n",
    "    weather_numeric_cols = [\n",
    "        'temperature', 'humidity', 'ws', 'pv', 'pa', 'ps', \n",
    "        'ss', 'dc10Tca', 'dc10LmcsCa', 'vs', 'ts'\n",
    "    ]\n",
    "    \n",
    "    # Circular features (need sin/cos transformation)\n",
    "    circular_cols = ['wd']\n",
    "    \n",
    "    # Weather columns with special imputation (zero)\n",
    "    weather_zero_impute_cols = ['rn', 'icsr']\n",
    "    \n",
    "    # Categorical weather columns\n",
    "    categorical_cols = ['lcsCh', '미세먼지', '초미세먼지']\n",
    "    \n",
    "    # Air quality columns (numerical)\n",
    "    air_numeric_cols = ['SO2', 'CO', 'O3', 'NO2', 'PM10', 'PM25']\n",
    "    \n",
    "    # Solar plant output columns \n",
    "    plant_output_cols = ['총량(kw)', '평균(kw)', '최대(kw)', '최소(kw)', '최대(시간별_kw)', '최소(시간별_kw)', 'value']\n",
    "    \n",
    "    # Target column\n",
    "    target_col = 'value'\n",
    "    \n",
    "    # All numeric columns for imputation\n",
    "    all_numeric_cols = weather_numeric_cols + air_numeric_cols + plant_output_cols\n",
    "    \n",
    "    # Analyze all files to collect plant information\n",
    "    print(\"Collecting plant information...\")\n",
    "    for file_name in tqdm(files):\n",
    "        plant_id = file_name.split('.')[0]  # Assuming filename contains plant ID\n",
    "        file_path = os.path.join(data_dir, file_name)\n",
    "        \n",
    "        # Load data - exclude '호기' column from the beginning\n",
    "        df = pd.read_parquet(file_path)\n",
    "        if '호기' in df.columns:\n",
    "            df = df.drop(columns=['호기'])\n",
    "        \n",
    "        # Extract date range information\n",
    "        if 'date' in df.columns:\n",
    "            min_date = df['date'].min()\n",
    "            max_date = df['date'].max()\n",
    "            date_range = f\"{min_date} ~ {max_date}\"\n",
    "        else:\n",
    "            date_range = \"Unknown\"\n",
    "        \n",
    "        # Store plant information\n",
    "        plants_info[plant_id] = {\n",
    "            'file_name': file_name,\n",
    "            'date_range': date_range,\n",
    "            'data_length': len(df),\n",
    "            'columns': list(df.columns)\n",
    "        }\n",
    "    \n",
    "    print(f\"Found {len(plants_info)} plants\")\n",
    "    \n",
    "    # Process each file\n",
    "    print(\"Processing each plant's data...\")\n",
    "    for file_name in tqdm(files):\n",
    "        plant_id = file_name.split('.')[0]\n",
    "        file_path = os.path.join(data_dir, file_name)\n",
    "        \n",
    "        # Load data - exclude '호기' column\n",
    "        df = pd.read_parquet(file_path)\n",
    "        if '호기' in df.columns:\n",
    "            df = df.drop(columns=['호기'])\n",
    "        \n",
    "        # Process datetime columns - 시간 관련 특성 생성\n",
    "        df = process_time_features(df, time_cols)\n",
    "        \n",
    "        # Process special weather features (wd, etc.)\n",
    "        df = process_weather_features(df, circular_cols)\n",
    "        \n",
    "        # Handle missing values according to the provided strategy\n",
    "        df = handle_missing_values(\n",
    "            df, \n",
    "            numeric_cols=all_numeric_cols,\n",
    "            zero_impute_cols=weather_zero_impute_cols,\n",
    "            categorical_cols=categorical_cols,\n",
    "            circular_cols=['wd_sin', 'wd_cos']\n",
    "        )\n",
    "        \n",
    "        # Save to dictionary\n",
    "        processed_data[plant_id] = df\n",
    "    \n",
    "    # Define feature columns\n",
    "    if len(processed_data) > 0:\n",
    "        sample_df = processed_data[list(processed_data.keys())[0]]\n",
    "        \n",
    "        # Target column is the hourly generation ('value')\n",
    "        target_col = 'value'\n",
    "        \n",
    "        # Excluded columns (raw time columns and derived intermediate columns)\n",
    "        excluded_cols = time_cols + ['datetime', 'hour', 'day_of_year', 'month', target_col]\n",
    "        \n",
    "        # All features except target and excluded time columns\n",
    "        feature_cols = [col for col in sample_df.columns if col not in excluded_cols]\n",
    "    else:\n",
    "        feature_cols = []\n",
    "    \n",
    "    return processed_data, feature_cols, target_col, plants_info\n",
    "\n",
    "def process_time_features(df, time_cols):\n",
    "    \"\"\"Process time-related columns and extract useful features\"\"\"\n",
    "    \n",
    "    # Convert date and time to datetime if they exist\n",
    "    if 'date' in df.columns and 'time' in df.columns:\n",
    "        # Ensure date and time are string type\n",
    "        df['date'] = df['date'].astype(str)\n",
    "        df['time'] = df['time'].astype(str)\n",
    "        \n",
    "        # Create datetime column\n",
    "        df['datetime'] = pd.to_datetime(df['date'] + ' ' + df['time'], errors='coerce')\n",
    "        df['hour'] = df['datetime'].dt.hour\n",
    "        \n",
    "        # Create cyclical time features\n",
    "        df['time_hour_sin'] = np.sin(2 * np.pi * df['hour']/24)\n",
    "        df['time_hour_cos'] = np.cos(2 * np.pi * df['hour']/24)\n",
    "        \n",
    "        # Day of year (for seasonal patterns)\n",
    "        df['day_of_year'] = df['datetime'].dt.dayofyear\n",
    "        df['time_day_sin'] = np.sin(2 * np.pi * df['day_of_year']/365)\n",
    "        df['time_day_cos'] = np.cos(2 * np.pi * df['day_of_year']/365)\n",
    "        \n",
    "        # Month as a cyclical feature (for seasonal patterns)\n",
    "        df['month'] = df['datetime'].dt.month\n",
    "        df['time_month_sin'] = np.sin(2 * np.pi * df['month']/12)\n",
    "        df['time_month_cos'] = np.cos(2 * np.pi * df['month']/12)\n",
    "        \n",
    "        # Create features for day/night based on sunrise/sunset\n",
    "        if 'sunrise' in df.columns and 'sunset' in df.columns:\n",
    "            try:\n",
    "                # Convert sunrise/sunset to datetime\n",
    "                df['sunrise'] = pd.to_datetime(df['date'] + ' ' + df['sunrise'], errors='coerce')\n",
    "                df['sunset'] = pd.to_datetime(df['date'] + ' ' + df['sunset'], errors='coerce')\n",
    "                \n",
    "                # Is daylight (1 if current time is between sunrise and sunset)\n",
    "                df['time_is_daylight'] = ((df['datetime'] >= df['sunrise']) & \n",
    "                                     (df['datetime'] <= df['sunset'])).astype(int)\n",
    "                \n",
    "                # Hours since sunrise and hours until sunset\n",
    "                df['time_hours_since_sunrise'] = (df['datetime'] - df['sunrise']).dt.total_seconds() / 3600\n",
    "                df['time_hours_until_sunset'] = (df['sunset'] - df['datetime']).dt.total_seconds() / 3600\n",
    "                \n",
    "                # Replace negative values with 0 (before sunrise or after sunset)\n",
    "                df['time_hours_since_sunrise'] = df['time_hours_since_sunrise'].clip(lower=0)\n",
    "                df['time_hours_until_sunset'] = df['time_hours_until_sunset'].clip(lower=0)\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing sunrise/sunset: {e}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "def process_weather_features(df, circular_cols):\n",
    "    \"\"\"특수한 날씨 피처들에 대한 전처리\"\"\"\n",
    "    \n",
    "    # 풍향(wd)의 순환적 특성 처리\n",
    "    if 'wd' in df.columns and 'wd' in circular_cols:\n",
    "        # 풍향이 0-360도 범위인지 확인\n",
    "        max_wd = df['wd'].max()\n",
    "        if pd.notna(max_wd):  # NaN 체크\n",
    "            if max_wd <= 360:\n",
    "                # 사인/코사인 변환\n",
    "                df['wd_sin'] = np.sin(2 * np.pi * df['wd'] / 360)\n",
    "                df['wd_cos'] = np.cos(2 * np.pi * df['wd'] / 360)\n",
    "            # 만약 다른 범위라면 적절히 조정\n",
    "            else:\n",
    "                df['wd_sin'] = np.sin(2 * np.pi * df['wd'] / max_wd)\n",
    "                df['wd_cos'] = np.cos(2 * np.pi * df['wd'] / max_wd)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def handle_missing_values(df, numeric_cols, zero_impute_cols, categorical_cols, circular_cols):\n",
    "    \"\"\"\n",
    "    Handle missing values based on the provided strategy\n",
    "    \n",
    "    Strategy:\n",
    "    1. For numeric weather features: Consider time pattern and use forward filling, \n",
    "       then anomaly imputation (median)\n",
    "    2. For rn, icsr: Fill NaN with 0\n",
    "    3. For categorical values: Forward fill, then mode imputation\n",
    "    4. For circular features (wd_sin, wd_cos): Handle as numeric\n",
    "    \"\"\"\n",
    "    \n",
    "    # First identify all columns that exist in the dataframe\n",
    "    existing_numeric_cols = [col for col in numeric_cols if col in df.columns]\n",
    "    existing_zero_impute_cols = [col for col in zero_impute_cols if col in df.columns]\n",
    "    existing_cat_cols = [col for col in categorical_cols if col in df.columns]\n",
    "    existing_circular_cols = [col for col in circular_cols if col in df.columns]\n",
    "    \n",
    "    # Group data by hour to capture daily patterns (if datetime exists)\n",
    "    if 'datetime' in df.columns and 'hour' in df.columns:\n",
    "        # Handle numeric columns with time-based strategy\n",
    "        for col in existing_numeric_cols + existing_circular_cols:\n",
    "            # First try forward fill (for small gaps)\n",
    "            df[col] = df.groupby(['hour'])[col].transform(lambda x: x.ffill())\n",
    "            \n",
    "            # For remaining NaNs, use median by hour\n",
    "            hourly_medians = df.groupby(['hour'])[col].transform('median')\n",
    "            df[col] = df[col].fillna(hourly_medians)\n",
    "            \n",
    "            # If still NaN, use overall median\n",
    "            df[col] = df[col].fillna(df[col].median())\n",
    "    else:\n",
    "        # Fallback to simple median imputation\n",
    "        for col in existing_numeric_cols + existing_circular_cols:\n",
    "            df[col] = df[col].fillna(df[col].median())\n",
    "    \n",
    "    # Handle zero-imputation columns\n",
    "    for col in existing_zero_impute_cols:\n",
    "        df[col] = df[col].fillna(0)\n",
    "    \n",
    "    # Handle categorical columns\n",
    "    for col in existing_cat_cols:\n",
    "        # First try forward fill \n",
    "        df[col] = df[col].ffill()\n",
    "        \n",
    "        # For remaining NaNs, use mode\n",
    "        if not df[col].mode().empty:\n",
    "            df[col] = df[col].fillna(df[col].mode().iloc[0])\n",
    "        else:\n",
    "            df[col] = df[col].fillna('unknown')\n",
    "    \n",
    "    return df\n",
    "\n",
    "def prepare_time_series_data(processed_data, feature_cols, target_col='value', window_size=1, prediction_size=1):\n",
    "    \"\"\"\n",
    "    시계열 예측을 위한 슬라이딩 윈도우 데이터 준비\n",
    "    \n",
    "    Args:\n",
    "        processed_data: 전처리된 데이터 딕셔너리\n",
    "        feature_cols: 입력 피처 칼럼 목록\n",
    "        target_col: 예측할 타겟 칼럼 ('value')\n",
    "        window_size: 윈도우 크기(일) - 전날 데이터\n",
    "        prediction_size: 예측 대상 크기(일) - 당일 데이터\n",
    "        \n",
    "    Returns:\n",
    "        time_series_data: 시계열 데이터 딕셔너리 {plant_id: {'X': features, 'y': targets, 'dates': dates}}\n",
    "    \"\"\"\n",
    "    time_series_data = {}\n",
    "    \n",
    "    # 각 발전소별 데이터 처리\n",
    "    for plant_id, df in processed_data.items():\n",
    "        if 'datetime' not in df.columns or target_col not in df.columns:\n",
    "            continue\n",
    "        \n",
    "        # 날짜별로 데이터 정렬\n",
    "        df = df.sort_values('datetime')\n",
    "        \n",
    "        # 날짜만 추출하여 고유한 날짜 목록 생성\n",
    "        df['date_only'] = df['datetime'].dt.date\n",
    "        unique_dates = df['date_only'].unique()\n",
    "        \n",
    "        if len(unique_dates) <= window_size + prediction_size:\n",
    "            print(f\"Plant {plant_id} has insufficient data: {len(unique_dates)} days\")\n",
    "            continue\n",
    "        \n",
    "        # 특성과 타겟 데이터 준비\n",
    "        X_data = []\n",
    "        y_data = []\n",
    "        dates_data = []\n",
    "        \n",
    "        # 각 날짜별로 24시간 데이터 그룹화\n",
    "        date_groups = {}\n",
    "        for date in unique_dates:\n",
    "            date_df = df[df['date_only'] == date]\n",
    "            if len(date_df) == 24:  # 하루에 24시간 데이터가 모두 있는 경우만 사용\n",
    "                date_groups[date] = date_df\n",
    "        \n",
    "        # 날짜 그룹을 시간순으로 정렬\n",
    "        sorted_dates = sorted(date_groups.keys())\n",
    "        \n",
    "        # 슬라이딩 윈도우 방식으로 데이터 생성\n",
    "        for i in range(len(sorted_dates) - window_size - prediction_size + 1):\n",
    "            # 입력 윈도우 (전날 데이터)\n",
    "            input_dates = sorted_dates[i:i+window_size]\n",
    "            input_dfs = [date_groups[date] for date in input_dates]\n",
    "            \n",
    "            # 예측 대상 (당일 데이터)\n",
    "            output_dates = sorted_dates[i+window_size:i+window_size+prediction_size]\n",
    "            output_dfs = [date_groups[date] for date in output_dates]\n",
    "            \n",
    "            # 입력 특성 데이터 (전날의 모든 특성)\n",
    "            X = pd.concat(input_dfs)[feature_cols].values\n",
    "            \n",
    "            # 예측 타겟 데이터 (당일의 발전량)\n",
    "            y = pd.concat(output_dfs)[target_col].values\n",
    "            \n",
    "            # 기록용 날짜 데이터 (예측 대상 날짜)\n",
    "            dates = pd.concat(output_dfs)['datetime'].values\n",
    "            \n",
    "            X_data.append(X)\n",
    "            y_data.append(y)\n",
    "            dates_data.append(dates)\n",
    "        \n",
    "        if X_data:\n",
    "            time_series_data[plant_id] = {\n",
    "                'X': X_data,\n",
    "                'y': y_data,\n",
    "                'dates': dates_data\n",
    "            }\n",
    "    \n",
    "    return time_series_data\n",
    "\n",
    "def split_time_series_data(time_series_data, plants_info, external_test_ratio=0.2, train_ratio=0.7, valid_ratio=0.15):\n",
    "    \"\"\"\n",
    "    시계열 데이터를 학습/검증/테스트 세트로 분할\n",
    "    \n",
    "    전략:\n",
    "    1. 일부 발전소(데이터가 적은)를 external_test_plants로 분리 - 모델 학습에 전혀 사용하지 않음\n",
    "    2. 나머지 발전소 데이터는 시간 순서대로 train/valid/test로 분할\n",
    "    \n",
    "    Args:\n",
    "        time_series_data: 시계열 데이터 딕셔너리\n",
    "        plants_info: 각 발전소 정보를 담은 딕셔너리\n",
    "        external_test_ratio: 외부 테스트용 발전소 비율\n",
    "        train_ratio: 학습 데이터 비율 (external test를 제외한 나머지에서)\n",
    "        valid_ratio: 검증 데이터 비율 (external test를 제외한 나머지에서)\n",
    "        \n",
    "    Returns:\n",
    "        split_data: 분할된 데이터 딕셔너리 (train, valid, test, external_test)\n",
    "    \"\"\"\n",
    "    split_data = {\n",
    "        'train': {'X': [], 'y': [], 'dates': [], 'plants': []},\n",
    "        'valid': {'X': [], 'y': [], 'dates': [], 'plants': []},\n",
    "        'test': {'X': [], 'y': [], 'dates': [], 'plants': []},\n",
    "        'external_test': {'X': [], 'y': [], 'dates': [], 'plants': []}\n",
    "    }\n",
    "    \n",
    "    # 데이터가 있는 발전소만 필터링\n",
    "    available_plants = [plant_id for plant_id in time_series_data if len(time_series_data[plant_id]['X']) >= 3]\n",
    "    \n",
    "    # 발전소를 데이터 길이에 따라 정렬 (오름차순 - 데이터가 적은 순)\n",
    "    sorted_plants = sorted(\n",
    "        available_plants, \n",
    "        key=lambda p: plants_info[p]['data_length'] if 'data_length' in plants_info[p] else 0\n",
    "    )\n",
    "    \n",
    "    # 외부 테스트용 발전소 선택 (데이터가 적은 발전소들)\n",
    "    n_external_test = max(1, int(len(sorted_plants) * external_test_ratio))\n",
    "    external_test_plants = sorted_plants[:n_external_test]\n",
    "    train_valid_test_plants = sorted_plants[n_external_test:]\n",
    "    \n",
    "    print(f\"External test plants: {external_test_plants}\")\n",
    "    print(f\"Regular split plants: {train_valid_test_plants}\")\n",
    "    \n",
    "    # 외부 테스트 발전소 데이터 분리\n",
    "    for plant_id in external_test_plants:\n",
    "        if plant_id not in time_series_data:\n",
    "            continue\n",
    "            \n",
    "        data = time_series_data[plant_id]\n",
    "        split_data['external_test']['X'].extend(data['X'])\n",
    "        split_data['external_test']['y'].extend(data['y'])\n",
    "        split_data['external_test']['dates'].extend(data['dates'])\n",
    "        split_data['external_test']['plants'].extend([plant_id] * len(data['X']))\n",
    "    \n",
    "    # 나머지 발전소 데이터를 시간 순서대로 분할\n",
    "    for plant_id in train_valid_test_plants:\n",
    "        if plant_id not in time_series_data:\n",
    "            continue\n",
    "            \n",
    "        data = time_series_data[plant_id]\n",
    "        n_samples = len(data['X'])\n",
    "        if n_samples < 3:  # 최소 3개 이상의 샘플이 필요\n",
    "            continue\n",
    "            \n",
    "        # 인덱스 계산\n",
    "        train_idx = int(n_samples * train_ratio)\n",
    "        valid_idx = int(n_samples * (train_ratio + valid_ratio))\n",
    "        \n",
    "        # 데이터 분할 - 시간 순서대로 분할\n",
    "        split_data['train']['X'].extend(data['X'][:train_idx])\n",
    "        split_data['train']['y'].extend(data['y'][:train_idx])\n",
    "        split_data['train']['dates'].extend(data['dates'][:train_idx])\n",
    "        split_data['train']['plants'].extend([plant_id] * train_idx)\n",
    "        \n",
    "        split_data['valid']['X'].extend(data['X'][train_idx:valid_idx])\n",
    "        split_data['valid']['y'].extend(data['y'][train_idx:valid_idx])\n",
    "        split_data['valid']['dates'].extend(data['dates'][train_idx:valid_idx])\n",
    "        split_data['valid']['plants'].extend([plant_id] * (valid_idx - train_idx))\n",
    "        \n",
    "        split_data['test']['X'].extend(data['X'][valid_idx:])\n",
    "        split_data['test']['y'].extend(data['y'][valid_idx:])\n",
    "        split_data['test']['dates'].extend(data['dates'][valid_idx:])\n",
    "        split_data['test']['plants'].extend([plant_id] * (n_samples - valid_idx))\n",
    "    \n",
    "    # 배열로 변환\n",
    "    for split_name in split_data:\n",
    "        if split_data[split_name]['X']:  # 비어있지 않은 경우만 처리\n",
    "            split_data[split_name]['X'] = np.array(split_data[split_name]['X'])\n",
    "            split_data[split_name]['y'] = np.array(split_data[split_name]['y'])\n",
    "            split_data[split_name]['dates'] = np.array(split_data[split_name]['dates'])\n",
    "            # plants는 리스트로 유지 (식별자이므로 변환 불필요)\n",
    "    \n",
    "    return split_data\n",
    "\n",
    "def save_time_series_data(split_data, output_dir='../data/modeling_data'):\n",
    "    \"\"\"\n",
    "    시계열 데이터 저장\n",
    "    \n",
    "    Args:\n",
    "        split_data: 분할된 시계열 데이터 딕셔너리\n",
    "        output_dir: 저장할 디렉토리 경로\n",
    "    \"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    for split_name, data in split_data.items():\n",
    "        #if not data['X']:  # 빈 데이터 세트 건너뛰기\n",
    "        #    continue\n",
    "            \n",
    "        split_dir = os.path.join(output_dir, split_name)\n",
    "        os.makedirs(split_dir, exist_ok=True)\n",
    "        \n",
    "        # NumPy 배열로 저장\n",
    "        np.save(os.path.join(split_dir, 'X.npy'), data['X'])\n",
    "        np.save(os.path.join(split_dir, 'y.npy'), data['y'])\n",
    "        np.save(os.path.join(split_dir, 'dates.npy'), data['dates'])\n",
    "        np.save(os.path.join(split_dir, 'plants.npy'), np.array(data['plants']))\n",
    "\n",
    "\n",
    "# Define paths\n",
    "META_DATA_PATH = '../data/solar_energy/meta_data.csv'\n",
    "data_dir = '../data/concat_data'\n",
    "output_dir = '../data/modelnig_data'\n",
    "\n",
    "# 윈도우 크기(일), 예측 대상 크기(일)\n",
    "window_size = 1\n",
    "prediction_size = 1\n",
    "\n",
    "# 외부 테스트용 발전소 비율\n",
    "external_test_ratio = 0.2\n",
    "\n",
    "# Load and preprocess data\n",
    "processed_data, feature_cols, target_col, plants_info = load_and_preprocess_data(META_DATA_PATH, data_dir)\n",
    "\n",
    "# Print summary\n",
    "print(f\"Processed {len(processed_data)} plants\")\n",
    "print(f\"Feature columns ({len(feature_cols)}): {feature_cols[:10]}...\")\n",
    "print(f\"Target column: {target_col}\")\n",
    "\n",
    "# 시계열 데이터 준비 (전날 -> 당일 예측)\n",
    "time_series_data = prepare_time_series_data(\n",
    "    processed_data, \n",
    "    feature_cols, \n",
    "    target_col=target_col,\n",
    "    window_size=window_size,\n",
    "    prediction_size=prediction_size\n",
    ")\n",
    "\n",
    "print(f\"Prepared time series data for {len(time_series_data)} plants\")\n",
    "total_samples = sum(len(data['X']) for plant_id, data in time_series_data.items())\n",
    "print(f\"Total samples: {total_samples}\")\n",
    "\n",
    "# 각 발전소별 샘플 수 출력\n",
    "for plant_id, data in time_series_data.items():\n",
    "    print(f\"Plant {plant_id}: {len(data['X'])} samples\")\n",
    "    if len(data['X']) > 0:\n",
    "        print(f\"  X shape: {data['X'][0].shape}, y shape: {data['y'][0].shape}\")\n",
    "\n",
    "# 시계열 데이터 분할 (학습/검증/테스트 + 외부 테스트)\n",
    "split_data = split_time_series_data(\n",
    "    time_series_data, \n",
    "    plants_info,\n",
    "    external_test_ratio=external_test_ratio\n",
    ")\n",
    "\n",
    "# 결과 요약 출력\n",
    "for split_name, data in split_data.items():\n",
    "    if len(data['X']) > 0:\n",
    "        print(f\"\\n{split_name.upper()} SET:\")\n",
    "        print(f\"  Samples: {len(data['X'])}\")\n",
    "        print(f\"  X shape: {data['X'].shape}\")\n",
    "        print(f\"  y shape: {data['y'].shape}\")\n",
    "        \n",
    "        # 발전소 정보\n",
    "        plant_counts = {}\n",
    "        for plant_id in data['plants']:\n",
    "            if plant_id in plant_counts:\n",
    "                plant_counts[plant_id] += 1\n",
    "            else:\n",
    "                plant_counts[plant_id] = 1\n",
    "        \n",
    "        print(f\"  Plants: {len(plant_counts)}\")\n",
    "        print(f\"  Plant distribution: {plant_counts}\")\n",
    "\n",
    "# 시계열 데이터 저장\n",
    "save_time_series_data(split_data, output_dir)\n",
    "print(f\"\\nTime series data saved to {output_dir}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "b16675e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[1320.215, 55.009, 389.059, ..., 18.683333333333334,\n",
       "         0.9396926207859083, 0.3420201433256688],\n",
       "        [1320.215, 55.009, 389.059, ..., 17.683333333333334, 1.0,\n",
       "         6.123233995736766e-17],\n",
       "        [1320.215, 55.009, 389.059, ..., 16.683333333333334, 1.0,\n",
       "         6.123233995736766e-17],\n",
       "        ...,\n",
       "        [1320.215, 55.009, 389.059, ..., 0.0, 0.9396926207859084,\n",
       "         -0.3420201433256687],\n",
       "        [1320.215, 55.009, 389.059, ..., 0.0, 1.0,\n",
       "         6.123233995736766e-17],\n",
       "        [1320.215, 55.009, 389.059, ..., 0.0, 1.0,\n",
       "         6.123233995736766e-17]],\n",
       "\n",
       "       [[2438.64, 101.61, 445.68, ..., 18.666666666666668,\n",
       "         0.9396926207859084, -0.3420201433256687],\n",
       "        [2438.64, 101.61, 445.68, ..., 17.666666666666668,\n",
       "         0.9396926207859084, -0.3420201433256687],\n",
       "        [2438.64, 101.61, 445.68, ..., 16.666666666666668,\n",
       "         0.9396926207859084, -0.3420201433256687],\n",
       "        ...,\n",
       "        [2438.64, 101.61, 445.68, ..., 0.0, -0.9396926207859085,\n",
       "         0.34202014332566816],\n",
       "        [2438.64, 101.61, 445.68, ..., 0.0, -0.9396926207859085,\n",
       "         0.34202014332566816],\n",
       "        [2438.64, 101.61, 445.68, ..., 0.0, 0.9396926207859083,\n",
       "         0.3420201433256688]],\n",
       "\n",
       "       [[1894.32, 78.93, 321.12, ..., 18.633333333333333,\n",
       "         0.766044443118978, 0.6427876096865394],\n",
       "        [1894.32, 78.93, 321.12, ..., 17.633333333333333,\n",
       "         0.3420201433256687, 0.9396926207859084],\n",
       "        [1894.32, 78.93, 321.12, ..., 16.633333333333333,\n",
       "         0.3420201433256687, 0.9396926207859084],\n",
       "        ...,\n",
       "        [1894.32, 78.93, 321.12, ..., 0.0, 0.9396926207859083,\n",
       "         0.3420201433256688],\n",
       "        [1894.32, 78.93, 321.12, ..., 0.0, 1.0, 6.123233995736766e-17],\n",
       "        [1894.32, 78.93, 321.12, ..., 0.0, 1.0, 6.123233995736766e-17]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[1166.814, 48.617, 353.407, ..., 17.566666666666666,\n",
       "         -0.3420201433256686, 0.9396926207859084],\n",
       "        [1166.814, 48.617, 353.407, ..., 16.566666666666666, -1.0,\n",
       "         -1.8369701987210297e-16],\n",
       "        [1166.814, 48.617, 353.407, ..., 15.566666666666666,\n",
       "         -0.7660444431189779, -0.6427876096865395],\n",
       "        ...,\n",
       "        [1166.814, 48.617, 353.407, ..., 0.0, 0.3420201433256687,\n",
       "         0.9396926207859084],\n",
       "        [1166.814, 48.617, 353.407, ..., 0.0, -2.4492935982947064e-16,\n",
       "         1.0],\n",
       "        [1166.814, 48.617, 353.407, ..., 0.0, -2.4492935982947064e-16,\n",
       "         1.0]],\n",
       "\n",
       "       [[1582.271, 65.928, 301.195, ..., 17.566666666666666,\n",
       "         -0.3420201433256686, 0.9396926207859084],\n",
       "        [1582.271, 65.928, 301.195, ..., 16.566666666666666,\n",
       "         0.3420201433256687, 0.9396926207859084],\n",
       "        [1582.271, 65.928, 301.195, ..., 15.566666666666666,\n",
       "         0.3420201433256687, 0.9396926207859084],\n",
       "        ...,\n",
       "        [1582.271, 65.928, 301.195, ..., 0.0, 0.3420201433256687,\n",
       "         0.9396926207859084],\n",
       "        [1582.271, 65.928, 301.195, ..., 0.0, -0.6427876096865396,\n",
       "         0.7660444431189778],\n",
       "        [1582.271, 65.928, 301.195, ..., 0.0, -0.3420201433256686,\n",
       "         0.9396926207859084]],\n",
       "\n",
       "       [[1726.106, 71.921, 315.441, ..., 17.583333333333332,\n",
       "         -0.3420201433256686, 0.9396926207859084],\n",
       "        [1726.106, 71.921, 315.441, ..., 16.583333333333332,\n",
       "         -0.3420201433256686, 0.9396926207859084],\n",
       "        [1726.106, 71.921, 315.441, ..., 15.583333333333334,\n",
       "         -2.4492935982947064e-16, 1.0],\n",
       "        ...,\n",
       "        [1726.106, 71.921, 315.441, ..., 0.0, -0.6427876096865396,\n",
       "         0.7660444431189778],\n",
       "        [1726.106, 71.921, 315.441, ..., 0.0, -0.6427876096865396,\n",
       "         0.7660444431189778],\n",
       "        [1726.106, 71.921, 315.441, ..., 0.0, -0.6427876096865396,\n",
       "         0.7660444431189778]]], dtype=object)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_data['external_test']['X']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f00bc57",
   "metadata": {},
   "source": [
    "### 결측률 확인\n",
    "\n",
    "- 결측률>50%인 rn (강수량), icsr (일사량)의 Nan은 그냥 0으로 두고 돌리기로 함 \n",
    "- 나머지는 imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0c3ff9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_columns = ['temperature', 'humidity', 'rn', 'ws',\n",
    "       'wd', 'pv', 'pa', 'ps', 'ss', 'icsr', 'dc10Tca', 'dc10LmcsCa', 'lcsCh',\n",
    "       'vs', 'ts', 'sunrise', 'sunset']\n",
    "air_columns = ['SO2', 'CO', 'O3', 'NO2', 'PM10',\n",
    "       'PM25', '미세먼지', '초미세먼지']\n",
    "\n",
    "total_weathers,total_airs = [],[]\n",
    "for f in files:\n",
    "    file = pd.read_parquet(os.path.join(final_data_path, f))\n",
    "    total_weathers.append(file[weather_columns+['date','time']])\n",
    "    total_airs.append(file[air_columns+['date','time']])\n",
    "\n",
    "total_weathers = pd.concat(total_weathers, ignore_index=True).drop_duplicates()\n",
    "total_airs = pd.concat(total_airs, ignore_index=True)   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "44fcdb2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weather_columns = ['temperature', 'humidity', 'rn', 'ws',\n",
    "       'wd', 'pv', 'pa', 'ps', 'ss', 'icsr', 'dc10Tca', 'dc10LmcsCa', 'lcsCh',\n",
    "       'vs', 'ts', 'sunrise', 'sunset']\n",
    "len(total_weathers['wd'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "985d2515",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weather data missing rates:\n",
      "temperature     0.0\n",
      "humidity        0.0\n",
      "rn             90.9\n",
      "ws              0.1\n",
      "wd              0.1\n",
      "pv              0.0\n",
      "pa              0.0\n",
      "ps              0.0\n",
      "ss             43.2\n",
      "icsr           68.2\n",
      "dc10Tca         3.9\n",
      "dc10LmcsCa      2.0\n",
      "lcsCh          48.9\n",
      "vs              0.4\n",
      "ts              0.0\n",
      "sunrise         0.0\n",
      "sunset          0.0\n",
      "date            0.0\n",
      "time            0.0\n",
      "dtype: float64\n",
      "결측률 >50%인 column:\n",
      "['rn', 'icsr']\n",
      "\n",
      "Air data missing rates:\n",
      "SO2      3.8\n",
      "CO       3.7\n",
      "O3       3.4\n",
      "NO2      4.0\n",
      "PM10     5.6\n",
      "PM25     6.1\n",
      "미세먼지     0.0\n",
      "초미세먼지    6.1\n",
      "date     0.0\n",
      "time     0.0\n",
      "dtype: float64\n",
      "결측률 >50%인 column:\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "# column별 결측률 확인 (rounded to 1 decimal place)\n",
    "print(\"Weather data missing rates:\")\n",
    "missing_rates = (total_weathers.isna().mean() * 100).round(1)\n",
    "print(missing_rates)\n",
    "\n",
    "# 결측률>50%인 column print\n",
    "print(\"결측률 >50%인 column:\")\n",
    "high_missing_cols = missing_rates[missing_rates > 50].index.tolist()\n",
    "print(high_missing_cols)\n",
    "\n",
    "print(\"\\nAir data missing rates:\")\n",
    "missing_rates = (total_airs.isna().mean() * 100).round(1)\n",
    "print(missing_rates)\n",
    "\n",
    "# 결측률>50%인 column print\n",
    "print(\"결측률 >50%인 column:\")\n",
    "print(missing_rates[missing_rates > 50].index.tolist())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

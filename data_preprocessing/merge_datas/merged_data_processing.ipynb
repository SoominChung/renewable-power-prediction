{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6238aa59",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "import datetime\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41372148",
   "metadata": {},
   "source": [
    "### generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "49b46983",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available 14시 forecast columns: ['temp_14', 'wd_14', 'sc_14', 'ws_14', 'pp_14']\n",
      "Available 20시 forecast columns: ['temp_20', 'wd_20', 'sc_20', 'ws_20', 'pp_20']\n",
      "Collecting plant information...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 37/37 [00:00<00:00, 124.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 37 plants\n",
      "Processing each plant's data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 1/37 [00:00<00:05,  7.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== Missing Values Before Processing =====\n",
      "rn: 27716개 (90.3%)\n",
      "ws: 30개 (0.1%)\n",
      "wd: 30개 (0.1%)\n",
      "ss: 13865개 (45.2%)\n",
      "icsr: 15108개 (49.2%)\n",
      "dc10Tca: 1163개 (3.8%)\n",
      "dc10LmcsCa: 81개 (0.3%)\n",
      "lcsCh: 16357개 (53.3%)\n",
      "vs: 8개 (0.0%)\n",
      "SO2: 1171개 (3.8%)\n",
      "CO: 466개 (1.5%)\n",
      "O3: 425개 (1.4%)\n",
      "NO2: 438개 (1.4%)\n",
      "PM10: 345개 (1.1%)\n",
      "PM25: 378개 (1.2%)\n",
      "초미세먼지: 378개 (1.2%)\n",
      "temp_14: 24개 (0.1%)\n",
      "wd_14: 24개 (0.1%)\n",
      "ws_14: 24개 (0.1%)\n",
      "temp_20: 24개 (0.1%)\n",
      "wd_20: 24개 (0.1%)\n",
      "ws_20: 24개 (0.1%)\n",
      "wd_sin: 30개 (0.1%)\n",
      "wd_cos: 30개 (0.1%)\n",
      "wd_14_sin: 24개 (0.1%)\n",
      "wd_14_cos: 24개 (0.1%)\n",
      "wd_20_sin: 24개 (0.1%)\n",
      "wd_20_cos: 24개 (0.1%)\n",
      "\n",
      "===== Missing Values After Processing =====\n",
      "모든 결측치가 성공적으로 처리되었습니다. NaN 값이 없습니다.\n",
      "===== Missing Values Before Processing =====\n",
      "temperature: 53개 (0.4%)\n",
      "humidity: 74개 (0.6%)\n",
      "rn: 11574개 (93.1%)\n",
      "ws: 54개 (0.4%)\n",
      "wd: 54개 (0.4%)\n",
      "pv: 74개 (0.6%)\n",
      "pa: 53개 (0.4%)\n",
      "ps: 53개 (0.4%)\n",
      "ss: 5750개 (46.3%)\n",
      "icsr: 5750개 (46.3%)\n",
      "dc10Tca: 112개 (0.9%)\n",
      "dc10LmcsCa: 165개 (1.3%)\n",
      "lcsCh: 6115개 (49.2%)\n",
      "vs: 152개 (1.2%)\n",
      "ts: 55개 (0.4%)\n",
      "SO2: 896개 (7.2%)\n",
      "CO: 893개 (7.2%)\n",
      "O3: 894개 (7.2%)\n",
      "NO2: 893개 (7.2%)\n",
      "PM10: 943개 (7.6%)\n",
      "PM25: 945개 (7.6%)\n",
      "초미세먼지: 945개 (7.6%)\n",
      "temp_20: 1036개 (8.3%)\n",
      "wd_20: 1036개 (8.3%)\n",
      "ws_20: 1036개 (8.3%)\n",
      "pp_20: 1036개 (8.3%)\n",
      "wd_sin: 54개 (0.4%)\n",
      "wd_cos: 54개 (0.4%)\n",
      "wd_20_sin: 1036개 (8.3%)\n",
      "wd_20_cos: 1036개 (8.3%)\n",
      "\n",
      "===== Missing Values After Processing =====\n",
      "모든 결측치가 성공적으로 처리되었습니다. NaN 값이 없습니다.\n",
      "===== Missing Values Before Processing =====\n",
      "temperature: 13개 (0.1%)\n",
      "humidity: 13개 (0.1%)\n",
      "rn: 19161개 (90.5%)\n",
      "ws: 16개 (0.1%)\n",
      "wd: 16개 (0.1%)\n",
      "pv: 13개 (0.1%)\n",
      "pa: 13개 (0.1%)\n",
      "ps: 13개 (0.1%)\n",
      "ss: 9680개 (45.7%)\n",
      "icsr: 21168개 (100.0%)\n",
      "dc10Tca: 179개 (0.8%)\n",
      "dc10LmcsCa: 65개 (0.3%)\n",
      "lcsCh: 12206개 (57.7%)\n",
      "vs: 48개 (0.2%)\n",
      "ts: 28개 (0.1%)\n",
      "SO2: 744개 (3.5%)\n",
      "CO: 751개 (3.5%)\n",
      "O3: 664개 (3.1%)\n",
      "NO2: 913개 (4.3%)\n",
      "PM10: 805개 (3.8%)\n",
      "PM25: 1071개 (5.1%)\n",
      "초미세먼지: 1071개 (5.1%)\n",
      "temp_20: 1098개 (5.2%)\n",
      "wd_20: 1098개 (5.2%)\n",
      "ws_20: 1098개 (5.2%)\n",
      "pp_20: 1098개 (5.2%)\n",
      "wd_sin: 16개 (0.1%)\n",
      "wd_cos: 16개 (0.1%)\n",
      "wd_20_sin: 1098개 (5.2%)\n",
      "wd_20_cos: 1098개 (5.2%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 3/37 [00:00<00:03,  9.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Missing Values After Processing =====\n",
      "모든 결측치가 성공적으로 처리되었습니다. NaN 값이 없습니다.\n",
      "===== Missing Values Before Processing =====\n",
      "temperature: 3개 (0.0%)\n",
      "rn: 19933개 (90.9%)\n",
      "ws: 15개 (0.1%)\n",
      "wd: 16개 (0.1%)\n",
      "pv: 2개 (0.0%)\n",
      "ps: 2개 (0.0%)\n",
      "ss: 9887개 (45.1%)\n",
      "icsr: 21936개 (100.0%)\n",
      "dc10Tca: 300개 (1.4%)\n",
      "dc10LmcsCa: 114개 (0.5%)\n",
      "lcsCh: 11414개 (52.0%)\n",
      "vs: 7개 (0.0%)\n",
      "SO2: 720개 (3.3%)\n",
      "CO: 493개 (2.2%)\n",
      "O3: 257개 (1.2%)\n",
      "NO2: 571개 (2.6%)\n",
      "PM10: 287개 (1.3%)\n",
      "PM25: 1395개 (6.4%)\n",
      "초미세먼지: 1395개 (6.4%)\n",
      "temp_14: 24개 (0.1%)\n",
      "wd_14: 24개 (0.1%)\n",
      "ws_14: 24개 (0.1%)\n",
      "temp_20: 24개 (0.1%)\n",
      "wd_20: 24개 (0.1%)\n",
      "ws_20: 24개 (0.1%)\n",
      "wd_sin: 16개 (0.1%)\n",
      "wd_cos: 16개 (0.1%)\n",
      "wd_14_sin: 24개 (0.1%)\n",
      "wd_14_cos: 24개 (0.1%)\n",
      "wd_20_sin: 24개 (0.1%)\n",
      "wd_20_cos: 24개 (0.1%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 4/37 [00:00<00:03,  9.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Missing Values After Processing =====\n",
      "모든 결측치가 성공적으로 처리되었습니다. NaN 값이 없습니다.\n",
      "===== Missing Values Before Processing =====\n",
      "temperature: 53개 (0.4%)\n",
      "humidity: 74개 (0.6%)\n",
      "rn: 11574개 (93.1%)\n",
      "ws: 54개 (0.4%)\n",
      "wd: 54개 (0.4%)\n",
      "pv: 74개 (0.6%)\n",
      "pa: 53개 (0.4%)\n",
      "ps: 53개 (0.4%)\n",
      "ss: 5750개 (46.3%)\n",
      "icsr: 5750개 (46.3%)\n",
      "dc10Tca: 112개 (0.9%)\n",
      "dc10LmcsCa: 165개 (1.3%)\n",
      "lcsCh: 6115개 (49.2%)\n",
      "vs: 152개 (1.2%)\n",
      "ts: 55개 (0.4%)\n",
      "SO2: 896개 (7.2%)\n",
      "CO: 893개 (7.2%)\n",
      "O3: 894개 (7.2%)\n",
      "NO2: 893개 (7.2%)\n",
      "PM10: 943개 (7.6%)\n",
      "PM25: 945개 (7.6%)\n",
      "초미세먼지: 945개 (7.6%)\n",
      "temp_20: 1036개 (8.3%)\n",
      "wd_20: 1036개 (8.3%)\n",
      "ws_20: 1036개 (8.3%)\n",
      "pp_20: 1036개 (8.3%)\n",
      "wd_sin: 54개 (0.4%)\n",
      "wd_cos: 54개 (0.4%)\n",
      "wd_20_sin: 1036개 (8.3%)\n",
      "wd_20_cos: 1036개 (8.3%)\n",
      "\n",
      "===== Missing Values After Processing =====\n",
      "모든 결측치가 성공적으로 처리되었습니다. NaN 값이 없습니다.\n",
      "===== Missing Values Before Processing =====\n",
      "rn: 4575개 (90.8%)\n",
      "ss: 2294개 (45.5%)\n",
      "icsr: 2294개 (45.5%)\n",
      "dc10Tca: 1개 (0.0%)\n",
      "lcsCh: 2797개 (55.5%)\n",
      "SO2: 129개 (2.6%)\n",
      "CO: 126개 (2.5%)\n",
      "O3: 132개 (2.6%)\n",
      "NO2: 168개 (3.3%)\n",
      "PM10: 109개 (2.2%)\n",
      "PM25: 178개 (3.5%)\n",
      "초미세먼지: 178개 (3.5%)\n",
      "temp_14: 24개 (0.5%)\n",
      "wd_14: 24개 (0.5%)\n",
      "ws_14: 24개 (0.5%)\n",
      "temp_20: 24개 (0.5%)\n",
      "wd_20: 24개 (0.5%)\n",
      "ws_20: 24개 (0.5%)\n",
      "wd_14_sin: 24개 (0.5%)\n",
      "wd_14_cos: 24개 (0.5%)\n",
      "wd_20_sin: 24개 (0.5%)\n",
      "wd_20_cos: 24개 (0.5%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▌        | 6/37 [00:00<00:02, 12.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Missing Values After Processing =====\n",
      "모든 결측치가 성공적으로 처리되었습니다. NaN 값이 없습니다.\n",
      "===== Missing Values Before Processing =====\n",
      "temperature: 13개 (0.1%)\n",
      "humidity: 13개 (0.1%)\n",
      "rn: 19161개 (90.5%)\n",
      "ws: 16개 (0.1%)\n",
      "wd: 16개 (0.1%)\n",
      "pv: 13개 (0.1%)\n",
      "pa: 13개 (0.1%)\n",
      "ps: 13개 (0.1%)\n",
      "ss: 9680개 (45.7%)\n",
      "icsr: 21168개 (100.0%)\n",
      "dc10Tca: 179개 (0.8%)\n",
      "dc10LmcsCa: 65개 (0.3%)\n",
      "lcsCh: 12206개 (57.7%)\n",
      "vs: 48개 (0.2%)\n",
      "ts: 28개 (0.1%)\n",
      "SO2: 744개 (3.5%)\n",
      "CO: 751개 (3.5%)\n",
      "O3: 664개 (3.1%)\n",
      "NO2: 913개 (4.3%)\n",
      "PM10: 805개 (3.8%)\n",
      "PM25: 1071개 (5.1%)\n",
      "초미세먼지: 1071개 (5.1%)\n",
      "temp_20: 1098개 (5.2%)\n",
      "wd_20: 1098개 (5.2%)\n",
      "ws_20: 1098개 (5.2%)\n",
      "pp_20: 1098개 (5.2%)\n",
      "wd_sin: 16개 (0.1%)\n",
      "wd_cos: 16개 (0.1%)\n",
      "wd_20_sin: 1098개 (5.2%)\n",
      "wd_20_cos: 1098개 (5.2%)\n",
      "\n",
      "===== Missing Values After Processing =====\n",
      "모든 결측치가 성공적으로 처리되었습니다. NaN 값이 없습니다.\n",
      "===== Missing Values Before Processing =====\n",
      "temperature: 53개 (0.4%)\n",
      "humidity: 74개 (0.6%)\n",
      "rn: 11574개 (93.1%)\n",
      "ws: 54개 (0.4%)\n",
      "wd: 54개 (0.4%)\n",
      "pv: 74개 (0.6%)\n",
      "pa: 53개 (0.4%)\n",
      "ps: 53개 (0.4%)\n",
      "ss: 5750개 (46.3%)\n",
      "icsr: 5750개 (46.3%)\n",
      "dc10Tca: 112개 (0.9%)\n",
      "dc10LmcsCa: 165개 (1.3%)\n",
      "lcsCh: 6115개 (49.2%)\n",
      "vs: 152개 (1.2%)\n",
      "ts: 55개 (0.4%)\n",
      "SO2: 896개 (7.2%)\n",
      "CO: 893개 (7.2%)\n",
      "O3: 894개 (7.2%)\n",
      "NO2: 893개 (7.2%)\n",
      "PM10: 943개 (7.6%)\n",
      "PM25: 945개 (7.6%)\n",
      "초미세먼지: 945개 (7.6%)\n",
      "temp_20: 1036개 (8.3%)\n",
      "wd_20: 1036개 (8.3%)\n",
      "ws_20: 1036개 (8.3%)\n",
      "pp_20: 1036개 (8.3%)\n",
      "wd_sin: 54개 (0.4%)\n",
      "wd_cos: 54개 (0.4%)\n",
      "wd_20_sin: 1036개 (8.3%)\n",
      "wd_20_cos: 1036개 (8.3%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 8/37 [00:00<00:02,  9.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Missing Values After Processing =====\n",
      "모든 결측치가 성공적으로 처리되었습니다. NaN 값이 없습니다.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 10/37 [00:00<00:02, 11.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== Missing Values Before Processing =====\n",
      "temperature: 53개 (0.4%)\n",
      "humidity: 74개 (0.6%)\n",
      "rn: 11574개 (93.1%)\n",
      "ws: 54개 (0.4%)\n",
      "wd: 54개 (0.4%)\n",
      "pv: 74개 (0.6%)\n",
      "pa: 53개 (0.4%)\n",
      "ps: 53개 (0.4%)\n",
      "ss: 5750개 (46.3%)\n",
      "icsr: 5750개 (46.3%)\n",
      "dc10Tca: 112개 (0.9%)\n",
      "dc10LmcsCa: 165개 (1.3%)\n",
      "lcsCh: 6115개 (49.2%)\n",
      "vs: 152개 (1.2%)\n",
      "ts: 55개 (0.4%)\n",
      "SO2: 896개 (7.2%)\n",
      "CO: 893개 (7.2%)\n",
      "O3: 894개 (7.2%)\n",
      "NO2: 893개 (7.2%)\n",
      "PM10: 943개 (7.6%)\n",
      "PM25: 945개 (7.6%)\n",
      "초미세먼지: 945개 (7.6%)\n",
      "temp_20: 1036개 (8.3%)\n",
      "wd_20: 1036개 (8.3%)\n",
      "ws_20: 1036개 (8.3%)\n",
      "pp_20: 1036개 (8.3%)\n",
      "wd_sin: 54개 (0.4%)\n",
      "wd_cos: 54개 (0.4%)\n",
      "wd_20_sin: 1036개 (8.3%)\n",
      "wd_20_cos: 1036개 (8.3%)\n",
      "\n",
      "===== Missing Values After Processing =====\n",
      "모든 결측치가 성공적으로 처리되었습니다. NaN 값이 없습니다.\n",
      "===== Missing Values Before Processing =====\n",
      "rn: 7184개 (89.1%)\n",
      "ss: 3705개 (45.9%)\n",
      "icsr: 3705개 (45.9%)\n",
      "dc10Tca: 2개 (0.0%)\n",
      "dc10LmcsCa: 2개 (0.0%)\n",
      "lcsCh: 4176개 (51.8%)\n",
      "SO2: 1213개 (15.0%)\n",
      "CO: 1214개 (15.1%)\n",
      "O3: 1210개 (15.0%)\n",
      "NO2: 1216개 (15.1%)\n",
      "PM10: 2019개 (25.0%)\n",
      "PM25: 2036개 (25.2%)\n",
      "초미세먼지: 2036개 (25.2%)\n",
      "temp_14: 24개 (0.3%)\n",
      "wd_14: 24개 (0.3%)\n",
      "ws_14: 24개 (0.3%)\n",
      "temp_20: 24개 (0.3%)\n",
      "wd_20: 24개 (0.3%)\n",
      "ws_20: 24개 (0.3%)\n",
      "wd_14_sin: 24개 (0.3%)\n",
      "wd_14_cos: 24개 (0.3%)\n",
      "wd_20_sin: 24개 (0.3%)\n",
      "wd_20_cos: 24개 (0.3%)\n",
      "\n",
      "===== Missing Values After Processing =====\n",
      "모든 결측치가 성공적으로 처리되었습니다. NaN 값이 없습니다.\n",
      "===== Missing Values Before Processing =====\n",
      "temperature: 13개 (0.1%)\n",
      "humidity: 13개 (0.1%)\n",
      "rn: 19161개 (90.5%)\n",
      "ws: 16개 (0.1%)\n",
      "wd: 16개 (0.1%)\n",
      "pv: 13개 (0.1%)\n",
      "pa: 13개 (0.1%)\n",
      "ps: 13개 (0.1%)\n",
      "ss: 9680개 (45.7%)\n",
      "icsr: 21168개 (100.0%)\n",
      "dc10Tca: 179개 (0.8%)\n",
      "dc10LmcsCa: 65개 (0.3%)\n",
      "lcsCh: 12206개 (57.7%)\n",
      "vs: 48개 (0.2%)\n",
      "ts: 28개 (0.1%)\n",
      "SO2: 744개 (3.5%)\n",
      "CO: 751개 (3.5%)\n",
      "O3: 664개 (3.1%)\n",
      "NO2: 913개 (4.3%)\n",
      "PM10: 805개 (3.8%)\n",
      "PM25: 1071개 (5.1%)\n",
      "초미세먼지: 1071개 (5.1%)\n",
      "temp_20: 1098개 (5.2%)\n",
      "wd_20: 1098개 (5.2%)\n",
      "ws_20: 1098개 (5.2%)\n",
      "pp_20: 1098개 (5.2%)\n",
      "wd_sin: 16개 (0.1%)\n",
      "wd_cos: 16개 (0.1%)\n",
      "wd_20_sin: 1098개 (5.2%)\n",
      "wd_20_cos: 1098개 (5.2%)\n",
      "\n",
      "===== Missing Values After Processing =====\n",
      "모든 결측치가 성공적으로 처리되었습니다. NaN 값이 없습니다.\n",
      "===== Missing Values Before Processing =====\n",
      "humidity: 4개 (0.0%)\n",
      "rn: 51401개 (90.6%)\n",
      "ws: 32개 (0.1%)\n",
      "wd: 147개 (0.3%)\n",
      "pa: 4개 (0.0%)\n",
      "ps: 4개 (0.0%)\n",
      "ss: 26025개 (45.9%)\n",
      "icsr: 26906개 (47.4%)\n",
      "dc10Tca: 7111개 (12.5%)\n",
      "dc10LmcsCa: 3083개 (5.4%)\n",
      "lcsCh: 30391개 (53.6%)\n",
      "vs: 500개 (0.9%)\n",
      "ts: 4개 (0.0%)\n",
      "SO2: 1111개 (2.0%)\n",
      "CO: 1034개 (1.8%)\n",
      "O3: 1266개 (2.2%)\n",
      "NO2: 1447개 (2.6%)\n",
      "PM10: 2421개 (4.3%)\n",
      "PM25: 1671개 (2.9%)\n",
      "초미세먼지: 1671개 (2.9%)\n",
      "temp_14: 24개 (0.0%)\n",
      "wd_14: 24개 (0.0%)\n",
      "ws_14: 24개 (0.0%)\n",
      "temp_20: 24개 (0.0%)\n",
      "wd_20: 24개 (0.0%)\n",
      "ws_20: 24개 (0.0%)\n",
      "wd_sin: 147개 (0.3%)\n",
      "wd_cos: 147개 (0.3%)\n",
      "wd_14_sin: 24개 (0.0%)\n",
      "wd_14_cos: 24개 (0.0%)\n",
      "wd_20_sin: 24개 (0.0%)\n",
      "wd_20_cos: 24개 (0.0%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 12/37 [00:01<00:02,  9.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Missing Values After Processing =====\n",
      "모든 결측치가 성공적으로 처리되었습니다. NaN 값이 없습니다.\n",
      "===== Missing Values Before Processing =====\n",
      "rn: 7184개 (89.1%)\n",
      "ss: 3705개 (45.9%)\n",
      "icsr: 3705개 (45.9%)\n",
      "dc10Tca: 2개 (0.0%)\n",
      "dc10LmcsCa: 2개 (0.0%)\n",
      "lcsCh: 4176개 (51.8%)\n",
      "SO2: 217개 (2.7%)\n",
      "CO: 211개 (2.6%)\n",
      "O3: 217개 (2.7%)\n",
      "NO2: 277개 (3.4%)\n",
      "PM10: 199개 (2.5%)\n",
      "PM25: 362개 (4.5%)\n",
      "초미세먼지: 362개 (4.5%)\n",
      "temp_14: 24개 (0.3%)\n",
      "wd_14: 24개 (0.3%)\n",
      "ws_14: 24개 (0.3%)\n",
      "temp_20: 24개 (0.3%)\n",
      "wd_20: 24개 (0.3%)\n",
      "ws_20: 24개 (0.3%)\n",
      "wd_14_sin: 24개 (0.3%)\n",
      "wd_14_cos: 24개 (0.3%)\n",
      "wd_20_sin: 24개 (0.3%)\n",
      "wd_20_cos: 24개 (0.3%)\n",
      "\n",
      "===== Missing Values After Processing =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 14/37 [00:01<00:02, 10.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "모든 결측치가 성공적으로 처리되었습니다. NaN 값이 없습니다.\n",
      "===== Missing Values Before Processing =====\n",
      "temperature: 53개 (0.4%)\n",
      "humidity: 74개 (0.6%)\n",
      "rn: 11574개 (93.1%)\n",
      "ws: 54개 (0.4%)\n",
      "wd: 54개 (0.4%)\n",
      "pv: 74개 (0.6%)\n",
      "pa: 53개 (0.4%)\n",
      "ps: 53개 (0.4%)\n",
      "ss: 5750개 (46.3%)\n",
      "icsr: 5750개 (46.3%)\n",
      "dc10Tca: 112개 (0.9%)\n",
      "dc10LmcsCa: 165개 (1.3%)\n",
      "lcsCh: 6115개 (49.2%)\n",
      "vs: 152개 (1.2%)\n",
      "ts: 55개 (0.4%)\n",
      "SO2: 896개 (7.2%)\n",
      "CO: 893개 (7.2%)\n",
      "O3: 894개 (7.2%)\n",
      "NO2: 893개 (7.2%)\n",
      "PM10: 943개 (7.6%)\n",
      "PM25: 945개 (7.6%)\n",
      "초미세먼지: 945개 (7.6%)\n",
      "temp_20: 1036개 (8.3%)\n",
      "wd_20: 1036개 (8.3%)\n",
      "ws_20: 1036개 (8.3%)\n",
      "pp_20: 1036개 (8.3%)\n",
      "wd_sin: 54개 (0.4%)\n",
      "wd_cos: 54개 (0.4%)\n",
      "wd_20_sin: 1036개 (8.3%)\n",
      "wd_20_cos: 1036개 (8.3%)\n",
      "\n",
      "===== Missing Values After Processing =====\n",
      "모든 결측치가 성공적으로 처리되었습니다. NaN 값이 없습니다.\n",
      "===== Missing Values Before Processing =====\n",
      "temperature: 53개 (0.4%)\n",
      "humidity: 74개 (0.6%)\n",
      "rn: 11574개 (93.1%)\n",
      "ws: 54개 (0.4%)\n",
      "wd: 54개 (0.4%)\n",
      "pv: 74개 (0.6%)\n",
      "pa: 53개 (0.4%)\n",
      "ps: 53개 (0.4%)\n",
      "ss: 5750개 (46.3%)\n",
      "icsr: 5750개 (46.3%)\n",
      "dc10Tca: 112개 (0.9%)\n",
      "dc10LmcsCa: 165개 (1.3%)\n",
      "lcsCh: 6115개 (49.2%)\n",
      "vs: 152개 (1.2%)\n",
      "ts: 55개 (0.4%)\n",
      "SO2: 896개 (7.2%)\n",
      "CO: 893개 (7.2%)\n",
      "O3: 894개 (7.2%)\n",
      "NO2: 893개 (7.2%)\n",
      "PM10: 943개 (7.6%)\n",
      "PM25: 945개 (7.6%)\n",
      "초미세먼지: 945개 (7.6%)\n",
      "temp_20: 1036개 (8.3%)\n",
      "wd_20: 1036개 (8.3%)\n",
      "ws_20: 1036개 (8.3%)\n",
      "pp_20: 1036개 (8.3%)\n",
      "wd_sin: 54개 (0.4%)\n",
      "wd_cos: 54개 (0.4%)\n",
      "wd_20_sin: 1036개 (8.3%)\n",
      "wd_20_cos: 1036개 (8.3%)\n",
      "\n",
      "===== Missing Values After Processing =====\n",
      "모든 결측치가 성공적으로 처리되었습니다. NaN 값이 없습니다.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|████▎     | 16/37 [00:01<00:02, 10.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== Missing Values Before Processing =====\n",
      "temperature: 3개 (0.0%)\n",
      "rn: 19933개 (90.9%)\n",
      "ws: 15개 (0.1%)\n",
      "wd: 16개 (0.1%)\n",
      "pv: 2개 (0.0%)\n",
      "ps: 2개 (0.0%)\n",
      "ss: 9887개 (45.1%)\n",
      "icsr: 21936개 (100.0%)\n",
      "dc10Tca: 300개 (1.4%)\n",
      "dc10LmcsCa: 114개 (0.5%)\n",
      "lcsCh: 11414개 (52.0%)\n",
      "vs: 7개 (0.0%)\n",
      "SO2: 720개 (3.3%)\n",
      "CO: 493개 (2.2%)\n",
      "O3: 257개 (1.2%)\n",
      "NO2: 571개 (2.6%)\n",
      "PM10: 287개 (1.3%)\n",
      "PM25: 1395개 (6.4%)\n",
      "초미세먼지: 1395개 (6.4%)\n",
      "temp_14: 24개 (0.1%)\n",
      "wd_14: 24개 (0.1%)\n",
      "ws_14: 24개 (0.1%)\n",
      "temp_20: 24개 (0.1%)\n",
      "wd_20: 24개 (0.1%)\n",
      "ws_20: 24개 (0.1%)\n",
      "wd_sin: 16개 (0.1%)\n",
      "wd_cos: 16개 (0.1%)\n",
      "wd_14_sin: 24개 (0.1%)\n",
      "wd_14_cos: 24개 (0.1%)\n",
      "wd_20_sin: 24개 (0.1%)\n",
      "wd_20_cos: 24개 (0.1%)\n",
      "\n",
      "===== Missing Values After Processing =====\n",
      "모든 결측치가 성공적으로 처리되었습니다. NaN 값이 없습니다.\n",
      "===== Missing Values Before Processing =====\n",
      "temperature: 53개 (0.4%)\n",
      "humidity: 74개 (0.6%)\n",
      "rn: 11574개 (93.1%)\n",
      "ws: 54개 (0.4%)\n",
      "wd: 54개 (0.4%)\n",
      "pv: 74개 (0.6%)\n",
      "pa: 53개 (0.4%)\n",
      "ps: 53개 (0.4%)\n",
      "ss: 5750개 (46.3%)\n",
      "icsr: 5750개 (46.3%)\n",
      "dc10Tca: 112개 (0.9%)\n",
      "dc10LmcsCa: 165개 (1.3%)\n",
      "lcsCh: 6115개 (49.2%)\n",
      "vs: 152개 (1.2%)\n",
      "ts: 55개 (0.4%)\n",
      "SO2: 896개 (7.2%)\n",
      "CO: 893개 (7.2%)\n",
      "O3: 894개 (7.2%)\n",
      "NO2: 893개 (7.2%)\n",
      "PM10: 943개 (7.6%)\n",
      "PM25: 945개 (7.6%)\n",
      "초미세먼지: 945개 (7.6%)\n",
      "temp_20: 1036개 (8.3%)\n",
      "wd_20: 1036개 (8.3%)\n",
      "ws_20: 1036개 (8.3%)\n",
      "pp_20: 1036개 (8.3%)\n",
      "wd_sin: 54개 (0.4%)\n",
      "wd_cos: 54개 (0.4%)\n",
      "wd_20_sin: 1036개 (8.3%)\n",
      "wd_20_cos: 1036개 (8.3%)\n",
      "\n",
      "===== Missing Values After Processing =====\n",
      "모든 결측치가 성공적으로 처리되었습니다. NaN 값이 없습니다.\n",
      "===== Missing Values Before Processing =====\n",
      "temperature: 53개 (0.4%)\n",
      "humidity: 74개 (0.6%)\n",
      "rn: 11574개 (93.1%)\n",
      "ws: 54개 (0.4%)\n",
      "wd: 54개 (0.4%)\n",
      "pv: 74개 (0.6%)\n",
      "pa: 53개 (0.4%)\n",
      "ps: 53개 (0.4%)\n",
      "ss: 5750개 (46.3%)\n",
      "icsr: 5750개 (46.3%)\n",
      "dc10Tca: 112개 (0.9%)\n",
      "dc10LmcsCa: 165개 (1.3%)\n",
      "lcsCh: 6115개 (49.2%)\n",
      "vs: 152개 (1.2%)\n",
      "ts: 55개 (0.4%)\n",
      "SO2: 896개 (7.2%)\n",
      "CO: 893개 (7.2%)\n",
      "O3: 894개 (7.2%)\n",
      "NO2: 893개 (7.2%)\n",
      "PM10: 943개 (7.6%)\n",
      "PM25: 945개 (7.6%)\n",
      "초미세먼지: 945개 (7.6%)\n",
      "temp_20: 1036개 (8.3%)\n",
      "wd_20: 1036개 (8.3%)\n",
      "ws_20: 1036개 (8.3%)\n",
      "pp_20: 1036개 (8.3%)\n",
      "wd_sin: 54개 (0.4%)\n",
      "wd_cos: 54개 (0.4%)\n",
      "wd_20_sin: 1036개 (8.3%)\n",
      "wd_20_cos: 1036개 (8.3%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 49%|████▊     | 18/37 [00:01<00:01, 11.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Missing Values After Processing =====\n",
      "모든 결측치가 성공적으로 처리되었습니다. NaN 값이 없습니다.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|█████▍    | 20/37 [00:01<00:01, 12.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== Missing Values Before Processing =====\n",
      "rn: 15029개 (90.8%)\n",
      "ws: 2개 (0.0%)\n",
      "wd: 5개 (0.0%)\n",
      "ss: 7417개 (44.8%)\n",
      "icsr: 16560개 (100.0%)\n",
      "dc10Tca: 142개 (0.9%)\n",
      "dc10LmcsCa: 55개 (0.3%)\n",
      "lcsCh: 8590개 (51.9%)\n",
      "vs: 3개 (0.0%)\n",
      "ts: 2개 (0.0%)\n",
      "SO2: 172개 (1.0%)\n",
      "CO: 277개 (1.7%)\n",
      "O3: 172개 (1.0%)\n",
      "NO2: 256개 (1.5%)\n",
      "PM10: 187개 (1.1%)\n",
      "PM25: 235개 (1.4%)\n",
      "초미세먼지: 235개 (1.4%)\n",
      "temp_14: 24개 (0.1%)\n",
      "wd_14: 24개 (0.1%)\n",
      "ws_14: 24개 (0.1%)\n",
      "pp_14: 24개 (0.1%)\n",
      "temp_20: 1120개 (6.8%)\n",
      "wd_20: 1120개 (6.8%)\n",
      "ws_20: 1120개 (6.8%)\n",
      "pp_20: 1120개 (6.8%)\n",
      "wd_sin: 5개 (0.0%)\n",
      "wd_cos: 5개 (0.0%)\n",
      "wd_14_sin: 24개 (0.1%)\n",
      "wd_14_cos: 24개 (0.1%)\n",
      "wd_20_sin: 1120개 (6.8%)\n",
      "wd_20_cos: 1120개 (6.8%)\n",
      "\n",
      "===== Missing Values After Processing =====\n",
      "모든 결측치가 성공적으로 처리되었습니다. NaN 값이 없습니다.\n",
      "===== Missing Values Before Processing =====\n",
      "rn: 7235개 (90.3%)\n",
      "ss: 3565개 (44.5%)\n",
      "icsr: 8016개 (100.0%)\n",
      "dc10Tca: 6개 (0.1%)\n",
      "dc10LmcsCa: 1개 (0.0%)\n",
      "lcsCh: 2622개 (32.7%)\n",
      "SO2: 337개 (4.2%)\n",
      "CO: 203개 (2.5%)\n",
      "O3: 111개 (1.4%)\n",
      "NO2: 125개 (1.6%)\n",
      "PM10: 306개 (3.8%)\n",
      "PM25: 538개 (6.7%)\n",
      "초미세먼지: 538개 (6.7%)\n",
      "temp_20: 668개 (8.3%)\n",
      "wd_20: 668개 (8.3%)\n",
      "ws_20: 668개 (8.3%)\n",
      "pp_20: 668개 (8.3%)\n",
      "wd_20_sin: 668개 (8.3%)\n",
      "wd_20_cos: 668개 (8.3%)\n",
      "\n",
      "===== Missing Values After Processing =====\n",
      "모든 결측치가 성공적으로 처리되었습니다. NaN 값이 없습니다.\n",
      "===== Missing Values Before Processing =====\n",
      "rn: 7184개 (89.1%)\n",
      "ss: 3705개 (45.9%)\n",
      "icsr: 3705개 (45.9%)\n",
      "dc10Tca: 2개 (0.0%)\n",
      "dc10LmcsCa: 2개 (0.0%)\n",
      "lcsCh: 4176개 (51.8%)\n",
      "SO2: 217개 (2.7%)\n",
      "CO: 211개 (2.6%)\n",
      "O3: 217개 (2.7%)\n",
      "NO2: 277개 (3.4%)\n",
      "PM10: 199개 (2.5%)\n",
      "PM25: 362개 (4.5%)\n",
      "초미세먼지: 362개 (4.5%)\n",
      "temp_14: 24개 (0.3%)\n",
      "wd_14: 24개 (0.3%)\n",
      "ws_14: 24개 (0.3%)\n",
      "temp_20: 24개 (0.3%)\n",
      "wd_20: 24개 (0.3%)\n",
      "ws_20: 24개 (0.3%)\n",
      "wd_14_sin: 24개 (0.3%)\n",
      "wd_14_cos: 24개 (0.3%)\n",
      "wd_20_sin: 24개 (0.3%)\n",
      "wd_20_cos: 24개 (0.3%)\n",
      "\n",
      "===== Missing Values After Processing =====\n",
      "모든 결측치가 성공적으로 처리되었습니다. NaN 값이 없습니다.\n",
      "===== Missing Values Before Processing =====\n",
      "rn: 6528개 (93.2%)\n",
      "ws: 13개 (0.2%)\n",
      "wd: 13개 (0.2%)\n",
      "ss: 3289개 (46.9%)\n",
      "icsr: 3289개 (46.9%)\n",
      "dc10Tca: 3개 (0.0%)\n",
      "dc10LmcsCa: 7개 (0.1%)\n",
      "lcsCh: 4053개 (57.8%)\n",
      "SO2: 67개 (1.0%)\n",
      "CO: 67개 (1.0%)\n",
      "O3: 73개 (1.0%)\n",
      "NO2: 68개 (1.0%)\n",
      "PM10: 45개 (0.6%)\n",
      "PM25: 30개 (0.4%)\n",
      "초미세먼지: 30개 (0.4%)\n",
      "temp_14: 24개 (0.3%)\n",
      "wd_14: 24개 (0.3%)\n",
      "ws_14: 24개 (0.3%)\n",
      "temp_20: 24개 (0.3%)\n",
      "wd_20: 24개 (0.3%)\n",
      "ws_20: 24개 (0.3%)\n",
      "wd_sin: 13개 (0.2%)\n",
      "wd_cos: 13개 (0.2%)\n",
      "wd_14_sin: 24개 (0.3%)\n",
      "wd_14_cos: 24개 (0.3%)\n",
      "wd_20_sin: 24개 (0.3%)\n",
      "wd_20_cos: 24개 (0.3%)\n",
      "\n",
      "===== Missing Values After Processing =====\n",
      "모든 결측치가 성공적으로 처리되었습니다. NaN 값이 없습니다.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▏   | 23/37 [00:02<00:01, 13.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== Missing Values Before Processing =====\n",
      "temperature: 53개 (0.4%)\n",
      "humidity: 74개 (0.6%)\n",
      "rn: 11574개 (93.1%)\n",
      "ws: 54개 (0.4%)\n",
      "wd: 54개 (0.4%)\n",
      "pv: 74개 (0.6%)\n",
      "pa: 53개 (0.4%)\n",
      "ps: 53개 (0.4%)\n",
      "ss: 5750개 (46.3%)\n",
      "icsr: 5750개 (46.3%)\n",
      "dc10Tca: 112개 (0.9%)\n",
      "dc10LmcsCa: 165개 (1.3%)\n",
      "lcsCh: 6115개 (49.2%)\n",
      "vs: 152개 (1.2%)\n",
      "ts: 55개 (0.4%)\n",
      "SO2: 896개 (7.2%)\n",
      "CO: 893개 (7.2%)\n",
      "O3: 894개 (7.2%)\n",
      "NO2: 893개 (7.2%)\n",
      "PM10: 943개 (7.6%)\n",
      "PM25: 945개 (7.6%)\n",
      "초미세먼지: 945개 (7.6%)\n",
      "temp_20: 1036개 (8.3%)\n",
      "wd_20: 1036개 (8.3%)\n",
      "ws_20: 1036개 (8.3%)\n",
      "pp_20: 1036개 (8.3%)\n",
      "wd_sin: 54개 (0.4%)\n",
      "wd_cos: 54개 (0.4%)\n",
      "wd_20_sin: 1036개 (8.3%)\n",
      "wd_20_cos: 1036개 (8.3%)\n",
      "\n",
      "===== Missing Values After Processing =====\n",
      "모든 결측치가 성공적으로 처리되었습니다. NaN 값이 없습니다.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|██████▊   | 25/37 [00:02<00:01, 10.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== Missing Values Before Processing =====\n",
      "temperature: 13개 (0.1%)\n",
      "humidity: 13개 (0.1%)\n",
      "rn: 19161개 (90.5%)\n",
      "ws: 16개 (0.1%)\n",
      "wd: 16개 (0.1%)\n",
      "pv: 13개 (0.1%)\n",
      "pa: 13개 (0.1%)\n",
      "ps: 13개 (0.1%)\n",
      "ss: 9680개 (45.7%)\n",
      "icsr: 21168개 (100.0%)\n",
      "dc10Tca: 179개 (0.8%)\n",
      "dc10LmcsCa: 65개 (0.3%)\n",
      "lcsCh: 12206개 (57.7%)\n",
      "vs: 48개 (0.2%)\n",
      "ts: 28개 (0.1%)\n",
      "SO2: 744개 (3.5%)\n",
      "CO: 751개 (3.5%)\n",
      "O3: 664개 (3.1%)\n",
      "NO2: 913개 (4.3%)\n",
      "PM10: 805개 (3.8%)\n",
      "PM25: 1071개 (5.1%)\n",
      "초미세먼지: 1071개 (5.1%)\n",
      "temp_20: 1098개 (5.2%)\n",
      "wd_20: 1098개 (5.2%)\n",
      "ws_20: 1098개 (5.2%)\n",
      "pp_20: 1098개 (5.2%)\n",
      "wd_sin: 16개 (0.1%)\n",
      "wd_cos: 16개 (0.1%)\n",
      "wd_20_sin: 1098개 (5.2%)\n",
      "wd_20_cos: 1098개 (5.2%)\n",
      "\n",
      "===== Missing Values After Processing =====\n",
      "모든 결측치가 성공적으로 처리되었습니다. NaN 값이 없습니다.\n",
      "===== Missing Values Before Processing =====\n",
      "rn: 7133개 (92.3%)\n",
      "ws: 13개 (0.2%)\n",
      "wd: 13개 (0.2%)\n",
      "ss: 3587개 (46.4%)\n",
      "icsr: 3587개 (46.4%)\n",
      "dc10Tca: 3개 (0.0%)\n",
      "dc10LmcsCa: 7개 (0.1%)\n",
      "lcsCh: 4328개 (56.0%)\n",
      "SO2: 129개 (1.7%)\n",
      "CO: 120개 (1.6%)\n",
      "O3: 123개 (1.6%)\n",
      "NO2: 119개 (1.5%)\n",
      "PM10: 172개 (2.2%)\n",
      "PM25: 130개 (1.7%)\n",
      "초미세먼지: 130개 (1.7%)\n",
      "temp_14: 24개 (0.3%)\n",
      "wd_14: 24개 (0.3%)\n",
      "ws_14: 24개 (0.3%)\n",
      "temp_20: 24개 (0.3%)\n",
      "wd_20: 24개 (0.3%)\n",
      "ws_20: 24개 (0.3%)\n",
      "wd_sin: 13개 (0.2%)\n",
      "wd_cos: 13개 (0.2%)\n",
      "wd_14_sin: 24개 (0.3%)\n",
      "wd_14_cos: 24개 (0.3%)\n",
      "wd_20_sin: 24개 (0.3%)\n",
      "wd_20_cos: 24개 (0.3%)\n",
      "\n",
      "===== Missing Values After Processing =====\n",
      "모든 결측치가 성공적으로 처리되었습니다. NaN 값이 없습니다.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|███████▎  | 27/37 [00:02<00:01,  9.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== Missing Values Before Processing =====\n",
      "humidity: 4개 (0.0%)\n",
      "rn: 51425개 (90.6%)\n",
      "ws: 32개 (0.1%)\n",
      "wd: 147개 (0.3%)\n",
      "pa: 4개 (0.0%)\n",
      "ps: 4개 (0.0%)\n",
      "ss: 26038개 (45.9%)\n",
      "icsr: 26919개 (47.4%)\n",
      "dc10Tca: 7111개 (12.5%)\n",
      "dc10LmcsCa: 3083개 (5.4%)\n",
      "lcsCh: 30400개 (53.6%)\n",
      "vs: 500개 (0.9%)\n",
      "ts: 4개 (0.0%)\n",
      "SO2: 1111개 (2.0%)\n",
      "CO: 1034개 (1.8%)\n",
      "O3: 1266개 (2.2%)\n",
      "NO2: 1447개 (2.5%)\n",
      "PM10: 2421개 (4.3%)\n",
      "PM25: 1671개 (2.9%)\n",
      "초미세먼지: 1671개 (2.9%)\n",
      "temp_14: 24개 (0.0%)\n",
      "wd_14: 24개 (0.0%)\n",
      "ws_14: 24개 (0.0%)\n",
      "temp_20: 24개 (0.0%)\n",
      "wd_20: 24개 (0.0%)\n",
      "ws_20: 24개 (0.0%)\n",
      "wd_sin: 147개 (0.3%)\n",
      "wd_cos: 147개 (0.3%)\n",
      "wd_14_sin: 24개 (0.0%)\n",
      "wd_14_cos: 24개 (0.0%)\n",
      "wd_20_sin: 24개 (0.0%)\n",
      "wd_20_cos: 24개 (0.0%)\n",
      "\n",
      "===== Missing Values After Processing =====\n",
      "모든 결측치가 성공적으로 처리되었습니다. NaN 값이 없습니다.\n",
      "===== Missing Values Before Processing =====\n",
      "rn: 7235개 (90.3%)\n",
      "ss: 3565개 (44.5%)\n",
      "icsr: 8016개 (100.0%)\n",
      "dc10Tca: 6개 (0.1%)\n",
      "dc10LmcsCa: 1개 (0.0%)\n",
      "lcsCh: 2622개 (32.7%)\n",
      "SO2: 337개 (4.2%)\n",
      "CO: 203개 (2.5%)\n",
      "O3: 111개 (1.4%)\n",
      "NO2: 125개 (1.6%)\n",
      "PM10: 306개 (3.8%)\n",
      "PM25: 538개 (6.7%)\n",
      "초미세먼지: 538개 (6.7%)\n",
      "temp_20: 668개 (8.3%)\n",
      "wd_20: 668개 (8.3%)\n",
      "ws_20: 668개 (8.3%)\n",
      "pp_20: 668개 (8.3%)\n",
      "wd_20_sin: 668개 (8.3%)\n",
      "wd_20_cos: 668개 (8.3%)\n",
      "\n",
      "===== Missing Values After Processing =====\n",
      "모든 결측치가 성공적으로 처리되었습니다. NaN 값이 없습니다.\n",
      "===== Missing Values Before Processing =====\n",
      "rn: 10832개 (91.0%)\n",
      "wd: 3개 (0.0%)\n",
      "ss: 5527개 (46.4%)\n",
      "icsr: 11904개 (100.0%)\n",
      "dc10Tca: 63개 (0.5%)\n",
      "dc10LmcsCa: 46개 (0.4%)\n",
      "lcsCh: 6289개 (52.8%)\n",
      "vs: 3개 (0.0%)\n",
      "ts: 1개 (0.0%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|███████▊  | 29/37 [00:02<00:00, 10.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SO2: 152개 (1.3%)\n",
      "CO: 257개 (2.2%)\n",
      "O3: 152개 (1.3%)\n",
      "NO2: 236개 (2.0%)\n",
      "PM10: 164개 (1.4%)\n",
      "PM25: 205개 (1.7%)\n",
      "초미세먼지: 205개 (1.7%)\n",
      "temp_14: 24개 (0.2%)\n",
      "wd_14: 24개 (0.2%)\n",
      "ws_14: 24개 (0.2%)\n",
      "pp_14: 24개 (0.2%)\n",
      "temp_20: 1014개 (8.5%)\n",
      "wd_20: 1014개 (8.5%)\n",
      "ws_20: 1014개 (8.5%)\n",
      "pp_20: 1014개 (8.5%)\n",
      "wd_sin: 3개 (0.0%)\n",
      "wd_cos: 3개 (0.0%)\n",
      "wd_14_sin: 24개 (0.2%)\n",
      "wd_14_cos: 24개 (0.2%)\n",
      "wd_20_sin: 1014개 (8.5%)\n",
      "wd_20_cos: 1014개 (8.5%)\n",
      "\n",
      "===== Missing Values After Processing =====\n",
      "모든 결측치가 성공적으로 처리되었습니다. NaN 값이 없습니다.\n",
      "===== Missing Values Before Processing =====\n",
      "temperature: 1개 (0.0%)\n",
      "humidity: 1개 (0.0%)\n",
      "rn: 11232개 (91.1%)\n",
      "ws: 1개 (0.0%)\n",
      "wd: 1개 (0.0%)\n",
      "pv: 1개 (0.0%)\n",
      "pa: 1개 (0.0%)\n",
      "ps: 1개 (0.0%)\n",
      "ss: 5696개 (46.2%)\n",
      "icsr: 7082개 (57.4%)\n",
      "dc10Tca: 99개 (0.8%)\n",
      "dc10LmcsCa: 30개 (0.2%)\n",
      "lcsCh: 5696개 (46.2%)\n",
      "vs: 3개 (0.0%)\n",
      "ts: 1개 (0.0%)\n",
      "SO2: 221개 (1.8%)\n",
      "CO: 608개 (4.9%)\n",
      "O3: 218개 (1.8%)\n",
      "NO2: 1950개 (15.8%)\n",
      "PM10: 247개 (2.0%)\n",
      "PM25: 233개 (1.9%)\n",
      "초미세먼지: 233개 (1.9%)\n",
      "temp_20: 1028개 (8.3%)\n",
      "wd_20: 1028개 (8.3%)\n",
      "ws_20: 1028개 (8.3%)\n",
      "pp_20: 1028개 (8.3%)\n",
      "wd_sin: 1개 (0.0%)\n",
      "wd_cos: 1개 (0.0%)\n",
      "wd_20_sin: 1028개 (8.3%)\n",
      "wd_20_cos: 1028개 (8.3%)\n",
      "\n",
      "===== Missing Values After Processing =====\n",
      "모든 결측치가 성공적으로 처리되었습니다. NaN 값이 없습니다.\n",
      "===== Missing Values Before Processing =====\n",
      "temperature: 53개 (0.4%)\n",
      "humidity: 74개 (0.6%)\n",
      "rn: 11574개 (93.1%)\n",
      "ws: 54개 (0.4%)\n",
      "wd: 54개 (0.4%)\n",
      "pv: 74개 (0.6%)\n",
      "pa: 53개 (0.4%)\n",
      "ps: 53개 (0.4%)\n",
      "ss: 5750개 (46.3%)\n",
      "icsr: 5750개 (46.3%)\n",
      "dc10Tca: 112개 (0.9%)\n",
      "dc10LmcsCa: 165개 (1.3%)\n",
      "lcsCh: 6115개 (49.2%)\n",
      "vs: 152개 (1.2%)\n",
      "ts: 55개 (0.4%)\n",
      "SO2: 896개 (7.2%)\n",
      "CO: 893개 (7.2%)\n",
      "O3: 894개 (7.2%)\n",
      "NO2: 893개 (7.2%)\n",
      "PM10: 943개 (7.6%)\n",
      "PM25: 945개 (7.6%)\n",
      "초미세먼지: 945개 (7.6%)\n",
      "temp_20: 1036개 (8.3%)\n",
      "wd_20: 1036개 (8.3%)\n",
      "ws_20: 1036개 (8.3%)\n",
      "pp_20: 1036개 (8.3%)\n",
      "wd_sin: 54개 (0.4%)\n",
      "wd_cos: 54개 (0.4%)\n",
      "wd_20_sin: 1036개 (8.3%)\n",
      "wd_20_cos: 1036개 (8.3%)\n",
      "\n",
      "===== Missing Values After Processing =====\n",
      "모든 결측치가 성공적으로 처리되었습니다. NaN 값이 없습니다.\n",
      "===== Missing Values Before Processing =====\n",
      "rn: 4575개 (90.8%)\n",
      "ss: 2294개 (45.5%)\n",
      "icsr: 2294개 (45.5%)\n",
      "dc10Tca: 1개 (0.0%)\n",
      "lcsCh: 2797개 (55.5%)\n",
      "SO2: 129개 (2.6%)\n",
      "CO: 126개 (2.5%)\n",
      "O3: 132개 (2.6%)\n",
      "NO2: 168개 (3.3%)\n",
      "PM10: 109개 (2.2%)\n",
      "PM25: 178개 (3.5%)\n",
      "초미세먼지: 178개 (3.5%)\n",
      "temp_14: 24개 (0.5%)\n",
      "wd_14: 24개 (0.5%)\n",
      "ws_14: 24개 (0.5%)\n",
      "temp_20: 24개 (0.5%)\n",
      "wd_20: 24개 (0.5%)\n",
      "ws_20: 24개 (0.5%)\n",
      "wd_14_sin: 24개 (0.5%)\n",
      "wd_14_cos: 24개 (0.5%)\n",
      "wd_20_sin: 24개 (0.5%)\n",
      "wd_20_cos: 24개 (0.5%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 89%|████████▉ | 33/37 [00:02<00:00, 11.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Missing Values After Processing =====\n",
      "모든 결측치가 성공적으로 처리되었습니다. NaN 값이 없습니다.\n",
      "===== Missing Values Before Processing =====\n",
      "temperature: 53개 (0.4%)\n",
      "humidity: 74개 (0.6%)\n",
      "rn: 11574개 (93.1%)\n",
      "ws: 54개 (0.4%)\n",
      "wd: 54개 (0.4%)\n",
      "pv: 74개 (0.6%)\n",
      "pa: 53개 (0.4%)\n",
      "ps: 53개 (0.4%)\n",
      "ss: 5750개 (46.3%)\n",
      "icsr: 5750개 (46.3%)\n",
      "dc10Tca: 112개 (0.9%)\n",
      "dc10LmcsCa: 165개 (1.3%)\n",
      "lcsCh: 6115개 (49.2%)\n",
      "vs: 152개 (1.2%)\n",
      "ts: 55개 (0.4%)\n",
      "SO2: 896개 (7.2%)\n",
      "CO: 893개 (7.2%)\n",
      "O3: 894개 (7.2%)\n",
      "NO2: 893개 (7.2%)\n",
      "PM10: 943개 (7.6%)\n",
      "PM25: 945개 (7.6%)\n",
      "초미세먼지: 945개 (7.6%)\n",
      "temp_20: 1036개 (8.3%)\n",
      "wd_20: 1036개 (8.3%)\n",
      "ws_20: 1036개 (8.3%)\n",
      "pp_20: 1036개 (8.3%)\n",
      "wd_sin: 54개 (0.4%)\n",
      "wd_cos: 54개 (0.4%)\n",
      "wd_20_sin: 1036개 (8.3%)\n",
      "wd_20_cos: 1036개 (8.3%)\n",
      "\n",
      "===== Missing Values After Processing =====\n",
      "모든 결측치가 성공적으로 처리되었습니다. NaN 값이 없습니다.\n",
      "===== Missing Values Before Processing =====\n",
      "rn: 27716개 (90.3%)\n",
      "ws: 30개 (0.1%)\n",
      "wd: 30개 (0.1%)\n",
      "ss: 13865개 (45.2%)\n",
      "icsr: 15108개 (49.2%)\n",
      "dc10Tca: 1163개 (3.8%)\n",
      "dc10LmcsCa: 81개 (0.3%)\n",
      "lcsCh: 16357개 (53.3%)\n",
      "vs: 8개 (0.0%)\n",
      "SO2: 683개 (2.2%)\n",
      "CO: 666개 (2.2%)\n",
      "O3: 661개 (2.2%)\n",
      "NO2: 836개 (2.7%)\n",
      "PM10: 809개 (2.6%)\n",
      "PM25: 822개 (2.7%)\n",
      "초미세먼지: 822개 (2.7%)\n",
      "temp_14: 24개 (0.1%)\n",
      "wd_14: 24개 (0.1%)\n",
      "ws_14: 24개 (0.1%)\n",
      "temp_20: 24개 (0.1%)\n",
      "wd_20: 24개 (0.1%)\n",
      "ws_20: 24개 (0.1%)\n",
      "wd_sin: 30개 (0.1%)\n",
      "wd_cos: 30개 (0.1%)\n",
      "wd_14_sin: 24개 (0.1%)\n",
      "wd_14_cos: 24개 (0.1%)\n",
      "wd_20_sin: 24개 (0.1%)\n",
      "wd_20_cos: 24개 (0.1%)\n",
      "\n",
      "===== Missing Values After Processing =====\n",
      "모든 결측치가 성공적으로 처리되었습니다. NaN 값이 없습니다.\n",
      "===== Missing Values Before Processing =====\n",
      "rn: 7184개 (89.1%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|█████████▍| 35/37 [00:03<00:00, 11.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ss: 3705개 (45.9%)\n",
      "icsr: 3705개 (45.9%)\n",
      "dc10Tca: 2개 (0.0%)\n",
      "dc10LmcsCa: 2개 (0.0%)\n",
      "lcsCh: 4176개 (51.8%)\n",
      "SO2: 217개 (2.7%)\n",
      "CO: 211개 (2.6%)\n",
      "O3: 217개 (2.7%)\n",
      "NO2: 277개 (3.4%)\n",
      "PM10: 199개 (2.5%)\n",
      "PM25: 362개 (4.5%)\n",
      "초미세먼지: 362개 (4.5%)\n",
      "temp_14: 24개 (0.3%)\n",
      "wd_14: 24개 (0.3%)\n",
      "ws_14: 24개 (0.3%)\n",
      "temp_20: 24개 (0.3%)\n",
      "wd_20: 24개 (0.3%)\n",
      "ws_20: 24개 (0.3%)\n",
      "wd_14_sin: 24개 (0.3%)\n",
      "wd_14_cos: 24개 (0.3%)\n",
      "wd_20_sin: 24개 (0.3%)\n",
      "wd_20_cos: 24개 (0.3%)\n",
      "\n",
      "===== Missing Values After Processing =====\n",
      "모든 결측치가 성공적으로 처리되었습니다. NaN 값이 없습니다.\n",
      "===== Missing Values Before Processing =====\n",
      "rn: 7184개 (89.1%)\n",
      "ss: 3705개 (45.9%)\n",
      "icsr: 3705개 (45.9%)\n",
      "dc10Tca: 2개 (0.0%)\n",
      "dc10LmcsCa: 2개 (0.0%)\n",
      "lcsCh: 4176개 (51.8%)\n",
      "SO2: 217개 (2.7%)\n",
      "CO: 211개 (2.6%)\n",
      "O3: 217개 (2.7%)\n",
      "NO2: 277개 (3.4%)\n",
      "PM10: 199개 (2.5%)\n",
      "PM25: 362개 (4.5%)\n",
      "초미세먼지: 362개 (4.5%)\n",
      "temp_14: 24개 (0.3%)\n",
      "wd_14: 24개 (0.3%)\n",
      "ws_14: 24개 (0.3%)\n",
      "temp_20: 24개 (0.3%)\n",
      "wd_20: 24개 (0.3%)\n",
      "ws_20: 24개 (0.3%)\n",
      "wd_14_sin: 24개 (0.3%)\n",
      "wd_14_cos: 24개 (0.3%)\n",
      "wd_20_sin: 24개 (0.3%)\n",
      "wd_20_cos: 24개 (0.3%)\n",
      "\n",
      "===== Missing Values After Processing =====\n",
      "모든 결측치가 성공적으로 처리되었습니다. NaN 값이 없습니다.\n",
      "===== Missing Values Before Processing =====\n",
      "temperature: 53개 (0.4%)\n",
      "humidity: 74개 (0.6%)\n",
      "rn: 11574개 (93.1%)\n",
      "ws: 54개 (0.4%)\n",
      "wd: 54개 (0.4%)\n",
      "pv: 74개 (0.6%)\n",
      "pa: 53개 (0.4%)\n",
      "ps: 53개 (0.4%)\n",
      "ss: 5750개 (46.3%)\n",
      "icsr: 5750개 (46.3%)\n",
      "dc10Tca: 112개 (0.9%)\n",
      "dc10LmcsCa: 165개 (1.3%)\n",
      "lcsCh: 6115개 (49.2%)\n",
      "vs: 152개 (1.2%)\n",
      "ts: 55개 (0.4%)\n",
      "SO2: 896개 (7.2%)\n",
      "CO: 893개 (7.2%)\n",
      "O3: 894개 (7.2%)\n",
      "NO2: 893개 (7.2%)\n",
      "PM10: 943개 (7.6%)\n",
      "PM25: 945개 (7.6%)\n",
      "초미세먼지: 945개 (7.6%)\n",
      "temp_20: 1036개 (8.3%)\n",
      "wd_20: 1036개 (8.3%)\n",
      "ws_20: 1036개 (8.3%)\n",
      "pp_20: 1036개 (8.3%)\n",
      "wd_sin: 54개 (0.4%)\n",
      "wd_cos: 54개 (0.4%)\n",
      "wd_20_sin: 1036개 (8.3%)\n",
      "wd_20_cos: 1036개 (8.3%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 37/37 [00:03<00:00, 11.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Missing Values After Processing =====\n",
      "모든 결측치가 성공적으로 처리되었습니다. NaN 값이 없습니다.\n",
      "===== Missing Values Before Processing =====\n",
      "rn: 19854개 (90.4%)\n",
      "ws: 30개 (0.1%)\n",
      "wd: 30개 (0.1%)\n",
      "ss: 9911개 (45.1%)\n",
      "icsr: 9919개 (45.2%)\n",
      "dc10Tca: 37개 (0.2%)\n",
      "dc10LmcsCa: 30개 (0.1%)\n",
      "lcsCh: 11707개 (53.3%)\n",
      "vs: 7개 (0.0%)\n",
      "SO2: 551개 (2.5%)\n",
      "CO: 537개 (2.4%)\n",
      "O3: 534개 (2.4%)\n",
      "NO2: 598개 (2.7%)\n",
      "PM10: 658개 (3.0%)\n",
      "PM25: 763개 (3.5%)\n",
      "초미세먼지: 763개 (3.5%)\n",
      "temp_14: 24개 (0.1%)\n",
      "wd_14: 24개 (0.1%)\n",
      "ws_14: 24개 (0.1%)\n",
      "temp_20: 24개 (0.1%)\n",
      "wd_20: 24개 (0.1%)\n",
      "ws_20: 24개 (0.1%)\n",
      "wd_sin: 30개 (0.1%)\n",
      "wd_cos: 30개 (0.1%)\n",
      "wd_14_sin: 24개 (0.1%)\n",
      "wd_14_cos: 24개 (0.1%)\n",
      "wd_20_sin: 24개 (0.1%)\n",
      "wd_20_cos: 24개 (0.1%)\n",
      "\n",
      "===== Missing Values After Processing =====\n",
      "모든 결측치가 성공적으로 처리되었습니다. NaN 값이 없습니다.\n",
      "Base feature columns (excluding forecast): 41\n",
      "14시 forecast columns: 5\n",
      "20시 forecast columns: 5\n",
      "Processed 37 plants\n",
      "Base feature columns (excluding forecast): 41\n",
      "14시 예보 컬럼: ['temp_14', 'wd_14', 'sc_14', 'ws_14', 'pp_14']\n",
      "20시 예보 컬럼: ['temp_20', 'wd_20', 'sc_20', 'ws_20', 'pp_20']\n",
      "Target column: value\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def load_and_preprocess_data(meta_data_path, data_dir):\n",
    "    \"\"\"\n",
    "    Load and preprocess all solar plant data files with forecast data\n",
    "    \n",
    "    Args:\n",
    "        meta_data_path: Path to metadata CSV\n",
    "        data_dir: Directory containing parquet files with forecast data\n",
    "    \n",
    "    Returns:\n",
    "        processed_data: Dictionary of preprocessed dataframes by plant\n",
    "        feature_cols: List of feature columns (excluding forecast columns)\n",
    "        forecast_cols_14: List of 14시 forecast columns\n",
    "        forecast_cols_20: List of 20시 forecast columns\n",
    "        target_col: Target column name\n",
    "        plants_info: Dictionary with information about each plant\n",
    "        meta_data: DataFrame containing plant metadata\n",
    "    \"\"\"\n",
    "    # Load metadata\n",
    "    meta_data = pd.read_csv(meta_data_path)\n",
    "    \n",
    "    # Get all files\n",
    "    files = os.listdir(data_dir)\n",
    "    \n",
    "    # Initialize dictionaries to store processed data\n",
    "    processed_data = {}\n",
    "    plants_info = {}\n",
    "    \n",
    "    # Define column types\n",
    "    time_cols = ['date', 'time', 'sunrise', 'sunset']\n",
    "    \n",
    "    # Weather/environment columns (numerical)\n",
    "    weather_numeric_cols = [\n",
    "        'temperature', 'humidity', 'ws', 'pv', 'pa', 'ps', \n",
    "        'ss', 'lcsCh','dc10Tca', 'dc10LmcsCa', 'vs', 'ts'\n",
    "    ]\n",
    "    \n",
    "    # Circular features (need sin/cos transformation)\n",
    "    circular_cols = ['wd']\n",
    "    \n",
    "    # Weather columns with special imputation (zero)\n",
    "    weather_zero_impute_cols = ['rn', 'icsr']\n",
    "    \n",
    "    # Categorical weather columns\n",
    "    categorical_cols = ['미세먼지', '초미세먼지']  # lcsCh 제외\n",
    "    \n",
    "    # Air quality columns (numerical)\n",
    "    air_numeric_cols = ['SO2', 'CO', 'O3', 'NO2', 'PM10', 'PM25']\n",
    "    \n",
    "    # Solar plant output columns \n",
    "    plant_output_cols = ['총량(kw)', '평균(kw)', '최대(kw)', '최소(kw)', '최대(시간별_kw)', '최소(시간별_kw)', 'value']\n",
    "    \n",
    "    # 14시 예보 컬럼 (직접 정의)\n",
    "    forecast_cols_14 = ['temp_14', 'wd_14', 'sc_14', 'ws_14', 'pp_14']\n",
    "    \n",
    "    # 20시 예보 컬럼 (직접 정의)\n",
    "    forecast_cols_20 = ['temp_20', 'wd_20', 'sc_20', 'ws_20', 'pp_20']\n",
    "    \n",
    "    # 예보 컬럼과 그 파생 컬럼들 (나중에 제외할 용도)\n",
    "    all_forecast_cols = forecast_cols_14 + forecast_cols_20 + [\n",
    "        'wd_14_sin', 'wd_14_cos', 'wd_20_sin', 'wd_20_cos'\n",
    "    ]\n",
    "    \n",
    "    # Target column\n",
    "    target_col = 'value'\n",
    "    \n",
    "    # All numeric columns for imputation\n",
    "    all_numeric_cols = (weather_numeric_cols + air_numeric_cols + plant_output_cols +\n",
    "                       [col for col in forecast_cols_14 + forecast_cols_20 \n",
    "                        if col not in ['sc_14', 'sc_20']])  # 하늘상태(sc)는 범주형\n",
    "    \n",
    "    # 샘플 파일 로드하여 실제 컬럼 확인\n",
    "    sample_files = [f for f in files if f.endswith('.parquet')]\n",
    "    if sample_files:\n",
    "        sample_file = sample_files[0]\n",
    "        try:\n",
    "            sample_df = pd.read_parquet(os.path.join(data_dir, sample_file))\n",
    "            # 실제 존재하는 예보 컬럼만 필터링\n",
    "            forecast_cols_14 = [col for col in forecast_cols_14 if col in sample_df.columns]\n",
    "            forecast_cols_20 = [col for col in forecast_cols_20 if col in sample_df.columns]\n",
    "            print(f\"Available 14시 forecast columns: {forecast_cols_14}\")\n",
    "            print(f\"Available 20시 forecast columns: {forecast_cols_20}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Could not check columns from sample file: {e}\")\n",
    "    \n",
    "    # Analyze all files to collect plant information\n",
    "    print(\"Collecting plant information...\")\n",
    "    for file_name in tqdm(files):\n",
    "        if not file_name.endswith('.parquet'):\n",
    "            continue\n",
    "            \n",
    "        plant_id = file_name.split('.')[0]  # Assuming filename contains plant ID\n",
    "        file_path = os.path.join(data_dir, file_name)\n",
    "        \n",
    "        # Load data - exclude '호기' column from the beginning\n",
    "        df = pd.read_parquet(file_path)\n",
    "        if '호기' in df.columns:\n",
    "            df = df.drop(columns=['호기'])\n",
    "        \n",
    "        # Extract date range information\n",
    "        if 'date' in df.columns:\n",
    "            min_date = df['date'].min()\n",
    "            max_date = df['date'].max()\n",
    "            date_range = f\"{min_date} ~ {max_date}\"\n",
    "        else:\n",
    "            date_range = \"Unknown\"\n",
    "        \n",
    "        # Store plant information\n",
    "        plants_info[plant_id] = {\n",
    "            'file_name': file_name,\n",
    "            'date_range': date_range,\n",
    "            'data_length': len(df),\n",
    "            'columns': list(df.columns)\n",
    "        }\n",
    "    \n",
    "    print(f\"Found {len(plants_info)} plants\")\n",
    "    \n",
    "    # Process each file\n",
    "    print(\"Processing each plant's data...\")\n",
    "    for file_name in tqdm(files):\n",
    "        if not file_name.endswith('.parquet'):\n",
    "            continue\n",
    "            \n",
    "        plant_id = file_name.split('.')[0]\n",
    "        file_path = os.path.join(data_dir, file_name)\n",
    "        \n",
    "        # Load data - exclude '호기' column\n",
    "        df = pd.read_parquet(file_path)\n",
    "        if '호기' in df.columns:\n",
    "            df = df.drop(columns=['호기'])\n",
    "        \n",
    "        # Process datetime columns - 시간 관련 특성 생성\n",
    "        df = process_time_features(df, time_cols)\n",
    "        \n",
    "        # Process special weather features (wd, etc.)\n",
    "        df = process_weather_features(df, circular_cols)\n",
    "        \n",
    "        # Process circular forecast features (wd_14, wd_20)\n",
    "        df = process_forecast_features(df, ['wd_14', 'wd_20'])\n",
    "\n",
    "        # NaN 값 처리 전 상태 출력 (NaN이 있는 컬럼만)\n",
    "        print(\"===== Missing Values Before Processing =====\")\n",
    "        for col in df.columns:\n",
    "            nan_count = df[col].isna().sum()\n",
    "            if nan_count > 0:\n",
    "                nan_ratio = df[col].isna().mean() * 100\n",
    "                print(f\"{col}: {nan_count}개 ({nan_ratio:.1f}%)\")\n",
    "\n",
    "        # Handle missing values according to the provided strategy\n",
    "        df = handle_missing_values(\n",
    "            df, \n",
    "            numeric_cols=all_numeric_cols,\n",
    "            zero_impute_cols=weather_zero_impute_cols,\n",
    "            categorical_cols=categorical_cols + ['sc_14', 'sc_20'],\n",
    "            circular_cols=['wd', 'wd_sin', 'wd_cos', 'wd_14_sin', 'wd_14_cos', \n",
    "                          'wd_20_sin', 'wd_20_cos']\n",
    "        )\n",
    "        \n",
    "        # NaN 값 처리 후 상태 출력 (NaN이 있는 컬럼만)\n",
    "        print(\"\\n===== Missing Values After Processing =====\")\n",
    "        nan_exists = False\n",
    "\n",
    "        for col in df.columns:\n",
    "            nan_count = df[col].isna().sum()\n",
    "            if nan_count > 0:\n",
    "                nan_ratio = df[col].isna().mean() * 100\n",
    "                print(f\"{col}: {nan_count}개 ({nan_ratio:.1f}%)\")\n",
    "                nan_exists = True\n",
    "\n",
    "        if not nan_exists:\n",
    "            print(\"모든 결측치가 성공적으로 처리되었습니다. NaN 값이 없습니다.\")\n",
    "                        \n",
    "        # Save to dictionary\n",
    "        processed_data[plant_id] = df\n",
    "    \n",
    "    # Define feature columns (excluding forecast columns and target)\n",
    "    if len(processed_data) > 0:\n",
    "        sample_df = processed_data[list(processed_data.keys())[0]]\n",
    "        \n",
    "        # Target column is the hourly generation ('value')\n",
    "        target_col = 'value'\n",
    "        \n",
    "        # Excluded columns (raw time columns, derived intermediate columns, and forecast columns)\n",
    "        excluded_cols = time_cols + ['datetime', 'hour', 'day_of_year', 'month', 'date_only'] + all_forecast_cols\n",
    "        \n",
    "        # Include 'value' in feature columns for historical data (과거 발전량 포함)\n",
    "        feature_cols = [col for col in sample_df.columns if col not in excluded_cols]\n",
    "        \n",
    "        print(f\"Base feature columns (excluding forecast): {len(feature_cols)}\")\n",
    "        print(f\"14시 forecast columns: {len(forecast_cols_14)}\")\n",
    "        print(f\"20시 forecast columns: {len(forecast_cols_20)}\")\n",
    "    else:\n",
    "        feature_cols = []\n",
    "    \n",
    "    return processed_data, feature_cols, forecast_cols_14, forecast_cols_20, target_col, plants_info, meta_data\n",
    "\n",
    "def process_time_features(df, time_cols):\n",
    "    \"\"\"Process time-related columns and extract useful features\"\"\"\n",
    "    \n",
    "    # Convert date and time to datetime if they exist\n",
    "    if 'date' in df.columns and 'time' in df.columns:\n",
    "        # Ensure date and time are string type\n",
    "        df['date'] = df['date'].astype(str)\n",
    "        df['time'] = df['time'].astype(str)\n",
    "        \n",
    "        # Create datetime column\n",
    "        df['datetime'] = pd.to_datetime(df['date'] + ' ' + df['time'], errors='coerce')\n",
    "        df['hour'] = df['datetime'].dt.hour\n",
    "        \n",
    "        # Create cyclical time features\n",
    "        df['time_hour_sin'] = np.sin(2 * np.pi * df['hour']/24)\n",
    "        df['time_hour_cos'] = np.cos(2 * np.pi * df['hour']/24)\n",
    "        \n",
    "        # Day of year (for seasonal patterns)\n",
    "        df['day_of_year'] = df['datetime'].dt.dayofyear\n",
    "        df['time_day_sin'] = np.sin(2 * np.pi * df['day_of_year']/365)\n",
    "        df['time_day_cos'] = np.cos(2 * np.pi * df['day_of_year']/365)\n",
    "        \n",
    "        # Month as a cyclical feature (for seasonal patterns)\n",
    "        df['month'] = df['datetime'].dt.month\n",
    "        df['time_month_sin'] = np.sin(2 * np.pi * df['month']/12)\n",
    "        df['time_month_cos'] = np.cos(2 * np.pi * df['month']/12)\n",
    "        \n",
    "        # Create features for day/night based on sunrise/sunset\n",
    "        if 'sunrise' in df.columns and 'sunset' in df.columns:\n",
    "            try:\n",
    "                # Convert sunrise/sunset to datetime\n",
    "                df['sunrise'] = pd.to_datetime(df['date'] + ' ' + df['sunrise'], errors='coerce')\n",
    "                df['sunset'] = pd.to_datetime(df['date'] + ' ' + df['sunset'], errors='coerce')\n",
    "                \n",
    "                # Is daylight (1 if current time is between sunrise and sunset)\n",
    "                df['time_is_daylight'] = ((df['datetime'] >= df['sunrise']) & \n",
    "                                     (df['datetime'] <= df['sunset'])).astype(int)\n",
    "                \n",
    "                # Hours since sunrise and hours until sunset\n",
    "                df['time_hours_since_sunrise'] = (df['datetime'] - df['sunrise']).dt.total_seconds() / 3600\n",
    "                df['time_hours_until_sunset'] = (df['sunset'] - df['datetime']).dt.total_seconds() / 3600\n",
    "                \n",
    "                # Replace negative values with 0 (before sunrise or after sunset)\n",
    "                df['time_hours_since_sunrise'] = df['time_hours_since_sunrise'].clip(lower=0)\n",
    "                df['time_hours_until_sunset'] = df['time_hours_until_sunset'].clip(lower=0)\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing sunrise/sunset: {e}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "def process_weather_features(df, circular_cols):\n",
    "    \"\"\"특수한 날씨 피처들에 대한 전처리\"\"\"\n",
    "    \n",
    "    # 풍향(wd)의 순환적 특성 처리\n",
    "    if 'wd' in df.columns and 'wd' in circular_cols:\n",
    "        # 풍향이 0-360도 범위인지 확인\n",
    "        max_wd = df['wd'].max()\n",
    "        if pd.notna(max_wd):  # NaN 체크\n",
    "            if max_wd <= 360:\n",
    "                # 사인/코사인 변환\n",
    "                df['wd_sin'] = np.sin(2 * np.pi * df['wd'] / 360)\n",
    "                df['wd_cos'] = np.cos(2 * np.pi * df['wd'] / 360)\n",
    "            # 만약 다른 범위라면 적절히 조정\n",
    "            else:\n",
    "                df['wd_sin'] = np.sin(2 * np.pi * df['wd'] / max_wd)\n",
    "                df['wd_cos'] = np.cos(2 * np.pi * df['wd'] / max_wd)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def process_forecast_features(df, circular_forecast_cols):\n",
    "    \"\"\"\n",
    "    예보 데이터의 순환적 특성 처리 (풍향 등)\n",
    "    \n",
    "    Args:\n",
    "        df: 데이터프레임\n",
    "        circular_forecast_cols: 순환적 특성을 가진 예보 컬럼 목록 (wd_14, wd_20 등)\n",
    "    \n",
    "    Returns:\n",
    "        처리된 데이터프레임\n",
    "    \"\"\"\n",
    "    # 각 순환적 특성 처리\n",
    "    for col in circular_forecast_cols:\n",
    "        if col in df.columns:\n",
    "            # 범위 확인 (풍향은 보통 0-360도)\n",
    "            max_val = df[col].max()\n",
    "            if pd.notna(max_val):  # NaN 체크\n",
    "                if max_val <= 360:\n",
    "                    # 사인/코사인 변환\n",
    "                    df[f'{col}_sin'] = np.sin(2 * np.pi * df[col] / 360)\n",
    "                    df[f'{col}_cos'] = np.cos(2 * np.pi * df[col] / 360)\n",
    "                # 다른 범위인 경우 조정\n",
    "                else:\n",
    "                    df[f'{col}_sin'] = np.sin(2 * np.pi * df[col] / max_val)\n",
    "                    df[f'{col}_cos'] = np.cos(2 * np.pi * df[col] / max_val)\n",
    "    \n",
    "    return df\n",
    "\n",
    "'''def handle_missing_values(df, numeric_cols, zero_impute_cols, categorical_cols, circular_cols):\n",
    "    \"\"\"\n",
    "    Handle missing values based on the provided strategy\n",
    "    \n",
    "    Strategy:\n",
    "    1. For numeric weather features: Consider time pattern and use forward filling, \n",
    "       then anomaly imputation (median)\n",
    "    2. For rn, icsr: Fill NaN with 0\n",
    "    3. For categorical values: Forward fill, then mode imputation\n",
    "    4. For circular features (wd_sin, wd_cos): Handle as numeric\n",
    "    \"\"\"\n",
    "    \n",
    "    # First identify all columns that exist in the dataframe\n",
    "    existing_numeric_cols = [col for col in numeric_cols if col in df.columns]\n",
    "    existing_zero_impute_cols = [col for col in zero_impute_cols if col in df.columns]\n",
    "    existing_cat_cols = [col for col in categorical_cols if col in df.columns]\n",
    "    existing_circular_cols = [col for col in circular_cols if col in df.columns]\n",
    "    \n",
    "    # Group data by hour to capture daily patterns (if datetime exists)\n",
    "    if 'datetime' in df.columns and 'hour' in df.columns:\n",
    "        # Handle numeric columns with time-based strategy\n",
    "        for col in existing_numeric_cols + existing_circular_cols:\n",
    "            if col not in df.columns:\n",
    "                continue\n",
    "                \n",
    "            # First try forward fill (for small gaps)\n",
    "            df[col] = df.groupby(['hour'])[col].transform(lambda x: x.ffill())\n",
    "            \n",
    "            # For remaining NaNs, use median by hour\n",
    "            hourly_medians = df.groupby(['hour'])[col].transform('median')\n",
    "            df[col] = df[col].fillna(hourly_medians)\n",
    "            \n",
    "            # If still NaN, use overall median\n",
    "            df[col] = df[col].fillna(df[col].median())\n",
    "    else:\n",
    "        # Fallback to simple median imputation\n",
    "        for col in existing_numeric_cols + existing_circular_cols:\n",
    "            if col in df.columns:\n",
    "                df[col] = df[col].fillna(df[col].median())\n",
    "    \n",
    "    # Handle zero-imputation columns\n",
    "    for col in existing_zero_impute_cols:\n",
    "        if col in df.columns:\n",
    "            df[col] = df[col].fillna(0)\n",
    "    \n",
    "    # Handle categorical columns\n",
    "    for col in existing_cat_cols:\n",
    "        if col not in df.columns:\n",
    "            continue\n",
    "            \n",
    "        # First try forward fill \n",
    "        df[col] = df[col].ffill()\n",
    "        \n",
    "        # For remaining NaNs, use mode\n",
    "        if not df[col].mode().empty:\n",
    "            df[col] = df[col].fillna(df[col].mode().iloc[0])\n",
    "        else:\n",
    "            df[col] = df[col].fillna('unknown')\n",
    "    \n",
    "    return df\n",
    "'''\n",
    "\n",
    "def handle_missing_values(df, numeric_cols, zero_impute_cols, categorical_cols, circular_cols):\n",
    "    \"\"\"\n",
    "    Handle missing values based on the provided strategy\n",
    "    \n",
    "    Strategy:\n",
    "    1. For numeric weather features: \n",
    "       a. First try linear interpolation for whole dataset\n",
    "       b. Then consider time pattern and use forward filling for remaining gaps\n",
    "       c. Finally use anomaly imputation (median) if still missing\n",
    "    2. For rn, icsr: Fill NaN with 0\n",
    "    3. For categorical values: Forward fill, then mode imputation\n",
    "    4. For circular features (wd_sin, wd_cos): Handle as numeric\n",
    "    \"\"\"\n",
    "    \n",
    "    # First identify all columns that exist in the dataframe\n",
    "    existing_numeric_cols = [col for col in numeric_cols if col in df.columns]\n",
    "    existing_zero_impute_cols = [col for col in zero_impute_cols if col in df.columns]\n",
    "    existing_cat_cols = [col for col in categorical_cols if col in df.columns]\n",
    "    existing_circular_cols = [col for col in circular_cols if col in df.columns]\n",
    "    \n",
    "    # Handle numeric and circular columns\n",
    "    for col in existing_numeric_cols + existing_circular_cols:\n",
    "        if col not in df.columns:\n",
    "            continue\n",
    "\n",
    "        # Step 0: 강제로 float 변환\n",
    "        df[col] = pd.to_numeric(df[col], errors='coerce').astype('float64')     \n",
    "        \n",
    "        # Step 1: Linear interpolation on the entire dataset\n",
    "        df[col] = df[col].interpolate(method='linear', limit_direction='both')\n",
    "        \n",
    "        # If there are still NaNs and we have datetime information\n",
    "        if df[col].isna().any() and 'datetime' in df.columns and 'hour' in df.columns:\n",
    "            # Step 2: Forward fill \n",
    "            df = df.sort_values(by=['datetime'])\n",
    "            df[col] = df[col].ffil()\n",
    "            #df[col] = df.groupby('hour')[col].transform(lambda x: x.ffill())\n",
    "            \n",
    "            # Step 3: Use hour-specific medians for remaining NaNs\n",
    "            hourly_medians = df.groupby('hour')[col].transform('median')\n",
    "            df[col] = df[col].fillna(hourly_medians)\n",
    "        \n",
    "        # Step 4: If still NaN, use overall median\n",
    "        overall_median = df[col].median()\n",
    "        if pd.notna(overall_median):\n",
    "            df[col] = df[col].fillna(overall_median)\n",
    "    \n",
    "    # Handle zero-imputation columns\n",
    "    for col in existing_zero_impute_cols:\n",
    "        if col in df.columns:\n",
    "            df[col] = df[col].fillna(0)\n",
    "    \n",
    "    # Handle categorical columns\n",
    "    for col in existing_cat_cols:\n",
    "        if col not in df.columns:\n",
    "            continue\n",
    "            \n",
    "        # First try forward fill \n",
    "        df[col] = df[col].ffill()\n",
    "        \n",
    "        # For remaining NaNs, use mode\n",
    "        if not df[col].mode().empty:\n",
    "            df[col] = df[col].fillna(df[col].mode().iloc[0])\n",
    "        else:\n",
    "            df[col] = df[col].fillna('unknown')\n",
    "    \n",
    "    return df\n",
    "\n",
    "def prepare_forecast_data(processed_data, feature_cols, forecast_cols_14, forecast_cols_20, \n",
    "                         target_col='value', history_days=1, include_forecast=True, meta_data=None):\n",
    "    \"\"\"\n",
    "    예보 데이터와 메타데이터를 포함한 시계열 예측 데이터 준비\n",
    "    \n",
    "    Args:\n",
    "        processed_data: 전처리된 데이터 딕셔너리\n",
    "        feature_cols: 입력 피처 칼럼 목록 (예보 제외)\n",
    "        forecast_cols_14: 14시 예보 컬럼 목록\n",
    "        forecast_cols_20: 20시 예보 컬럼 목록\n",
    "        target_col: 예측할 타겟 칼럼 ('value')\n",
    "        history_days: 과거 데이터를 몇 일치 포함할지 (기본값: 1)\n",
    "        include_forecast: 예보 데이터 포함 여부\n",
    "        meta_data: 발전소 메타데이터 DataFrame\n",
    "        \n",
    "    Returns:\n",
    "        forecast_data: 예보 데이터와 메타데이터를 포함한 시계열 데이터 딕셔너리\n",
    "    \"\"\"\n",
    "    forecast_data = {\n",
    "        '14시': {},  # 14시 예측 시나리오\n",
    "        '20시': {}   # 20시 예측 시나리오\n",
    "    }\n",
    "\n",
    "    # 컬럼 정보 저장 딕셔너리\n",
    "    column_info = {\n",
    "        '14시': {\n",
    "            'historical_cols': feature_cols + forecast_cols_14,                   # 과거 데이터 컬럼 (기본 특성, 'value' 포함)\n",
    "            'base_feature_cols': feature_cols,                 # 기본 특성 컬럼만\n",
    "            'forecast_cols': forecast_cols_14                  # 예보 컬럼만\n",
    "        },\n",
    "        '20시': {\n",
    "            'historical_cols': feature_cols + forecast_cols_20,                   # 과거 데이터 컬럼 (기본 특성, 'value' 포함)\n",
    "            'base_feature_cols': feature_cols,                 # 기본 특성 컬럼만\n",
    "            'forecast_cols': forecast_cols_20                  # 예보 컬럼만\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # 메타데이터 처리 - 발전소별 정보 추출\n",
    "    plant_meta = {}\n",
    "    if meta_data is not None:\n",
    "        for _, row in meta_data.iterrows():\n",
    "            plant_name = row['name']\n",
    "            plant_meta[plant_name] = {\n",
    "                'capacity_kw': float(row['용량(MW)']) * 1000,  # MW -> kW 변환\n",
    "                'year_built': int(row['준공년도']),\n",
    "                'plant_age': 2025 - int(row['준공년도']),  # 현재 연도 기준 발전소 나이\n",
    "                'region': row['광역시'] if pd.notna(row['광역시']) else \"Unknown\",\n",
    "                'city': row['city'] if pd.notna(row['city']) else \"Unknown\",\n",
    "                'district': row['district'] if pd.notna(row['district']) else \"Unknown\"\n",
    "            }\n",
    "    \n",
    "    # 각 발전소별 데이터 처리\n",
    "    for plant_id, df in processed_data.items():\n",
    "        if 'datetime' not in df.columns or target_col not in df.columns:\n",
    "            continue\n",
    "        \n",
    "        # 날짜별로 데이터 정렬\n",
    "        df = df.sort_values('datetime')\n",
    "        \n",
    "        # 날짜만 추출하여 고유한 날짜 목록 생성\n",
    "        if 'date_only' not in df.columns:\n",
    "            df['date_only'] = df['datetime'].dt.date\n",
    "            \n",
    "        unique_dates = df['date_only'].unique()\n",
    "        \n",
    "        if len(unique_dates) <= history_days + 1:  # 최소 (history_days + 1)일 이상의 데이터가 필요\n",
    "            print(f\"Plant {plant_id} has insufficient data: {len(unique_dates)} days\")\n",
    "            continue\n",
    "        \n",
    "        # 각 날짜별로 24시간 데이터 그룹화\n",
    "        date_groups = {}\n",
    "        for date in unique_dates:\n",
    "            date_df = df[df['date_only'] == date]\n",
    "            if len(date_df) == 24:  # 하루에 24시간 데이터가 모두 있는 경우만 사용\n",
    "                date_groups[date] = date_df.sort_values('hour')\n",
    "        \n",
    "        # 날짜 그룹을 시간순으로 정렬\n",
    "        sorted_dates = sorted(date_groups.keys())\n",
    "        \n",
    "        # 메타데이터 추출\n",
    "        meta_features = {}\n",
    "        if plant_id in plant_meta:\n",
    "            meta_features = plant_meta[plant_id]\n",
    "        else:\n",
    "            # 메타데이터가 없는 경우 기본값 설정\n",
    "            meta_features = {\n",
    "                'capacity_kw': 0.0,\n",
    "                'year_built': 0,\n",
    "                'plant_age': 0,\n",
    "                'region': \"Unknown\",\n",
    "                'city': \"Unknown\",\n",
    "                'district': \"Unknown\"\n",
    "            }\n",
    "        \n",
    "        # 두 예측 시나리오에 대한 데이터 준비\n",
    "        for scenario in ['14시', '20시']:\n",
    "            cutoff_hour = 14 if scenario == '14시' else 20\n",
    "            \n",
    "            # 시나리오에 맞는 예보 컬럼만 사용\n",
    "            forecast_cols = forecast_cols_14 if scenario == '14시' else forecast_cols_20\n",
    "            \n",
    "            # 과거 데이터에 포함할 특성 (기본 특성 + value)\n",
    "            historical_feature_cols = feature_cols + forecast_cols\n",
    "            \n",
    "            # 시나리오별 데이터 저장용 (딕셔너리 형태로 저장)\n",
    "            samples = []\n",
    "            \n",
    "            # 각 예측 대상 날짜에 대해 데이터 생성\n",
    "            for i in range(len(sorted_dates) - history_days - 1):\n",
    "                # 과거 데이터 날짜들 (D-history_days ~ D-1)\n",
    "                history_dates = sorted_dates[i:i+history_days]\n",
    "                \n",
    "                # 전날 (D-1)\n",
    "                prev_date = sorted_dates[i+history_days-1]\n",
    "                \n",
    "                # 예측 대상일 (D-day)\n",
    "                target_date = sorted_dates[i+history_days]\n",
    "                \n",
    "                # 과거 데이터 (D-history_days ~ D-2)\n",
    "                history_data = []\n",
    "                for hist_date in history_dates[:-1]:  # 전전날까지\n",
    "                    if hist_date in date_groups:\n",
    "                        # 과거 데이터에 예보 정보 포함 (+ value 포함)\n",
    "                        history_data.append(date_groups[hist_date][historical_feature_cols].values)\n",
    "                \n",
    "                # 전날 데이터 (D-1)\n",
    "                prev_data = date_groups[prev_date]\n",
    "                \n",
    "                # 전날 cutoff_hour 이전 데이터만 사용\n",
    "                prev_data_cutoff = prev_data[prev_data['hour'] < cutoff_hour]\n",
    "                \n",
    "                # 전날 데이터에도 예보 정보 포함 (+ value 포함)\n",
    "                prev_with_padding = prev_data_cutoff[historical_feature_cols].values\n",
    "                \n",
    "                # 패딩 추가 (전날 cutoff_hour ~ 23시까지)\n",
    "                if cutoff_hour < 24:\n",
    "                    # 패딩 크기 계산\n",
    "                    padding_size = 24 - cutoff_hour\n",
    "                    \n",
    "                    # 0으로 채운 패딩 생성\n",
    "                    padding = np.zeros((padding_size, len(historical_feature_cols)))\n",
    "                    \n",
    "                    # 전날 데이터와 패딩 결합\n",
    "                    prev_with_padding = np.vstack([prev_with_padding, padding])\n",
    "                \n",
    "                # 과거 데이터 리스트 생성 (D-history_days ~ D-2, D-1+패딩)\n",
    "                X_historical_parts = history_data + [prev_with_padding]\n",
    "                \n",
    "                # 과거 데이터 결합\n",
    "                X_historical = np.vstack(X_historical_parts)  # 형태: (history_days*24, feature_dim)\n",
    "                \n",
    "                # 타겟 데이터 (D-day)\n",
    "                target_data = date_groups[target_date]\n",
    "                \n",
    "                # 예보 데이터 추가 (시나리오에 맞는 예보 컬럼만 사용)\n",
    "                # 여기서는 D-day에 대한 예보 데이터만 포함 (과거 예보는 이미 X_historical에 포함됨)\n",
    "                if include_forecast and forecast_cols:\n",
    "                    # 예측 대상일(D-day)의 예보 데이터\n",
    "                    X_forecast = target_data[forecast_cols].values  # 형태: (24, forecast_dim)\n",
    "                else:\n",
    "                    # 예보 데이터가 없는 경우 빈 배열\n",
    "                    X_forecast = np.zeros((24, len(forecast_cols) if forecast_cols else 0))\n",
    "                \n",
    "                # 메타데이터 처리\n",
    "                # 메타데이터는 각 샘플에 대해 동일한 값으로 사용됨\n",
    "                X_meta = np.array([\n",
    "                    float(meta_features['capacity_kw']),\n",
    "                    float(meta_features['plant_age']),\n",
    "                    meta_features['region'],\n",
    "                ])\n",
    "                \n",
    "                # 지역 정보를 원-핫 인코딩으로 처리하려면 별도 처리 필요\n",
    "                # 현재는 간단하게 숫자 특성만 사용\n",
    "                \n",
    "                # 데이터 딕셔너리로 저장\n",
    "                sample = {\n",
    "                    'X_historical': X_historical,\n",
    "                    'X_forecast': X_forecast,\n",
    "                    'X_meta': X_meta,\n",
    "                    'y': target_data[target_col].values,\n",
    "                    'dates': target_data['datetime'].values\n",
    "                }\n",
    "                \n",
    "                samples.append(sample)\n",
    "            \n",
    "            # 발전소별, 시나리오별 데이터 저장\n",
    "            if samples:\n",
    "                forecast_data[scenario][plant_id] = samples\n",
    "    \n",
    "    # 메타데이터 컬럼 이름 저장\n",
    "    meta_column_info = {\n",
    "        'meta_cols': ['capacity_kw', 'plant_age', 'region']\n",
    "    }\n",
    "    \n",
    "    # 컬럼 정보에 메타데이터 컬럼 추가\n",
    "    for scenario in column_info:\n",
    "        column_info[scenario].update(meta_column_info)\n",
    "    \n",
    "    return forecast_data, column_info\n",
    "\n",
    "def split_forecast_data(forecast_data, plants_info, external_test_ratio=0.2, train_ratio=0.7, valid_ratio=0.15):\n",
    "    \"\"\"\n",
    "    예보 데이터를 포함한 시계열 데이터를 학습/검증/테스트 세트로 분할\n",
    "    \n",
    "    전략:\n",
    "    1. 일부 발전소(데이터가 적은)를 external_test_plants로 분리 - 모델 학습에 전혀 사용하지 않음\n",
    "    2. 나머지 발전소 데이터는 시간 순서대로 train/valid/test로 분할\n",
    "    \n",
    "    Args:\n",
    "        forecast_data: 예보 데이터를 포함한 시계열 데이터 딕셔너리 {'14시': {...}, '20시': {...}}\n",
    "        plants_info: 각 발전소 정보를 담은 딕셔너리\n",
    "        external_test_ratio: 외부 테스트용 발전소 비율\n",
    "        train_ratio: 학습 데이터 비율 (external test를 제외한 나머지에서)\n",
    "        valid_ratio: 검증 데이터 비율 (external test를 제외한 나머지에서)\n",
    "        \n",
    "    Returns:\n",
    "        split_forecast_data: 분할된 데이터 딕셔너리\n",
    "    \"\"\"\n",
    "    # 시나리오별 분할 데이터 저장 딕셔너리\n",
    "    split_forecast_data = {}\n",
    "    \n",
    "    # 데이터가 있는 발전소 식별\n",
    "    available_plants = set()\n",
    "    for scenario in forecast_data:\n",
    "        for plant_id in forecast_data[scenario]:\n",
    "            available_plants.add(plant_id)\n",
    "    \n",
    "    # 발전소를 데이터 길이에 따라 정렬 (오름차순 - 데이터가 적은 순)\n",
    "    sorted_plants = sorted(\n",
    "        list(available_plants), \n",
    "        key=lambda p: plants_info[p]['data_length'] if 'data_length' in plants_info[p] else 0\n",
    "    )\n",
    "    \n",
    "    # 외부 테스트용 발전소 선택 (데이터가 적은 발전소들)\n",
    "    n_external_test = max(1, int(len(sorted_plants) * external_test_ratio))\n",
    "    external_test_plants = sorted_plants[:n_external_test]\n",
    "    train_valid_test_plants = sorted_plants[n_external_test:]\n",
    "    \n",
    "    print(f\"External test plants: {external_test_plants}\")\n",
    "    print(f\"Regular split plants: {train_valid_test_plants}\")\n",
    "    \n",
    "    # 각 시나리오별 데이터 분할\n",
    "    for scenario in forecast_data:\n",
    "        split_data = {\n",
    "            'train': {\n",
    "                'X_historical': [], 'X_forecast': [], 'X_meta': [], 'y': [], 'dates': [], 'plants': []\n",
    "            },\n",
    "            'valid': {\n",
    "                'X_historical': [], 'X_forecast': [], 'X_meta': [], 'y': [], 'dates': [], 'plants': []\n",
    "            },\n",
    "            'test': {\n",
    "                'X_historical': [], 'X_forecast': [], 'X_meta': [], 'y': [], 'dates': [], 'plants': []\n",
    "            },\n",
    "            'external_test': {\n",
    "                'X_historical': [], 'X_forecast': [], 'X_meta': [], 'y': [], 'dates': [], 'plants': []\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # 외부 테스트 발전소 데이터 분리\n",
    "        for plant_id in external_test_plants:\n",
    "            if plant_id not in forecast_data[scenario]:\n",
    "                continue\n",
    "                \n",
    "            samples = forecast_data[scenario][plant_id]\n",
    "            n_samples = len(samples)\n",
    "            \n",
    "            for sample in samples:\n",
    "                split_data['external_test']['X_historical'].append(sample['X_historical'])\n",
    "                split_data['external_test']['X_forecast'].append(sample['X_forecast'])\n",
    "                split_data['external_test']['X_meta'].append(sample['X_meta'])\n",
    "                split_data['external_test']['y'].append(sample['y'])\n",
    "                split_data['external_test']['dates'].append(sample['dates'])\n",
    "                split_data['external_test']['plants'].append(plant_id)\n",
    "        \n",
    "        # 나머지 발전소 데이터를 시간 순서대로 분할\n",
    "        for plant_id in train_valid_test_plants:\n",
    "            if plant_id not in forecast_data[scenario]:\n",
    "                continue\n",
    "                \n",
    "            samples = forecast_data[scenario][plant_id]\n",
    "            n_samples = len(samples)\n",
    "            if n_samples < 3:  # 최소 3개 이상의 샘플이 필요\n",
    "                continue\n",
    "                \n",
    "            # 인덱스 계산\n",
    "            train_idx = int(n_samples * train_ratio)\n",
    "            valid_idx = int(n_samples * (train_ratio + valid_ratio))\n",
    "            \n",
    "            # 데이터 분할 - 시간 순서대로 분할\n",
    "            for i, sample in enumerate(samples):\n",
    "                if i < train_idx:\n",
    "                    # Train set\n",
    "                    split_data['train']['X_historical'].append(sample['X_historical'])\n",
    "                    split_data['train']['X_forecast'].append(sample['X_forecast'])\n",
    "                    split_data['train']['X_meta'].append(sample['X_meta'])\n",
    "                    split_data['train']['y'].append(sample['y'])\n",
    "                    split_data['train']['dates'].append(sample['dates'])\n",
    "                    split_data['train']['plants'].append(plant_id)\n",
    "                elif i < valid_idx:\n",
    "                    # Validation set\n",
    "                    split_data['valid']['X_historical'].append(sample['X_historical'])\n",
    "                    split_data['valid']['X_forecast'].append(sample['X_forecast'])\n",
    "                    split_data['valid']['X_meta'].append(sample['X_meta'])\n",
    "                    split_data['valid']['y'].append(sample['y'])\n",
    "                    split_data['valid']['dates'].append(sample['dates'])\n",
    "                    split_data['valid']['plants'].append(plant_id)\n",
    "                else:\n",
    "                    # Test set\n",
    "                    split_data['test']['X_historical'].append(sample['X_historical'])\n",
    "                    split_data['test']['X_forecast'].append(sample['X_forecast'])\n",
    "                    split_data['test']['X_meta'].append(sample['X_meta'])\n",
    "                    split_data['test']['y'].append(sample['y'])\n",
    "                    split_data['test']['dates'].append(sample['dates'])\n",
    "                    split_data['test']['plants'].append(plant_id)\n",
    "        \n",
    "        # 배열로 변환\n",
    "        for split_name in split_data:\n",
    "            if split_data[split_name]['X_historical']:  # 비어있지 않은 경우만 처리\n",
    "                split_data[split_name]['X_historical'] = np.array(split_data[split_name]['X_historical'])\n",
    "                split_data[split_name]['X_forecast'] = np.array(split_data[split_name]['X_forecast'])\n",
    "                split_data[split_name]['X_meta'] = np.array(split_data[split_name]['X_meta'])\n",
    "                split_data[split_name]['y'] = np.array(split_data[split_name]['y'])\n",
    "                split_data[split_name]['dates'] = np.array(split_data[split_name]['dates'])\n",
    "                # plants는 리스트로 유지 (식별자이므로 변환 불필요)\n",
    "        \n",
    "        # 시나리오별 분할 데이터 저장\n",
    "        split_forecast_data[scenario] = split_data\n",
    "    \n",
    "    return split_forecast_data\n",
    "\n",
    "def save_forecast_data(split_forecast_data, output_dir='../data/forecast_modeling_data'):\n",
    "    \"\"\"\n",
    "    분할된 예보 데이터 저장\n",
    "    \n",
    "    Args:\n",
    "        split_forecast_data: 분할된 예보 데이터 딕셔너리 (시나리오별)\n",
    "        output_dir: 저장할 디렉토리 경로\n",
    "    \"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    for scenario, split_data in split_forecast_data.items():\n",
    "        scenario_dir = os.path.join(output_dir, scenario)\n",
    "        os.makedirs(scenario_dir, exist_ok=True)\n",
    "        \n",
    "        for split_name, data in split_data.items():\n",
    "            if data['X_historical'].size == 0:  # 빈 데이터 세트 건너뛰기\n",
    "                continue\n",
    "                \n",
    "            split_dir = os.path.join(scenario_dir, split_name)\n",
    "            os.makedirs(split_dir, exist_ok=True)\n",
    "            \n",
    "            # 데이터 저장\n",
    "            np.save(os.path.join(split_dir, 'X_historical.npy'), data['X_historical'])\n",
    "            np.save(os.path.join(split_dir, 'X_forecast.npy'), data['X_forecast'])\n",
    "            np.save(os.path.join(split_dir, 'X_meta.npy'), data['X_meta'])\n",
    "            np.save(os.path.join(split_dir, 'y.npy'), data['y'])\n",
    "            np.save(os.path.join(split_dir, 'dates.npy'), data['dates'])\n",
    "            np.save(os.path.join(split_dir, 'plants.npy'), np.array(data['plants']))\n",
    "            \n",
    "            print(f\"Saved {split_name} data for {scenario} scenario:\")\n",
    "            print(f\"  X_historical shape: {data['X_historical'].shape}\")\n",
    "            print(f\"  X_forecast shape: {data['X_forecast'].shape}\")\n",
    "            print(f\"  X_meta shape: {data['X_meta'].shape}\")\n",
    "            print(f\"  y shape: {data['y'].shape}\")\n",
    "\n",
    "\n",
    "# Define paths\n",
    "META_DATA_PATH = '../data/solar_energy/meta_data.csv'\n",
    "data_dir = '../data/concat_forecast_data'\n",
    "output_dir = '../data/forecast_modeling_data'\n",
    "\n",
    "# 과거 데이터 일수, 외부 테스트용 발전소 비율\n",
    "history_days = 2  # 전날 + 전전날 데이터 사용\n",
    "external_test_ratio = 0.2\n",
    "\n",
    "# 데이터 로드 및 전처리\n",
    "processed_data, feature_cols, forecast_cols_14, forecast_cols_20, target_col, plants_info, meta_data = load_and_preprocess_data(\n",
    "    META_DATA_PATH, data_dir\n",
    ")\n",
    "\n",
    "# 결과 요약 출력\n",
    "print(f\"Processed {len(processed_data)} plants\")\n",
    "print(f\"Base feature columns (excluding forecast): {len(feature_cols)}\")\n",
    "print(f\"14시 예보 컬럼: {forecast_cols_14}\")\n",
    "print(f\"20시 예보 컬럼: {forecast_cols_20}\")\n",
    "print(f\"Target column: {target_col}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7ebde460",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded metadata for 46 plants\n",
      "Metadata columns: ['name', '발전소명', '구분', '용량(MW)', '준공년도', '비 고', 'location_name', '광역시', 'city', 'district']\n",
      "Column information for 14시 saved to ../data/forecast_modeling_data/14시_columns.json\n",
      "Column information for 20시 saved to ../data/forecast_modeling_data/20시_columns.json\n",
      "\n",
      "14시 시나리오:\n",
      "  발전소 수: 37\n",
      "  총 샘플 수: 24741\n",
      "  발전소 부산운동장: 1276 샘플\n",
      "    X_historical 형태: (48, 46)\n",
      "    X_forecast 형태: (24, 5)\n",
      "    X_meta 형태: (3,)\n",
      "    y 형태: (24,)\n",
      "  발전소 화촌주민참여형: 515 샘플\n",
      "    X_historical 형태: (48, 46)\n",
      "    X_forecast 형태: (24, 5)\n",
      "    X_meta 형태: (3,)\n",
      "    y 형태: (24,)\n",
      "  발전소 삼척소내_2: 879 샘플\n",
      "    X_historical 형태: (48, 46)\n",
      "    X_forecast 형태: (24, 5)\n",
      "    X_meta 형태: (3,)\n",
      "    y 형태: (24,)\n",
      "  발전소 영월본부: 911 샘플\n",
      "    X_historical 형태: (48, 46)\n",
      "    X_forecast 형태: (24, 5)\n",
      "    X_meta 형태: (3,)\n",
      "    y 형태: (24,)\n",
      "  발전소 하동변전소: 515 샘플\n",
      "    X_historical 형태: (48, 46)\n",
      "    X_forecast 형태: (24, 5)\n",
      "    X_meta 형태: (3,)\n",
      "    y 형태: (24,)\n",
      "\n",
      "20시 시나리오:\n",
      "  발전소 수: 37\n",
      "  총 샘플 수: 24741\n",
      "  발전소 부산운동장: 1276 샘플\n",
      "    X_historical 형태: (48, 46)\n",
      "    X_forecast 형태: (24, 5)\n",
      "    X_meta 형태: (3,)\n",
      "    y 형태: (24,)\n",
      "  발전소 화촌주민참여형: 515 샘플\n",
      "    X_historical 형태: (48, 46)\n",
      "    X_forecast 형태: (24, 5)\n",
      "    X_meta 형태: (3,)\n",
      "    y 형태: (24,)\n",
      "  발전소 삼척소내_2: 879 샘플\n",
      "    X_historical 형태: (48, 46)\n",
      "    X_forecast 형태: (24, 5)\n",
      "    X_meta 형태: (3,)\n",
      "    y 형태: (24,)\n",
      "  발전소 영월본부: 911 샘플\n",
      "    X_historical 형태: (48, 46)\n",
      "    X_forecast 형태: (24, 5)\n",
      "    X_meta 형태: (3,)\n",
      "    y 형태: (24,)\n",
      "  발전소 하동변전소: 515 샘플\n",
      "    X_historical 형태: (48, 46)\n",
      "    X_forecast 형태: (24, 5)\n",
      "    X_meta 형태: (3,)\n",
      "    y 형태: (24,)\n",
      "External test plants: ['신인천 1_2단계 주차장', '신인천 북측부지', '부산신항', '부산역선상주차장', '위미2리', '무릉리', '신인천전망대']\n",
      "Regular split plants: ['신인천 주차장', '신인천소내', '인천수산정수장', '신인천해수구취수구', '이천시 백사면A', '감우리', '하동본부_6', '하동정수장', '화촌주민참여형', '하동공설운동장', '하동보건소', '하동본부_1', '하동본부_2', '하동하수처리장', '하동본부_3', '하동변전소', '하동본부_4', '하동본부_5', '이천D(백사면B)', '삼척소내_4', '삼척소내_2', '삼척소내_3', '삼척소내_1', '영월본부', '영월철도부지', '부산본부_2', '부산운동장', '부산수처리장', '부산복합자재창고', '부산본부_1']\n",
      "\n",
      "14시 시나리오:\n",
      "  TRAIN 세트:\n",
      "    샘플 수: 15895\n",
      "    X_historical 형태: (15895, 48, 46)\n",
      "    X_forecast 형태: (15895, 24, 5)\n",
      "    X_meta 형태: (15895, 3)\n",
      "    y 형태: (15895, 24)\n",
      "    발전소 수: 30\n",
      "    발전소 분포: {'신인천 주차장': 233, '신인천소내': 233, '인천수산정수장': 233, '신인천해수구취수구': 233, '이천시 백사면A': 345, '감우리': 357, '하동본부_6': 360, '하동정수장': 360, '화촌주민참여형': 360, '하동공설운동장': 360, '하동보건소': 360, '하동본부_1': 360, '하동본부_2': 360, '하동하수처리장': 360, '하동본부_3': 360, '하동변전소': 360, '하동본부_4': 360, '하동본부_5': 360, '이천D(백사면B)': 480, '삼척소내_4': 615, '삼척소내_2': 615, '삼척소내_3': 615, '삼척소내_1': 615, '영월본부': 637, '영월철도부지': 637, '부산본부_2': 637, '부산운동장': 893, '부산수처리장': 893, '부산복합자재창고': 1652, '부산본부_1': 1652}\n",
      "  VALID 세트:\n",
      "    샘플 수: 3406\n",
      "    X_historical 형태: (3406, 48, 46)\n",
      "    X_forecast 형태: (3406, 24, 5)\n",
      "    X_meta 형태: (3406, 3)\n",
      "    y 형태: (3406, 24)\n",
      "    발전소 수: 30\n",
      "    발전소 분포: {'신인천 주차장': 50, '신인천소내': 50, '인천수산정수장': 50, '신인천해수구취수구': 50, '이천시 백사면A': 74, '감우리': 77, '하동본부_6': 77, '하동정수장': 77, '화촌주민참여형': 77, '하동공설운동장': 77, '하동보건소': 77, '하동본부_1': 77, '하동본부_2': 77, '하동하수처리장': 77, '하동본부_3': 77, '하동변전소': 77, '하동본부_4': 77, '하동본부_5': 77, '이천D(백사면B)': 103, '삼척소내_4': 132, '삼척소내_2': 132, '삼척소내_3': 132, '삼척소내_1': 132, '영월본부': 137, '영월철도부지': 137, '부산본부_2': 136, '부산운동장': 191, '부산수처리장': 191, '부산복합자재창고': 354, '부산본부_1': 354}\n",
      "  TEST 세트:\n",
      "    샘플 수: 3423\n",
      "    X_historical 형태: (3423, 48, 46)\n",
      "    X_forecast 형태: (3423, 24, 5)\n",
      "    X_meta 형태: (3423, 3)\n",
      "    y 형태: (3423, 24)\n",
      "    발전소 수: 30\n",
      "    발전소 분포: {'신인천 주차장': 50, '신인천소내': 50, '인천수산정수장': 50, '신인천해수구취수구': 50, '이천시 백사면A': 74, '감우리': 77, '하동본부_6': 78, '하동정수장': 78, '화촌주민참여형': 78, '하동공설운동장': 78, '하동보건소': 78, '하동본부_1': 78, '하동본부_2': 78, '하동하수처리장': 78, '하동본부_3': 78, '하동변전소': 78, '하동본부_4': 78, '하동본부_5': 78, '이천D(백사면B)': 104, '삼척소내_4': 132, '삼척소내_2': 132, '삼척소내_3': 132, '삼척소내_1': 132, '영월본부': 137, '영월철도부지': 137, '부산본부_2': 137, '부산운동장': 192, '부산수처리장': 192, '부산복합자재창고': 355, '부산본부_1': 354}\n",
      "  EXTERNAL_TEST 세트:\n",
      "    샘플 수: 2017\n",
      "    X_historical 형태: (2017, 48, 46)\n",
      "    X_forecast 형태: (2017, 24, 5)\n",
      "    X_meta 형태: (2017, 3)\n",
      "    y 형태: (2017, 24)\n",
      "    발전소 수: 7\n",
      "    발전소 분포: {'신인천 1_2단계 주차장': 207, '신인천 북측부지': 207, '부산신항': 289, '부산역선상주차장': 319, '위미2리': 331, '무릉리': 331, '신인천전망대': 333}\n",
      "\n",
      "20시 시나리오:\n",
      "  TRAIN 세트:\n",
      "    샘플 수: 15895\n",
      "    X_historical 형태: (15895, 48, 46)\n",
      "    X_forecast 형태: (15895, 24, 5)\n",
      "    X_meta 형태: (15895, 3)\n",
      "    y 형태: (15895, 24)\n",
      "    발전소 수: 30\n",
      "    발전소 분포: {'신인천 주차장': 233, '신인천소내': 233, '인천수산정수장': 233, '신인천해수구취수구': 233, '이천시 백사면A': 345, '감우리': 357, '하동본부_6': 360, '하동정수장': 360, '화촌주민참여형': 360, '하동공설운동장': 360, '하동보건소': 360, '하동본부_1': 360, '하동본부_2': 360, '하동하수처리장': 360, '하동본부_3': 360, '하동변전소': 360, '하동본부_4': 360, '하동본부_5': 360, '이천D(백사면B)': 480, '삼척소내_4': 615, '삼척소내_2': 615, '삼척소내_3': 615, '삼척소내_1': 615, '영월본부': 637, '영월철도부지': 637, '부산본부_2': 637, '부산운동장': 893, '부산수처리장': 893, '부산복합자재창고': 1652, '부산본부_1': 1652}\n",
      "  VALID 세트:\n",
      "    샘플 수: 3406\n",
      "    X_historical 형태: (3406, 48, 46)\n",
      "    X_forecast 형태: (3406, 24, 5)\n",
      "    X_meta 형태: (3406, 3)\n",
      "    y 형태: (3406, 24)\n",
      "    발전소 수: 30\n",
      "    발전소 분포: {'신인천 주차장': 50, '신인천소내': 50, '인천수산정수장': 50, '신인천해수구취수구': 50, '이천시 백사면A': 74, '감우리': 77, '하동본부_6': 77, '하동정수장': 77, '화촌주민참여형': 77, '하동공설운동장': 77, '하동보건소': 77, '하동본부_1': 77, '하동본부_2': 77, '하동하수처리장': 77, '하동본부_3': 77, '하동변전소': 77, '하동본부_4': 77, '하동본부_5': 77, '이천D(백사면B)': 103, '삼척소내_4': 132, '삼척소내_2': 132, '삼척소내_3': 132, '삼척소내_1': 132, '영월본부': 137, '영월철도부지': 137, '부산본부_2': 136, '부산운동장': 191, '부산수처리장': 191, '부산복합자재창고': 354, '부산본부_1': 354}\n",
      "  TEST 세트:\n",
      "    샘플 수: 3423\n",
      "    X_historical 형태: (3423, 48, 46)\n",
      "    X_forecast 형태: (3423, 24, 5)\n",
      "    X_meta 형태: (3423, 3)\n",
      "    y 형태: (3423, 24)\n",
      "    발전소 수: 30\n",
      "    발전소 분포: {'신인천 주차장': 50, '신인천소내': 50, '인천수산정수장': 50, '신인천해수구취수구': 50, '이천시 백사면A': 74, '감우리': 77, '하동본부_6': 78, '하동정수장': 78, '화촌주민참여형': 78, '하동공설운동장': 78, '하동보건소': 78, '하동본부_1': 78, '하동본부_2': 78, '하동하수처리장': 78, '하동본부_3': 78, '하동변전소': 78, '하동본부_4': 78, '하동본부_5': 78, '이천D(백사면B)': 104, '삼척소내_4': 132, '삼척소내_2': 132, '삼척소내_3': 132, '삼척소내_1': 132, '영월본부': 137, '영월철도부지': 137, '부산본부_2': 137, '부산운동장': 192, '부산수처리장': 192, '부산복합자재창고': 355, '부산본부_1': 354}\n",
      "  EXTERNAL_TEST 세트:\n",
      "    샘플 수: 2017\n",
      "    X_historical 형태: (2017, 48, 46)\n",
      "    X_forecast 형태: (2017, 24, 5)\n",
      "    X_meta 형태: (2017, 3)\n",
      "    y 형태: (2017, 24)\n",
      "    발전소 수: 7\n",
      "    발전소 분포: {'신인천 1_2단계 주차장': 207, '신인천 북측부지': 207, '부산신항': 289, '부산역선상주차장': 319, '위미2리': 331, '무릉리': 331, '신인천전망대': 333}\n",
      "Saved train data for 14시 scenario:\n",
      "  X_historical shape: (15895, 48, 46)\n",
      "  X_forecast shape: (15895, 24, 5)\n",
      "  X_meta shape: (15895, 3)\n",
      "  y shape: (15895, 24)\n",
      "Saved valid data for 14시 scenario:\n",
      "  X_historical shape: (3406, 48, 46)\n",
      "  X_forecast shape: (3406, 24, 5)\n",
      "  X_meta shape: (3406, 3)\n",
      "  y shape: (3406, 24)\n",
      "Saved test data for 14시 scenario:\n",
      "  X_historical shape: (3423, 48, 46)\n",
      "  X_forecast shape: (3423, 24, 5)\n",
      "  X_meta shape: (3423, 3)\n",
      "  y shape: (3423, 24)\n",
      "Saved external_test data for 14시 scenario:\n",
      "  X_historical shape: (2017, 48, 46)\n",
      "  X_forecast shape: (2017, 24, 5)\n",
      "  X_meta shape: (2017, 3)\n",
      "  y shape: (2017, 24)\n",
      "Saved train data for 20시 scenario:\n",
      "  X_historical shape: (15895, 48, 46)\n",
      "  X_forecast shape: (15895, 24, 5)\n",
      "  X_meta shape: (15895, 3)\n",
      "  y shape: (15895, 24)\n",
      "Saved valid data for 20시 scenario:\n",
      "  X_historical shape: (3406, 48, 46)\n",
      "  X_forecast shape: (3406, 24, 5)\n",
      "  X_meta shape: (3406, 3)\n",
      "  y shape: (3406, 24)\n",
      "Saved test data for 20시 scenario:\n",
      "  X_historical shape: (3423, 48, 46)\n",
      "  X_forecast shape: (3423, 24, 5)\n",
      "  X_meta shape: (3423, 3)\n",
      "  y shape: (3423, 24)\n",
      "Saved external_test data for 20시 scenario:\n",
      "  X_historical shape: (2017, 48, 46)\n",
      "  X_forecast shape: (2017, 24, 5)\n",
      "  X_meta shape: (2017, 3)\n",
      "  y shape: (2017, 24)\n",
      "\n",
      "예보 데이터와 메타데이터가 포함된 모델링 데이터 저장 완료: ../data/forecast_modeling_data\n"
     ]
    }
   ],
   "source": [
    "# 메타데이터 요약 출력\n",
    "print(f\"Loaded metadata for {len(meta_data)} plants\")\n",
    "print(f\"Metadata columns: {meta_data.columns.tolist()}\")\n",
    "\n",
    "# 예보 데이터와 메타데이터를 포함한 시계열 데이터 준비\n",
    "forecast_data, column_info = prepare_forecast_data(\n",
    "    processed_data, \n",
    "    feature_cols, \n",
    "    forecast_cols_14, \n",
    "    forecast_cols_20,\n",
    "    target_col=target_col,\n",
    "    history_days=history_days,\n",
    "    include_forecast=True,\n",
    "    meta_data=meta_data\n",
    ")\n",
    "\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# 각 시나리오별 컬럼 정보를 JSON 파일로 저장\n",
    "for scenario, cols in column_info.items():\n",
    "    # 리스트를 JSON으로 직렬화 가능하게 변환\n",
    "    serializable_cols = {\n",
    "        'historical_cols': list(cols['historical_cols']),\n",
    "        'base_feature_cols': list(cols['base_feature_cols']),\n",
    "        'forecast_cols': list(cols['forecast_cols']),\n",
    "        'meta_cols': list(cols['meta_cols'])  # 메타데이터 컬럼 추가\n",
    "    }\n",
    "    \n",
    "    # JSON 파일로 저장\n",
    "    col_file = os.path.join(output_dir, f\"{scenario}_columns.json\")\n",
    "    with open(col_file, 'w') as f:\n",
    "        json.dump(serializable_cols, f, indent=2)\n",
    "    \n",
    "    print(f\"Column information for {scenario} saved to {col_file}\")\n",
    "\n",
    "# 각 시나리오별 발전소 수 및 샘플 수 출력\n",
    "for scenario, scenario_data in forecast_data.items():\n",
    "    plant_count = len(scenario_data)\n",
    "    total_samples = sum(len(samples) for plant_id, samples in scenario_data.items())\n",
    "    \n",
    "    print(f\"\\n{scenario} 시나리오:\")\n",
    "    print(f\"  발전소 수: {plant_count}\")\n",
    "    print(f\"  총 샘플 수: {total_samples}\")\n",
    "    \n",
    "    # 각 발전소별 샘플 수 출력 (처음 5개만)\n",
    "    for i, (plant_id, samples) in enumerate(scenario_data.items()):\n",
    "        if i >= 5:  # 처음 5개 발전소만 출력\n",
    "            break\n",
    "            \n",
    "        print(f\"  발전소 {plant_id}: {len(samples)} 샘플\")\n",
    "        if len(samples) > 0:\n",
    "            print(f\"    X_historical 형태: {samples[0]['X_historical'].shape}\")\n",
    "            print(f\"    X_forecast 형태: {samples[0]['X_forecast'].shape}\")\n",
    "            print(f\"    X_meta 형태: {samples[0]['X_meta'].shape}\")\n",
    "            print(f\"    y 형태: {samples[0]['y'].shape}\")\n",
    "\n",
    "# 데이터 분할 (학습/검증/테스트 + 외부 테스트)\n",
    "split_forecast_data = split_forecast_data(\n",
    "    forecast_data, \n",
    "    plants_info,\n",
    "    external_test_ratio=external_test_ratio\n",
    ")\n",
    "\n",
    "# 각 시나리오별, 세트별 결과 요약 출력\n",
    "for scenario, split_data in split_forecast_data.items():\n",
    "    print(f\"\\n{scenario.upper()} 시나리오:\")\n",
    "    \n",
    "    for split_name, data in split_data.items():\n",
    "        if data['X_historical'].size == 0:\n",
    "            continue\n",
    "            \n",
    "        print(f\"  {split_name.upper()} 세트:\")\n",
    "        print(f\"    샘플 수: {len(data['X_historical'])}\")\n",
    "        print(f\"    X_historical 형태: {data['X_historical'].shape}\")\n",
    "        print(f\"    X_forecast 형태: {data['X_forecast'].shape}\")\n",
    "        print(f\"    X_meta 형태: {data['X_meta'].shape}\")\n",
    "        print(f\"    y 형태: {data['y'].shape}\")\n",
    "        \n",
    "        # 발전소 정보\n",
    "        plant_counts = {}\n",
    "        for plant_id in data['plants']:\n",
    "            if plant_id in plant_counts:\n",
    "                plant_counts[plant_id] += 1\n",
    "            else:\n",
    "                plant_counts[plant_id] = 1\n",
    "        \n",
    "        print(f\"    발전소 수: {len(plant_counts)}\")\n",
    "        print(f\"    발전소 분포: {plant_counts}\")\n",
    "\n",
    "# 데이터 저장\n",
    "save_forecast_data(split_forecast_data, output_dir)\n",
    "print(f\"\\n예보 데이터와 메타데이터가 포함된 모델링 데이터 저장 완료: {output_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ce79b4ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Historical columns (46): ['총량(kw)', '평균(kw)', '최대(kw)', '최소(kw)', '최대(시간별_kw)', '최소(시간별_kw)', 'value', 'temperature', 'humidity', 'rn', 'ws', 'wd', 'pv', 'pa', 'ps', 'ss', 'icsr', 'dc10Tca', 'dc10LmcsCa', 'lcsCh', 'vs', 'ts', 'SO2', 'CO', 'O3', 'NO2', 'PM10', 'PM25', '미세먼지', '초미세먼지', 'time_hour_sin', 'time_hour_cos', 'time_day_sin', 'time_day_cos', 'time_month_sin', 'time_month_cos', 'time_is_daylight', 'time_hours_since_sunrise', 'time_hours_until_sunset', 'wd_sin', 'wd_cos', 'temp_14', 'wd_14', 'sc_14', 'ws_14', 'pp_14']\n",
      "Forecast columns (5): ['temp_14', 'wd_14', 'sc_14', 'ws_14', 'pp_14']\n",
      "Meta columns (3): ['capacity_kw', 'plant_age', 'region']\n"
     ]
    }
   ],
   "source": [
    "def load_column_info(scenario, column_file):\n",
    "    with open(column_file, 'r') as f:\n",
    "        return json.load(f)\n",
    "\n",
    "# 14시 시나리오의 컬럼 정보 로드\n",
    "scenario = '14시'\n",
    "column_info = load_column_info(scenario, f\"../data/forecast_modeling_data/{scenario}_columns.json\")\n",
    "\n",
    "# 컬럼 정보 활용\n",
    "historical_cols = column_info['historical_cols']\n",
    "base_feature_cols = column_info['base_feature_cols']\n",
    "forecast_cols = column_info['forecast_cols']\n",
    "meta_cols = column_info.get('meta_cols', [])  # meta_cols가 없을 경우 빈 리스트 반환\n",
    "\n",
    "print(f\"Historical columns ({len(historical_cols)}): {historical_cols}\")\n",
    "print(f\"Forecast columns ({len(forecast_cols)}): {forecast_cols}\")\n",
    "print(f\"Meta columns ({len(meta_cols)}): {meta_cols}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a52aa017",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('temp_14', 'wd_14', 'sc_14', 'ws_14', 'pp_14')"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'총량(kw)', '평균(kw)', '최대(kw)', '최소(kw)', '최대(시간별_kw)', '최소(시간별_kw)', 'value',\n",
    "'temperature', 'humidity', 'rn', 'ws', 'wd', 'pv', 'pa', 'ps', 'ss', 'icsr', \n",
    "'dc10Tca', 'dc10LmcsCa', 'lcsCh', 'vs', 'ts', \n",
    "'SO2', 'CO', 'O3', 'NO2', 'PM10', 'PM25', '미세먼지', '초미세먼지', \n",
    "'time_hour_sin', 'time_hour_cos', 'time_day_sin', 'time_day_cos', 'time_month_sin', 'time_month_cos', \n",
    "'time_is_daylight', 'time_hours_since_sunrise', 'time_hours_until_sunset', 'wd_sin', 'wd_cos', \n",
    "'temp_14', 'wd_14', 'sc_14', 'ws_14', 'pp_14'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5953d117",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3423, 24, 5)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[[20.0, 94.0, '1.0', 1.0, 0.0],\n",
       "        [21.0, 139.0, '1.0', 2.0, 0.0],\n",
       "        [22.0, 185.0, '1.0', 2.0, 0.0],\n",
       "        ...,\n",
       "        [16.0, 117.0, '1.0', 1.0, 0.0],\n",
       "        [16.0, 117.0, '1.0', 1.0, 0.0],\n",
       "        [16.0, 117.0, '1.0', 1.0, 0.0]],\n",
       "\n",
       "       [[20.0, 124.0, '1.0', 1.0, 0.0],\n",
       "        [21.0, 146.0, '1.0', 2.0, 0.0],\n",
       "        [23.0, 169.0, '1.0', 2.0, 0.0],\n",
       "        ...,\n",
       "        [19.0, 114.0, '4.0', 3.0, 30.0],\n",
       "        [19.0, 114.0, '4.0', 3.0, 30.0],\n",
       "        [19.0, 114.0, '4.0', 3.0, 30.0]],\n",
       "\n",
       "       [[19.0, 145.0, '4.0', 3.0, 60.0],\n",
       "        [19.0, 128.0, '4.0', 3.0, 60.0],\n",
       "        [20.0, 112.0, '4.0', 2.0, 60.0],\n",
       "        ...,\n",
       "        [18.0, 45.0, '4.0', 2.0, 70.0],\n",
       "        [18.0, 45.0, '4.0', 2.0, 70.0],\n",
       "        [18.0, 45.0, '4.0', 2.0, 70.0]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[24.0, 52.0, '1.0', 3.0, 10.0],\n",
       "        [25.0, 63.0, '1.0', 4.0, 10.0],\n",
       "        [25.0, 73.0, '1.0', 4.0, 10.0],\n",
       "        ...,\n",
       "        [21.0, 41.0, '3.0', 3.0, 20.0],\n",
       "        [21.0, 41.0, '3.0', 3.0, 20.0],\n",
       "        [21.0, 41.0, '3.0', 3.0, 20.0]],\n",
       "\n",
       "       [[24.0, 78.0, '3.0', 4.0, 20.0],\n",
       "        [25.0, 78.0, '3.0', 4.0, 23.0],\n",
       "        [25.0, 78.0, '3.0', 4.0, 27.0],\n",
       "        ...,\n",
       "        [21.0, 27.0, '4.0', 3.0, 30.0],\n",
       "        [21.0, 27.0, '4.0', 3.0, 30.0],\n",
       "        [21.0, 27.0, '4.0', 3.0, 30.0]],\n",
       "\n",
       "       [[25.0, 53.0, '4.0', 4.0, 30.0],\n",
       "        [26.0, 59.0, '4.0', 4.0, 30.0],\n",
       "        [27.0, 66.0, '4.0', 4.0, 30.0],\n",
       "        ...,\n",
       "        [22.0, 34.0, '3.0', 4.0, 20.0],\n",
       "        [22.0, 34.0, '3.0', 4.0, 20.0],\n",
       "        [22.0, 34.0, '3.0', 4.0, 20.0]]], dtype=object)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.load('../data/forecast_modeling_data/20시/test/X_forecast.npy',allow_pickle=True)\n",
    "print(a.shape)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4533a94c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3423, 48, 46)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[[3477.426, 144.893, 463.785, ..., '1.0', 3.0, 0.0],\n",
       "        [3477.426, 144.893, 463.785, ..., '1.0', 3.0, 0.0],\n",
       "        [3477.426, 144.893, 463.785, ..., '1.0', 3.0, 0.0],\n",
       "        ...,\n",
       "        [0.0, 0.0, 0.0, ..., 0.0, 0.0, 0.0],\n",
       "        [0.0, 0.0, 0.0, ..., 0.0, 0.0, 0.0],\n",
       "        [0.0, 0.0, 0.0, ..., 0.0, 0.0, 0.0]],\n",
       "\n",
       "       [[3312.063, 138.003, 458.964, ..., '4.0', 3.0, 30.0],\n",
       "        [3312.063, 138.003, 458.964, ..., '4.0', 3.0, 30.0],\n",
       "        [3312.063, 138.003, 458.964, ..., '4.0', 2.0, 30.0],\n",
       "        ...,\n",
       "        [0.0, 0.0, 0.0, ..., 0.0, 0.0, 0.0],\n",
       "        [0.0, 0.0, 0.0, ..., 0.0, 0.0, 0.0],\n",
       "        [0.0, 0.0, 0.0, ..., 0.0, 0.0, 0.0]],\n",
       "\n",
       "       [[3377.147, 140.714, 431.484, ..., '1.0', 1.0, 0.0],\n",
       "        [3377.147, 140.714, 431.484, ..., '1.0', 2.0, 0.0],\n",
       "        [3377.147, 140.714, 431.484, ..., '1.0', 2.0, 0.0],\n",
       "        ...,\n",
       "        [0.0, 0.0, 0.0, ..., 0.0, 0.0, 0.0],\n",
       "        [0.0, 0.0, 0.0, ..., 0.0, 0.0, 0.0],\n",
       "        [0.0, 0.0, 0.0, ..., 0.0, 0.0, 0.0]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[723.564, 30.149, 120.312, ..., '3.0', 2.0, 10.0],\n",
       "        [723.564, 30.149, 120.312, ..., '3.0', 3.0, 7.0],\n",
       "        [723.564, 30.149, 120.312, ..., '3.0', 3.0, 3.0],\n",
       "        ...,\n",
       "        [0.0, 0.0, 0.0, ..., 0.0, 0.0, 0.0],\n",
       "        [0.0, 0.0, 0.0, ..., 0.0, 0.0, 0.0],\n",
       "        [0.0, 0.0, 0.0, ..., 0.0, 0.0, 0.0]],\n",
       "\n",
       "       [[691.416, 28.809, 120.384, ..., '3.0', 4.0, 20.0],\n",
       "        [691.416, 28.809, 120.384, ..., '3.0', 5.0, 20.0],\n",
       "        [691.416, 28.809, 120.384, ..., '3.0', 5.0, 20.0],\n",
       "        ...,\n",
       "        [0.0, 0.0, 0.0, ..., 0.0, 0.0, 0.0],\n",
       "        [0.0, 0.0, 0.0, ..., 0.0, 0.0, 0.0],\n",
       "        [0.0, 0.0, 0.0, ..., 0.0, 0.0, 0.0]],\n",
       "\n",
       "       [[575.964, 23.999, 88.416, ..., '1.0', 3.0, 10.0],\n",
       "        [575.964, 23.999, 88.416, ..., '1.0', 4.0, 10.0],\n",
       "        [575.964, 23.999, 88.416, ..., '1.0', 4.0, 10.0],\n",
       "        ...,\n",
       "        [0.0, 0.0, 0.0, ..., 0.0, 0.0, 0.0],\n",
       "        [0.0, 0.0, 0.0, ..., 0.0, 0.0, 0.0],\n",
       "        [0.0, 0.0, 0.0, ..., 0.0, 0.0, 0.0]]], dtype=object)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.load('../data/forecast_modeling_data/20시/test/X_historical.npy',allow_pickle=True)\n",
    "print(a.shape)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e3db0120",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3423, 3)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([['603.0', '6.0', '인천광역시'],\n",
       "       ['603.0', '6.0', '인천광역시'],\n",
       "       ['603.0', '6.0', '인천광역시'],\n",
       "       ...,\n",
       "       ['390.0', '17.0', '부산'],\n",
       "       ['390.0', '17.0', '부산'],\n",
       "       ['390.0', '17.0', '부산']], dtype='<U32')"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.load('../data/forecast_modeling_data/20시/test/X_meta.npy',allow_pickle=True)\n",
    "print(a.shape)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "f29d3662",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "데이터 세트별 날짜 범위 현황\n",
      "================================================================================\n",
      "\n",
      "🕐 14시 시나리오\n",
      "------------------------------------------------------------\n",
      "📁 TRAIN       \n",
      "   📅 날짜 범위: 2015-01-04 ~ 2022-08-04\n",
      "   📊 총 일수: 2759일\n",
      "   📈 총 샘플: 381,480개\n",
      "   🔢 일평균 샘플: 138.3개/일\n",
      "\n",
      "📁 VALID       \n",
      "   📅 날짜 범위: 2019-07-24 ~ 2022-10-17\n",
      "   📊 총 일수: 812일\n",
      "   📈 총 샘플: 81,744개\n",
      "   🔢 일평균 샘플: 100.7개/일\n",
      "\n",
      "📁 TEST        \n",
      "   📅 날짜 범위: 2020-07-12 ~ 2022-12-30\n",
      "   📊 총 일수: 487일\n",
      "   📈 총 샘플: 82,152개\n",
      "   🔢 일평균 샘플: 168.7개/일\n",
      "\n",
      "📁 EXTERNAL_TEST\n",
      "   📅 날짜 범위: 2020-08-03 ~ 2022-12-30\n",
      "   📊 총 일수: 664일\n",
      "   📈 총 샘플: 48,408개\n",
      "   🔢 일평균 샘플: 72.9개/일\n",
      "\n",
      "📋 시나리오 요약:\n",
      "   전체 기간: 2015-01-04 ~ 2022-12-30\n",
      "   총 데이터 일수: 4722일\n",
      "   총 샘플 수: 593,784개\n",
      "\n",
      "⏰ 시간적 분할 순서:\n",
      "   1. train: 2015-01-04 ~ 2022-08-04\n",
      "   2. valid: 2019-07-24 ~ 2022-10-17\n",
      "   3. test: 2020-07-12 ~ 2022-12-30\n",
      "   4. external_test: 2020-08-03 ~ 2022-12-30\n",
      "\n",
      "🕐 20시 시나리오\n",
      "------------------------------------------------------------\n",
      "📁 TRAIN       \n",
      "   📅 날짜 범위: 2015-01-04 ~ 2022-08-04\n",
      "   📊 총 일수: 2759일\n",
      "   📈 총 샘플: 381,480개\n",
      "   🔢 일평균 샘플: 138.3개/일\n",
      "\n",
      "📁 VALID       \n",
      "   📅 날짜 범위: 2019-07-24 ~ 2022-10-17\n",
      "   📊 총 일수: 812일\n",
      "   📈 총 샘플: 81,744개\n",
      "   🔢 일평균 샘플: 100.7개/일\n",
      "\n",
      "📁 TEST        \n",
      "   📅 날짜 범위: 2020-07-12 ~ 2022-12-30\n",
      "   📊 총 일수: 487일\n",
      "   📈 총 샘플: 82,152개\n",
      "   🔢 일평균 샘플: 168.7개/일\n",
      "\n",
      "📁 EXTERNAL_TEST\n",
      "   📅 날짜 범위: 2020-08-03 ~ 2022-12-30\n",
      "   📊 총 일수: 664일\n",
      "   📈 총 샘플: 48,408개\n",
      "   🔢 일평균 샘플: 72.9개/일\n",
      "\n",
      "📋 시나리오 요약:\n",
      "   전체 기간: 2015-01-04 ~ 2022-12-30\n",
      "   총 데이터 일수: 4722일\n",
      "   총 샘플 수: 593,784개\n",
      "\n",
      "⏰ 시간적 분할 순서:\n",
      "   1. train: 2015-01-04 ~ 2022-08-04\n",
      "   2. valid: 2019-07-24 ~ 2022-10-17\n",
      "   3. test: 2020-07-12 ~ 2022-12-30\n",
      "   4. external_test: 2020-08-03 ~ 2022-12-30\n",
      "\n",
      "🔍 14시 시나리오 시간적 일관성 상세 분석\n",
      "============================================================\n",
      "📅 Split 간 날짜 겹침 확인:\n",
      "   ⚠️  train ↔ valid: 738일 겹침\n",
      "   ⚠️  train ↔ test: 354일 겹침\n",
      "   ⚠️  train ↔ external_test: 515일 겹침\n",
      "   ⚠️  valid ↔ test: 326일 겹침\n",
      "   ⚠️  valid ↔ external_test: 436일 겹침\n",
      "   ⚠️  test ↔ external_test: 465일 겹침\n",
      "\n",
      "📈 Split 간 시간적 연속성:\n",
      "   1. train: 2015-01-04 ~ 2022-08-04\n",
      "   2. valid: 2019-07-24 ~ 2022-10-17\n",
      "      ⚠️  이전 split과 1108일 겹침\n",
      "   3. test: 2020-07-12 ~ 2022-12-30\n",
      "      ⚠️  이전 split과 828일 겹침\n",
      "   4. external_test: 2020-08-03 ~ 2022-12-30\n",
      "      ⚠️  이전 split과 880일 겹침\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "def check_date_ranges(base_path='../data/forecast_modeling_data'):\n",
    "    \"\"\"\n",
    "    각 시나리오별, 데이터 세트별 날짜 범위 확인\n",
    "    \n",
    "    Args:\n",
    "        base_path: 데이터 폴더 경로\n",
    "    \"\"\"\n",
    "    \n",
    "    scenarios = ['14시', '20시']\n",
    "    splits = ['train', 'valid', 'test', 'external_test']\n",
    "    \n",
    "    print(\"=\" * 80)\n",
    "    print(\"데이터 세트별 날짜 범위 현황\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    for scenario in scenarios:\n",
    "        print(f\"\\n🕐 {scenario} 시나리오\")\n",
    "        print(\"-\" * 60)\n",
    "        \n",
    "        scenario_summary = []\n",
    "        \n",
    "        for split in splits:\n",
    "            try:\n",
    "                # dates.npy 파일 로드\n",
    "                dates_path = f\"{base_path}/{scenario}/{split}/dates.npy\"\n",
    "                dates = np.load(dates_path, allow_pickle=True)\n",
    "                \n",
    "                # numpy datetime64를 pandas datetime으로 변환\n",
    "                if len(dates) > 0:\n",
    "                    # 3D 배열인 경우 flatten\n",
    "                    if dates.ndim > 1:\n",
    "                        dates_flat = dates.flatten()\n",
    "                    else:\n",
    "                        dates_flat = dates\n",
    "                    \n",
    "                    # datetime 변환\n",
    "                    dates_pd = pd.to_datetime(dates_flat)\n",
    "                    \n",
    "                    # 날짜만 추출 (시간 제거)\n",
    "                    dates_only = dates_pd.date\n",
    "                    unique_dates = sorted(set(dates_only))\n",
    "                    \n",
    "                    start_date = unique_dates[0]\n",
    "                    end_date = unique_dates[-1]\n",
    "                    total_days = len(unique_dates)\n",
    "                    total_samples = len(dates_flat)\n",
    "                    \n",
    "                    print(f\"📁 {split.upper():<12}\")\n",
    "                    print(f\"   📅 날짜 범위: {start_date} ~ {end_date}\")\n",
    "                    print(f\"   📊 총 일수: {total_days}일\")\n",
    "                    print(f\"   📈 총 샘플: {total_samples:,}개\")\n",
    "                    print(f\"   🔢 일평균 샘플: {total_samples/total_days:.1f}개/일\")\n",
    "                    print()\n",
    "                    \n",
    "                    scenario_summary.append({\n",
    "                        'split': split,\n",
    "                        'start_date': start_date,\n",
    "                        'end_date': end_date,\n",
    "                        'days': total_days,\n",
    "                        'samples': total_samples\n",
    "                    })\n",
    "                    \n",
    "            except FileNotFoundError:\n",
    "                print(f\"❌ {split.upper()}: 파일을 찾을 수 없습니다.\")\n",
    "            except Exception as e:\n",
    "                print(f\"❌ {split.upper()}: 오류 발생 - {e}\")\n",
    "        \n",
    "        # 시나리오 요약\n",
    "        if scenario_summary:\n",
    "            print(\"📋 시나리오 요약:\")\n",
    "            total_days_scenario = sum(s['days'] for s in scenario_summary)\n",
    "            total_samples_scenario = sum(s['samples'] for s in scenario_summary)\n",
    "            \n",
    "            print(f\"   전체 기간: {min(s['start_date'] for s in scenario_summary)} ~ \"\n",
    "                  f\"{max(s['end_date'] for s in scenario_summary)}\")\n",
    "            print(f\"   총 데이터 일수: {total_days_scenario}일\")\n",
    "            print(f\"   총 샘플 수: {total_samples_scenario:,}개\")\n",
    "            \n",
    "            # 시간적 연속성 확인\n",
    "            print(\"\\n⏰ 시간적 분할 순서:\")\n",
    "            sorted_summary = sorted(scenario_summary, key=lambda x: x['start_date'])\n",
    "            for i, s in enumerate(sorted_summary):\n",
    "                print(f\"   {i+1}. {s['split']}: {s['start_date']} ~ {s['end_date']}\")\n",
    "\n",
    "def check_temporal_consistency(base_path='../data/forecast_modeling_data', scenario='14시'):\n",
    "    \"\"\"\n",
    "    시간적 일관성 상세 확인 (겹치는 날짜가 있는지 등)\n",
    "    \n",
    "    Args:\n",
    "        base_path: 데이터 폴더 경로\n",
    "        scenario: 확인할 시나리오\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"\\n🔍 {scenario} 시나리오 시간적 일관성 상세 분석\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    splits = ['train', 'valid', 'test', 'external_test']\n",
    "    split_dates = {}\n",
    "    \n",
    "    # 각 split별 날짜 수집\n",
    "    for split in splits:\n",
    "        try:\n",
    "            dates_path = f\"{base_path}/{scenario}/{split}/dates.npy\"\n",
    "            dates = np.load(dates_path, allow_pickle=True)\n",
    "            \n",
    "            if dates.ndim > 1:\n",
    "                dates_flat = dates.flatten()\n",
    "            else:\n",
    "                dates_flat = dates\n",
    "            \n",
    "            dates_pd = pd.to_datetime(dates_flat)\n",
    "            unique_dates = set(dates_pd.date)\n",
    "            split_dates[split] = unique_dates\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ {split}: {e}\")\n",
    "            split_dates[split] = set()\n",
    "    \n",
    "    # 겹치는 날짜 확인\n",
    "    print(\"📅 Split 간 날짜 겹침 확인:\")\n",
    "    splits_with_data = [s for s in splits if split_dates[s]]\n",
    "    \n",
    "    overlap_found = False\n",
    "    for i, split1 in enumerate(splits_with_data):\n",
    "        for split2 in splits_with_data[i+1:]:\n",
    "            overlap = split_dates[split1] & split_dates[split2]\n",
    "            if overlap:\n",
    "                overlap_found = True\n",
    "                print(f\"   ⚠️  {split1} ↔ {split2}: {len(overlap)}일 겹침\")\n",
    "                if len(overlap) <= 5:  # 5일 이하면 구체적으로 출력\n",
    "                    print(f\"      겹치는 날짜: {sorted(overlap)}\")\n",
    "    \n",
    "    if not overlap_found:\n",
    "        print(\"   ✅ 모든 split 간 날짜 겹침 없음 (정상)\")\n",
    "    \n",
    "    # 날짜 간격 확인\n",
    "    print(f\"\\n📈 Split 간 시간적 연속성:\")\n",
    "    sorted_splits = sorted([(split, min(dates), max(dates)) \n",
    "                           for split, dates in split_dates.items() if dates],\n",
    "                          key=lambda x: x[1])\n",
    "    \n",
    "    for i, (split, start, end) in enumerate(sorted_splits):\n",
    "        print(f\"   {i+1}. {split}: {start} ~ {end}\")\n",
    "        if i > 0:\n",
    "            prev_end = sorted_splits[i-1][2]\n",
    "            gap = (start - prev_end).days - 1\n",
    "            if gap > 0:\n",
    "                print(f\"      ⚠️  이전 split과 {gap}일 간격\")\n",
    "            elif gap < 0:\n",
    "                print(f\"      ⚠️  이전 split과 {abs(gap)}일 겹침\")\n",
    "            else:\n",
    "                print(f\"      ✅ 이전 split과 연속됨\")\n",
    "\n",
    "# 사용 예시\n",
    "if __name__ == \"__main__\":\n",
    "    # 전체 날짜 범위 확인\n",
    "    check_date_ranges()\n",
    "    \n",
    "    # 상세 일관성 분석 (14시 시나리오)\n",
    "    check_temporal_consistency(scenario='14시')\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3b26ddc",
   "metadata": {},
   "source": [
    "### normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "8726766a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder, MinMaxScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c936f783",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== 14시 시나리오 데이터 정규화 =====\n",
      "Historical columns (46):\n",
      "  Numerical: 27\n",
      "  Categorical: 3\n",
      "  Cyclical: 8\n",
      "  Special: 1\n",
      "  Plant Output: 7\n",
      "Forecast columns (5):\n",
      "  Numerical: 4\n",
      "  Categorical: 1\n",
      "  Cyclical: 0\n",
      "  Special: 0\n",
      "  Plant Output: 0\n",
      "Meta columns (3): ['capacity_kw', 'plant_age', 'region']\n",
      "\n",
      "모든 split에서 지역 정보 수집 중...\n",
      "발견된 모든 지역: ['강원도', '경기도', '경상남도', '부산', '인천광역시', '제주특별자치도', '충청북도']\n",
      "메타데이터 로드 성공\n",
      "Train 메타데이터 전처리 중...\n",
      "발전소 메타데이터 사전 구성 완료: 37개 발전소\n",
      "Train data shapes:\n",
      "  X_historical_train: (15895, 48, 46)\n",
      "  X_forecast_train: (15895, 24, 5)\n",
      "  y_train: (15895, 24)\n",
      "  X_meta_train: (15895, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/soomin/Library/Python/3.9/lib/python/site-packages/sklearn/utils/validation.py:2732: UserWarning: X has feature names, but OneHotEncoder was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "범주형 특성 '미세먼지' 레이블 인코딩:\n",
      "  원본 카테고리: ['좋음', '보통', '나쁨', '매우나쁨']\n",
      "  변환 매핑: {'좋음': 0, '보통': 1, '나쁨': 2, '매우나쁨': 3}\n",
      "\n",
      "범주형 특성 '초미세먼지' 레이블 인코딩:\n",
      "  원본 카테고리: ['좋음', '보통', '나쁨', '매우나쁨']\n",
      "  변환 매핑: {'좋음': 0, '보통': 1, '나쁨': 2, '매우나쁨': 3}\n",
      "\n",
      "범주형 특성 'sc_14' 레이블 인코딩:\n",
      "  원본 카테고리: ['1.0', '2.0', '3.0', '4.0', '0.0', 'nan']\n",
      "  변환 매핑: {'1.0': 0, '2.0': 1, '3.0': 2, '4.0': 3, '0.0': 4, 'nan': 5}\n",
      "\n",
      "발전소별 발전량 데이터 정규화 및 파생 피처 추가:\n",
      "\n",
      "Historical preprocessor fitting (excluding padding)...\n",
      "Forecast preprocessor fitting...\n",
      "변환된 특성 수:\n",
      "  Historical: 54\n",
      "  Forecast: 5\n",
      "  Meta: 10\n",
      "\n",
      "Processing train data...\n",
      "  train shapes:\n",
      "    X_historical: (15895, 48, 46)\n",
      "    X_forecast: (15895, 24, 5)\n",
      "    y: (15895, 24)\n",
      "    X_meta: (15895, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/soomin/Library/Python/3.9/lib/python/site-packages/sklearn/utils/validation.py:2732: UserWarning: X has feature names, but OneHotEncoder was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Normalized X_historical: (15895, 48, 54)\n",
      "    Normalized X_forecast: (15895, 24, 5)\n",
      "    Normalized X_meta: (15895, 10)\n",
      "\n",
      "Processing valid data...\n",
      "  valid shapes:\n",
      "    X_historical: (3406, 48, 46)\n",
      "    X_forecast: (3406, 24, 5)\n",
      "    y: (3406, 24)\n",
      "    X_meta: (3406, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/soomin/Library/Python/3.9/lib/python/site-packages/sklearn/utils/validation.py:2732: UserWarning: X has feature names, but OneHotEncoder was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Normalized X_historical: (3406, 48, 54)\n",
      "    Normalized X_forecast: (3406, 24, 5)\n",
      "    Normalized X_meta: (3406, 10)\n",
      "\n",
      "Processing test data...\n",
      "  test shapes:\n",
      "    X_historical: (3423, 48, 46)\n",
      "    X_forecast: (3423, 24, 5)\n",
      "    y: (3423, 24)\n",
      "    X_meta: (3423, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/soomin/Library/Python/3.9/lib/python/site-packages/sklearn/utils/validation.py:2732: UserWarning: X has feature names, but OneHotEncoder was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Normalized X_historical: (3423, 48, 54)\n",
      "    Normalized X_forecast: (3423, 24, 5)\n",
      "    Normalized X_meta: (3423, 10)\n",
      "\n",
      "Processing external_test data...\n",
      "  external_test shapes:\n",
      "    X_historical: (2017, 48, 46)\n",
      "    X_forecast: (2017, 24, 5)\n",
      "    y: (2017, 24)\n",
      "    X_meta: (2017, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/soomin/Library/Python/3.9/lib/python/site-packages/sklearn/utils/validation.py:2732: UserWarning: X has feature names, but OneHotEncoder was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Normalized X_historical: (2017, 48, 54)\n",
      "    Normalized X_forecast: (2017, 24, 5)\n",
      "    Normalized X_meta: (2017, 10)\n",
      "지역 인코더 저장 완료 - 총 8개 지역\n",
      "수치형 메타데이터 스케일러 저장 완료\n",
      "14시 시나리오 데이터 정규화 완료!\n",
      "===== 20시 시나리오 데이터 정규화 =====\n",
      "Historical columns (46):\n",
      "  Numerical: 27\n",
      "  Categorical: 3\n",
      "  Cyclical: 8\n",
      "  Special: 1\n",
      "  Plant Output: 7\n",
      "Forecast columns (5):\n",
      "  Numerical: 4\n",
      "  Categorical: 1\n",
      "  Cyclical: 0\n",
      "  Special: 0\n",
      "  Plant Output: 0\n",
      "Meta columns (3): ['capacity_kw', 'plant_age', 'region']\n",
      "\n",
      "모든 split에서 지역 정보 수집 중...\n",
      "발견된 모든 지역: ['강원도', '경기도', '경상남도', '부산', '인천광역시', '제주특별자치도', '충청북도']\n",
      "메타데이터 로드 성공\n",
      "Train 메타데이터 전처리 중...\n",
      "발전소 메타데이터 사전 구성 완료: 37개 발전소\n",
      "Train data shapes:\n",
      "  X_historical_train: (15895, 48, 46)\n",
      "  X_forecast_train: (15895, 24, 5)\n",
      "  y_train: (15895, 24)\n",
      "  X_meta_train: (15895, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/soomin/Library/Python/3.9/lib/python/site-packages/sklearn/utils/validation.py:2732: UserWarning: X has feature names, but OneHotEncoder was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "범주형 특성 '미세먼지' 레이블 인코딩:\n",
      "  원본 카테고리: ['좋음', '보통', '나쁨', '매우나쁨']\n",
      "  변환 매핑: {'좋음': 0, '보통': 1, '나쁨': 2, '매우나쁨': 3}\n",
      "\n",
      "범주형 특성 '초미세먼지' 레이블 인코딩:\n",
      "  원본 카테고리: ['좋음', '보통', '나쁨', '매우나쁨']\n",
      "  변환 매핑: {'좋음': 0, '보통': 1, '나쁨': 2, '매우나쁨': 3}\n",
      "\n",
      "범주형 특성 'sc_20' 레이블 인코딩:\n",
      "  원본 카테고리: ['1.0', '2.0', '3.0', '4.0', '0.0', 'nan']\n",
      "  변환 매핑: {'1.0': 0, '2.0': 1, '3.0': 2, '4.0': 3, '0.0': 4, 'nan': 5}\n",
      "\n",
      "발전소별 발전량 데이터 정규화 및 파생 피처 추가:\n",
      "\n",
      "Historical preprocessor fitting (excluding padding)...\n",
      "Forecast preprocessor fitting...\n",
      "변환된 특성 수:\n",
      "  Historical: 54\n",
      "  Forecast: 5\n",
      "  Meta: 10\n",
      "\n",
      "Processing train data...\n",
      "  train shapes:\n",
      "    X_historical: (15895, 48, 46)\n",
      "    X_forecast: (15895, 24, 5)\n",
      "    y: (15895, 24)\n",
      "    X_meta: (15895, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/soomin/Library/Python/3.9/lib/python/site-packages/sklearn/utils/validation.py:2732: UserWarning: X has feature names, but OneHotEncoder was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Normalized X_historical: (15895, 48, 54)\n",
      "    Normalized X_forecast: (15895, 24, 5)\n",
      "    Normalized X_meta: (15895, 10)\n",
      "\n",
      "Processing valid data...\n",
      "  valid shapes:\n",
      "    X_historical: (3406, 48, 46)\n",
      "    X_forecast: (3406, 24, 5)\n",
      "    y: (3406, 24)\n",
      "    X_meta: (3406, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/soomin/Library/Python/3.9/lib/python/site-packages/sklearn/utils/validation.py:2732: UserWarning: X has feature names, but OneHotEncoder was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Normalized X_historical: (3406, 48, 54)\n",
      "    Normalized X_forecast: (3406, 24, 5)\n",
      "    Normalized X_meta: (3406, 10)\n",
      "\n",
      "Processing test data...\n",
      "  test shapes:\n",
      "    X_historical: (3423, 48, 46)\n",
      "    X_forecast: (3423, 24, 5)\n",
      "    y: (3423, 24)\n",
      "    X_meta: (3423, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/soomin/Library/Python/3.9/lib/python/site-packages/sklearn/utils/validation.py:2732: UserWarning: X has feature names, but OneHotEncoder was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Normalized X_historical: (3423, 48, 54)\n",
      "    Normalized X_forecast: (3423, 24, 5)\n",
      "    Normalized X_meta: (3423, 10)\n",
      "\n",
      "Processing external_test data...\n",
      "  external_test shapes:\n",
      "    X_historical: (2017, 48, 46)\n",
      "    X_forecast: (2017, 24, 5)\n",
      "    y: (2017, 24)\n",
      "    X_meta: (2017, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/soomin/Library/Python/3.9/lib/python/site-packages/sklearn/utils/validation.py:2732: UserWarning: X has feature names, but OneHotEncoder was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Normalized X_historical: (2017, 48, 54)\n",
      "    Normalized X_forecast: (2017, 24, 5)\n",
      "    Normalized X_meta: (2017, 10)\n",
      "지역 인코더 저장 완료 - 총 8개 지역\n",
      "수치형 메타데이터 스케일러 저장 완료\n",
      "20시 시나리오 데이터 정규화 완료!\n",
      "모든 데이터 정규화 완료!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def load_column_info(input_dir, scenario):\n",
    "    \"\"\"컬럼 정보 로드\"\"\"\n",
    "    col_file = os.path.join(input_dir, f\"{scenario}_columns.json\")\n",
    "    with open(col_file, 'r') as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def identify_column_types(columns):\n",
    "    \"\"\"\n",
    "    특성 컬럼 유형 식별\n",
    "    \n",
    "    Args:\n",
    "        columns: 컬럼 목록\n",
    "    \n",
    "    Returns:\n",
    "        numerical_cols: 수치형 컬럼 목록\n",
    "        categorical_cols: 범주형 컬럼 목록\n",
    "        cyclical_cols: 순환형 컬럼 목록 (sin/cos 쌍)\n",
    "        special_cols: 특수 처리 필요 컬럼 목록\n",
    "        plant_output_cols: 발전량 관련 컬럼 목록\n",
    "    \"\"\"\n",
    "    # 발전량 관련 컬럼 (발전소별 정규화 필요)\n",
    "    plant_output_cols = ['총량(kw)', '평균(kw)', '최대(kw)', '최소(kw)', \n",
    "                          '최대(시간별_kw)', '최소(시간별_kw)', 'value']\n",
    "    plant_output_cols = [col for col in plant_output_cols if col in columns]\n",
    "    \n",
    "    # 범주형 컬럼 (기본적으로 알려진 범주형 컬럼들)\n",
    "    categorical_cols = ['미세먼지', '초미세먼지', 'sc_14', 'sc_20']\n",
    "    categorical_cols = [col for col in categorical_cols if col in columns]\n",
    "    \n",
    "    # 순환형 컬럼 (이미 sin/cos 변환된 컬럼들)\n",
    "    cyclical_pairs = [\n",
    "        ('time_hour_sin', 'time_hour_cos'),\n",
    "        ('time_day_sin', 'time_day_cos'),\n",
    "        ('time_month_sin', 'time_month_cos'),\n",
    "        ('wd_sin', 'wd_cos'),\n",
    "        ('wd_14_sin', 'wd_14_cos'),\n",
    "        ('wd_20_sin', 'wd_20_cos')\n",
    "    ]\n",
    "    \n",
    "    cyclical_cols = []\n",
    "    for sin_col, cos_col in cyclical_pairs:\n",
    "        if sin_col in columns and cos_col in columns:\n",
    "            cyclical_cols.extend([sin_col, cos_col])\n",
    "    \n",
    "    # 특수 처리 필요 컬럼\n",
    "    special_cols = ['time_is_daylight']  # 0/1 값을 가지는 특수 컬럼\n",
    "    special_cols = [col for col in special_cols if col in columns]\n",
    "    \n",
    "    # 나머지는 수치형으로 처리\n",
    "    numerical_cols = [col for col in columns if col not in categorical_cols + \n",
    "                     cyclical_cols + special_cols + plant_output_cols]\n",
    "    \n",
    "    return numerical_cols, categorical_cols, cyclical_cols, special_cols, plant_output_cols\n",
    "\n",
    "def create_preprocessor(numerical_cols, categorical_cols, cyclical_cols, special_cols, plant_output_cols):\n",
    "    \"\"\"\n",
    "    전처리 파이프라인 생성\n",
    "    \n",
    "    Args:\n",
    "        numerical_cols: 수치형 컬럼 목록\n",
    "        categorical_cols: 범주형 컬럼 목록\n",
    "        cyclical_cols: 순환형 컬럼 목록\n",
    "        special_cols: 특수 처리 컬럼 목록\n",
    "    \n",
    "    Returns:\n",
    "        preprocessor: 전처리 파이프라인\n",
    "    \"\"\"\n",
    "    # 수치형 변수 전처리 (StandardScaler)\n",
    "    numerical_transformer = Pipeline(steps=[\n",
    "        ('scaler', StandardScaler())\n",
    "    ])\n",
    "    \n",
    "    # 범주형 변수 전처리 (LabelEncoder로 변경)\n",
    "    # LabelEncoder는 각 컬럼별로 독립적으로 적용해야 하므로 'passthrough'로 설정\n",
    "    categorical_transformer = 'passthrough'\n",
    "    \n",
    "    # 순환형 변수 전처리 (MinMaxScaler->아님! 안해도 이미 -1,1 사이임)\n",
    "    '''\n",
    "    cyclical_transformer = Pipeline(steps=[\n",
    "        ('minmax', MinMaxScaler(feature_range=(-1, 1)))\n",
    "    ])\n",
    "    '''\n",
    "    cyclical_transformer = 'passthrough'\n",
    "    \n",
    "    # 특수 변수 전처리 (그대로 유지)\n",
    "    special_transformer = 'passthrough'\n",
    "\n",
    "    # 발전량 관련 변수 전처리 (StandardScaler)\n",
    "    plant_output_transformer = Pipeline(steps=[\n",
    "        ('scaler', StandardScaler())  # 발전량 값이 크므로 StandardScaler 적용\n",
    "    ])    \n",
    "    \n",
    "    # 전처리 파이프라인 구성\n",
    "    transformers = []\n",
    "    \n",
    "    if numerical_cols:\n",
    "        transformers.append(('num', numerical_transformer, numerical_cols))\n",
    "    \n",
    "    if categorical_cols:\n",
    "        transformers.append(('cat', categorical_transformer, categorical_cols))\n",
    "    \n",
    "    if cyclical_cols:\n",
    "        transformers.append(('cyc', cyclical_transformer, cyclical_cols))\n",
    "    \n",
    "    if special_cols:\n",
    "        transformers.append(('spe', special_transformer, special_cols))\n",
    "\n",
    "    if plant_output_cols:\n",
    "        transformers.append(('out', plant_output_transformer, plant_output_cols))        \n",
    "    \n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=transformers,\n",
    "        remainder='drop'  # 지정되지 않은 컬럼 제외\n",
    "    )\n",
    "    \n",
    "    return preprocessor\n",
    "\n",
    "def get_feature_names(preprocessor, numerical_cols, categorical_cols, cyclical_cols, \n",
    "                      special_cols, plant_output_cols, additional_features=None):\n",
    "    \"\"\"\n",
    "    전처리 후 특성 이름 추출\n",
    "    \n",
    "    Args:\n",
    "        preprocessor: 학습된 전처리 파이프라인\n",
    "        numerical_cols: 수치형 컬럼 목록\n",
    "        categorical_cols: 범주형 컬럼 목록\n",
    "        cyclical_cols: 순환형 컬럼 목록\n",
    "        special_cols: 특수 처리 컬럼 목록\n",
    "        plant_output_cols: 발전량 관련 컬럼 목록\n",
    "        additional_features: 추가된 파생 피처 목록\n",
    "    \n",
    "    Returns:\n",
    "        feature_names: 전처리 후 특성 이름 목록\n",
    "    \"\"\"\n",
    "    feature_names = []\n",
    "    \n",
    "    # 수치형 변수 이름 (변경 없음)\n",
    "    if numerical_cols:\n",
    "        feature_names.extend(numerical_cols)\n",
    "    \n",
    "    # 범주형 변수 이름 (LabelEncoder 적용 시 이름 변경 없음)\n",
    "    if categorical_cols:\n",
    "        feature_names.extend(categorical_cols)\n",
    "    \n",
    "    # 순환형 변수 이름 (변경 없음)\n",
    "    if cyclical_cols:\n",
    "        feature_names.extend(cyclical_cols)\n",
    "    \n",
    "    # 특수 변수 이름 (변경 없음)\n",
    "    if special_cols:\n",
    "        feature_names.extend(special_cols)\n",
    "    \n",
    "    # 발전량 관련 변수 이름 (정규화 후에도 이름 유지)\n",
    "    if plant_output_cols:\n",
    "        feature_names.extend(plant_output_cols)\n",
    "    \n",
    "    # 추가 파생 피처 이름\n",
    "    if additional_features:\n",
    "        feature_names.extend(additional_features)\n",
    "    \n",
    "    return feature_names\n",
    "\n",
    "'''def preprocess_meta_data(meta_data):\n",
    "    \"\"\"\n",
    "    메타데이터 전처리\n",
    "    \n",
    "    Args:\n",
    "        meta_data: 메타데이터 배열 (X_meta)\n",
    "    \n",
    "    Returns:\n",
    "        processed_meta: 전처리된 메타데이터\n",
    "        meta_feature_names: 메타데이터 피처 이름\n",
    "        region_encoder: 지역 인코더 (있는 경우)\n",
    "    \"\"\"\n",
    "    # 메타데이터가 없으면 빈 배열 반환\n",
    "    if meta_data is None or len(meta_data) == 0:\n",
    "        return np.array([]), [], None\n",
    "    \n",
    "    print(\"메타데이터 전처리 중...\")\n",
    "    \n",
    "    # 메타데이터를 DataFrame으로 변환\n",
    "    if isinstance(meta_data[0], dict):\n",
    "        # 딕셔너리 형태인 경우\n",
    "        meta_df = pd.DataFrame(meta_data)\n",
    "    else:\n",
    "        # 배열 형태인 경우\n",
    "        meta_df = pd.DataFrame(meta_data, columns=['capacity_kw', 'plant_age', 'region'])\n",
    "    \n",
    "    # 메타데이터 컬럼 추출\n",
    "    numeric_cols = ['capacity_kw', 'plant_age']\n",
    "    numeric_cols = [col for col in numeric_cols if col in meta_df.columns]\n",
    "    \n",
    "    categorical_cols = ['region']\n",
    "    categorical_cols = [col for col in categorical_cols if col in meta_df.columns]\n",
    "    \n",
    "    # 수치형 메타데이터 스케일링\n",
    "    meta_scaler = StandardScaler()\n",
    "    if numeric_cols:\n",
    "        # NaN 값 처리\n",
    "        meta_df[numeric_cols] = meta_df[numeric_cols].fillna(0)\n",
    "        meta_df[numeric_cols] = meta_scaler.fit_transform(meta_df[numeric_cols])\n",
    "    \n",
    "    # 범주형 메타데이터 원-핫 인코딩\n",
    "    region_encoder = None\n",
    "    if categorical_cols:\n",
    "        # NaN 값 처리\n",
    "        meta_df[categorical_cols] = meta_df[categorical_cols].fillna('Unknown')\n",
    "        \n",
    "        # 원-핫 인코딩\n",
    "        region_encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "        region_encoded = region_encoder.fit_transform(meta_df[categorical_cols])\n",
    "        \n",
    "        # 원-핫 인코딩 결과를 DataFrame으로 변환\n",
    "        region_cols = [f\"region_{cat}\" for cat in region_encoder.categories_[0]]\n",
    "        region_df = pd.DataFrame(region_encoded, columns=region_cols)\n",
    "        \n",
    "        # 원본 데이터프레임에서 범주형 컬럼 제거\n",
    "        meta_df = meta_df.drop(columns=categorical_cols)\n",
    "        \n",
    "        # 원-핫 인코딩 결과 병합\n",
    "        meta_df = pd.concat([meta_df, region_df], axis=1)\n",
    "    \n",
    "    # 메타데이터 피처 이름\n",
    "    meta_feature_names = list(meta_df.columns)\n",
    "    \n",
    "    # 전처리된 메타데이터 반환\n",
    "    return meta_df.values, meta_feature_names, region_encoder\n",
    "'''\n",
    "\n",
    "\n",
    "def create_region_encoder_from_all_splits(scenario_dir, split_names):\n",
    "    \"\"\"\n",
    "    모든 split의 지역 정보를 합쳐서 OneHotEncoder 생성\n",
    "    \n",
    "    Args:\n",
    "        scenario_dir: 시나리오 디렉토리 경로\n",
    "        split_names: 분할 데이터 이름 목록\n",
    "    \n",
    "    Returns:\n",
    "        region_encoder: 모든 지역을 포함한 OneHotEncoder\n",
    "        all_regions: 발견된 모든 지역 목록\n",
    "    \"\"\"\n",
    "    all_regions = set()\n",
    "    \n",
    "    # 모든 split에서 지역 정보 수집\n",
    "    for split_name in split_names:\n",
    "        split_dir = os.path.join(scenario_dir, split_name)\n",
    "        try:\n",
    "            X_meta = np.load(os.path.join(split_dir, 'X_meta.npy'), allow_pickle=True)\n",
    "            \n",
    "            # 지역 정보 추출 (인덱스 2가 region)\n",
    "            for meta in X_meta:\n",
    "                if len(meta) > 2:\n",
    "                    region = str(meta[2]) if meta[2] is not None else \"Unknown\"\n",
    "                    all_regions.add(region)\n",
    "                    \n",
    "        except Exception as e:\n",
    "            print(f\"  {split_name} 세트에서 지역 정보 수집 실패: {e}\")\n",
    "    \n",
    "    print(f\"발견된 모든 지역: {sorted(all_regions)}\")\n",
    "    \n",
    "    # OneHotEncoder 생성\n",
    "    if all_regions:\n",
    "        # Unknown 지역도 포함\n",
    "        if \"Unknown\" not in all_regions:\n",
    "            all_regions.add(\"Unknown\")\n",
    "        \n",
    "        # 지역 목록을 정렬하여 일관성 유지\n",
    "        region_list = sorted(list(all_regions))\n",
    "        \n",
    "        # OneHotEncoder 학습\n",
    "        region_encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "        region_encoder.fit(np.array(region_list).reshape(-1, 1))\n",
    "        \n",
    "        return region_encoder, region_list\n",
    "    else:\n",
    "        return None, []\n",
    "\n",
    "\n",
    "def preprocess_meta_data_train_only(meta_data, region_encoder):\n",
    "    \"\"\"\n",
    "    Train 메타데이터 전처리 (수치형만 학습, 지역은 미리 만든 encoder 사용)\n",
    "    \n",
    "    Args:\n",
    "        meta_data: Train 메타데이터 배열\n",
    "        region_encoder: 미리 생성된 지역 인코더\n",
    "    \n",
    "    Returns:\n",
    "        processed_meta: 전처리된 메타데이터\n",
    "        meta_feature_names: 메타데이터 피처 이름\n",
    "        numeric_scaler: 수치형 데이터 스케일러\n",
    "    \"\"\"\n",
    "    # 메타데이터가 없으면 빈 배열 반환\n",
    "    if meta_data is None or len(meta_data) == 0:\n",
    "        return np.array([]), [], None\n",
    "    \n",
    "    print(\"Train 메타데이터 전처리 중...\")\n",
    "    \n",
    "    # 메타데이터를 DataFrame으로 변환\n",
    "    if isinstance(meta_data[0], dict):\n",
    "        meta_df = pd.DataFrame(meta_data)\n",
    "    else:\n",
    "        meta_df = pd.DataFrame(meta_data, columns=['capacity_kw', 'plant_age', 'region'])\n",
    "    \n",
    "    # 메타데이터 컬럼 추출\n",
    "    numeric_cols = ['capacity_kw', 'plant_age']\n",
    "    numeric_cols = [col for col in numeric_cols if col in meta_df.columns]\n",
    "    \n",
    "    categorical_cols = ['region']\n",
    "    categorical_cols = [col for col in categorical_cols if col in meta_df.columns]\n",
    "    \n",
    "    # ✅ 수치형 메타데이터 스케일링 (Train 데이터로만 학습)\n",
    "    numeric_scaler = None\n",
    "    if numeric_cols:\n",
    "        # NaN 값 처리\n",
    "        meta_df[numeric_cols] = meta_df[numeric_cols].fillna(0)\n",
    "        \n",
    "        # Train 데이터로 scaler 학습\n",
    "        numeric_scaler = StandardScaler()\n",
    "        meta_df[numeric_cols] = numeric_scaler.fit_transform(meta_df[numeric_cols])\n",
    "    \n",
    "    # ✅ 범주형 메타데이터 원-핫 인코딩 (미리 만든 encoder 사용)\n",
    "    if categorical_cols and region_encoder is not None:\n",
    "        # NaN 값 처리\n",
    "        meta_df[categorical_cols] = meta_df[categorical_cols].fillna('Unknown')\n",
    "        \n",
    "        # 미리 학습된 encoder로 변환\n",
    "        region_encoded = region_encoder.transform(meta_df[categorical_cols])\n",
    "        \n",
    "        # 원-핫 인코딩 결과를 DataFrame으로 변환\n",
    "        region_cols = [f\"region_{cat}\" for cat in region_encoder.categories_[0]]\n",
    "        region_df = pd.DataFrame(region_encoded, columns=region_cols)\n",
    "        \n",
    "        # 원본 데이터프레임에서 범주형 컬럼 제거\n",
    "        meta_df = meta_df.drop(columns=categorical_cols)\n",
    "        \n",
    "        # 원-핫 인코딩 결과 병합\n",
    "        meta_df = pd.concat([meta_df, region_df], axis=1)\n",
    "    \n",
    "    # 메타데이터 피처 이름\n",
    "    meta_feature_names = list(meta_df.columns)\n",
    "    \n",
    "    # 전처리된 메타데이터와 수치형 스케일러 반환\n",
    "    return meta_df.values, meta_feature_names, numeric_scaler\n",
    "\n",
    "\n",
    "def apply_meta_preprocessing(X_meta, meta_cols, region_encoder, numeric_scaler):\n",
    "    \"\"\"\n",
    "    학습된 전처리기를 사용하여 메타데이터 변환\n",
    "    \n",
    "    Args:\n",
    "        X_meta: 변환할 메타데이터\n",
    "        meta_cols: 메타데이터 컬럼명\n",
    "        region_encoder: 학습된 지역 인코더\n",
    "        numeric_scaler: 학습된 수치형 스케일러\n",
    "    \n",
    "    Returns:\n",
    "        X_meta_normalized: 전처리된 메타데이터\n",
    "    \"\"\"\n",
    "    if X_meta is None:\n",
    "        return None\n",
    "    \n",
    "    # 메타데이터를 DataFrame으로 변환\n",
    "    if isinstance(X_meta[0], (list, np.ndarray)):\n",
    "        meta_df = pd.DataFrame(X_meta, columns=meta_cols[:len(X_meta[0])])\n",
    "    else:\n",
    "        meta_df = pd.DataFrame(X_meta.reshape(-1, 1), columns=meta_cols[:1])\n",
    "    \n",
    "    # 수치형 메타데이터 변환\n",
    "    numeric_meta_cols = [col for col in ['capacity_kw', 'plant_age'] if col in meta_df.columns]\n",
    "    if numeric_meta_cols and numeric_scaler is not None:\n",
    "        # ✅ 학습된 scaler로 transform만 수행\n",
    "        meta_df[numeric_meta_cols] = meta_df[numeric_meta_cols].fillna(0)\n",
    "        meta_df[numeric_meta_cols] = numeric_scaler.transform(meta_df[numeric_meta_cols])\n",
    "    \n",
    "    # 범주형 메타데이터 변환\n",
    "    categorical_meta_cols = [col for col in ['region'] if col in meta_df.columns]\n",
    "    if categorical_meta_cols and region_encoder is not None:\n",
    "        # NaN 값 처리\n",
    "        meta_df[categorical_meta_cols] = meta_df[categorical_meta_cols].fillna('Unknown')\n",
    "        \n",
    "        # ✅ 학습된 encoder로 transform만 수행\n",
    "        region_encoded = region_encoder.transform(meta_df[categorical_meta_cols])\n",
    "        \n",
    "        # 원-핫 인코딩 결과를 DataFrame으로 변환\n",
    "        region_cols = [f\"region_{cat}\" for cat in region_encoder.categories_[0]]\n",
    "        region_df = pd.DataFrame(region_encoded, columns=region_cols)\n",
    "        \n",
    "        # 원본 데이터프레임에서 범주형 컬럼 제거\n",
    "        meta_df = meta_df.drop(columns=categorical_meta_cols)\n",
    "        \n",
    "        # 원-핫 인코딩 결과 병합\n",
    "        meta_df = pd.concat([meta_df, region_df], axis=1)\n",
    "    \n",
    "    return meta_df.values\n",
    "\n",
    "def normalize_plant_output(data, plant_meta, plant_id, plant_output_cols):\n",
    "    \"\"\"\n",
    "    발전량 데이터 정규화 (설비 용량 기반)\n",
    "    \n",
    "    Args:\n",
    "        data: 발전소 데이터 (DataFrame)\n",
    "        plant_meta: 발전소 메타데이터 (Dictionary)\n",
    "        plant_id: 발전소 ID\n",
    "        plant_output_cols: 발전량 관련 컬럼 목록\n",
    "    \n",
    "    Returns:\n",
    "        normalized_data: 정규화된 데이터 (DataFrame)\n",
    "        added_features: 추가된 파생 피처 목록\n",
    "    \"\"\"\n",
    "    # 데이터 복사본 생성\n",
    "    normalized_data = data.copy()\n",
    "    added_features = []\n",
    "    \n",
    "    # 발전소 용량 정보가 없으면 통계 기반 정규화 수행\n",
    "    if plant_meta is None or 'capacity_kw' not in plant_meta or plant_meta['capacity_kw'] == 0:\n",
    "        # 각 발전량 컬럼별로 평균/표준편차 정규화 수행\n",
    "        for col in plant_output_cols:\n",
    "            if col in normalized_data.columns:\n",
    "                # 문자열인 경우 숫자로 변환 시도\n",
    "                try:\n",
    "                    normalized_data[col] = pd.to_numeric(normalized_data[col], errors='coerce').astype(np.float32)\n",
    "                except:\n",
    "                    print(f\"    경고: '{col}' 컬럼 숫자 변환 실패, 정규화 건너뜀\")\n",
    "                    continue\n",
    "                \n",
    "                # NaN 값 처리\n",
    "                if normalized_data[col].isna().any():\n",
    "                    print(f\"    경고: '{col}' 컬럼에 NaN 값 존재, 0으로 대체\")\n",
    "                    normalized_data[col] = normalized_data[col].fillna(0)\n",
    "                \n",
    "                mean = normalized_data[col].mean()\n",
    "                std = normalized_data[col].std()\n",
    "                if std == 0:\n",
    "                    std = 1.0  # 0으로 나누기 방지\n",
    "                normalized_data[col] = (normalized_data[col] - mean) / std\n",
    "        \n",
    "        print(f\"  발전소 {plant_id}의 용량 정보 없음: 통계 기반 정규화 수행\")\n",
    "        return normalized_data, added_features\n",
    "    \n",
    "    # 발전소 용량 정보가 있는 경우 용량 기반 정규화 수행\n",
    "    capacity_kw = plant_meta['capacity_kw']\n",
    "    #print(f\"  발전소 {plant_id}의 용량: {capacity_kw} kW\")\n",
    "    \n",
    "    # 발전량 데이터를 설비 용량으로 나누어 정규화 (이용률로 변환)\n",
    "    for col in plant_output_cols:\n",
    "        if col in normalized_data.columns:\n",
    "            # 문자열인 경우 숫자로 변환 시도\n",
    "            try:\n",
    "                normalized_data[col] = pd.to_numeric(normalized_data[col], errors='coerce').astype(np.float32)\n",
    "            except:\n",
    "                print(f\"    경고: '{col}' 컬럼 숫자 변환 실패, 정규화 건너뜀\")\n",
    "                continue\n",
    "            \n",
    "            # NaN 값 처리\n",
    "            if normalized_data[col].isna().any():\n",
    "                print(f\"    경고: '{col}' 컬럼에 NaN 값 존재, 0으로 대체\")\n",
    "                normalized_data[col] = normalized_data[col].fillna(0)\n",
    "            \n",
    "            # capacity_kw가 문자열인 경우 숫자로 변환\n",
    "            if isinstance(capacity_kw, str):\n",
    "                try:\n",
    "                    capacity_kw = float(capacity_kw)\n",
    "                except:\n",
    "                    print(f\"    경고: 용량 값 '{capacity_kw}' 숫자 변환 실패, 정규화 건너뜀\")\n",
    "                    continue\n",
    "            \n",
    "            # 용량이 0인 경우 처리\n",
    "            if capacity_kw == 0:\n",
    "                print(f\"    경고: 용량이 0, 정규화 대신 0으로 설정\")\n",
    "                normalized_data[f\"{col}_ratio\"] = 0\n",
    "            else:\n",
    "                # 발전량을 용량으로 나누어 이용률로 변환 (0~1 범위)\n",
    "                normalized_data[f\"{col}_ratio\"] = normalized_data[col] / capacity_kw\n",
    "            \n",
    "            added_features.append(f\"{col}_ratio\")\n",
    "    \n",
    "    # 하루 전 같은 시간대 대비 변화율 계산 (value 컬럼 대상)\n",
    "    if 'value' in normalized_data.columns and len(normalized_data) > 24:\n",
    "        # 문자열인 경우 숫자로 변환 시도\n",
    "        if not pd.api.types.is_numeric_dtype(normalized_data['value']):\n",
    "            try:\n",
    "                normalized_data['value'] = pd.to_numeric(normalized_data['value'], errors='coerce')\n",
    "                normalized_data['value'] = normalized_data['value'].fillna(0)\n",
    "            except:\n",
    "                print(f\"    경고: 'value' 컬럼 숫자 변환 실패, 변화율 계산 건너뜀\")\n",
    "                return normalized_data, added_features\n",
    "        \n",
    "        # 24시간(하루) 전 발전량\n",
    "        prev_day_values = normalized_data['value'].shift(24)\n",
    "        \n",
    "        # 변화율 계산 (이전 값이 0인 경우 처리)\n",
    "        normalized_data['value_day_change'] = normalized_data['value'].div(\n",
    "            prev_day_values.replace(0, 1e-8)).replace([np.inf, -np.inf], 0) - 1\n",
    "        \n",
    "        # NaN 값을 0으로 대체\n",
    "        normalized_data['value_day_change'] = normalized_data['value_day_change'].fillna(0)\n",
    "        \n",
    "        # 이상치 제한 (-1 ~ 1 범위로 클리핑)\n",
    "        normalized_data['value_day_change'] = np.clip(normalized_data['value_day_change'], -1, 1)\n",
    "        \n",
    "        added_features.append('value_day_change')\n",
    "    \n",
    "    return normalized_data, added_features\n",
    "\n",
    "\n",
    "def normalize_single_target_value(y_sample, plant_id, plant_meta):\n",
    "    \"\"\"\n",
    "    단일 샘플의 Y값 정규화 (X와 동일한 방식)\n",
    "    \"\"\"\n",
    "    # 용량 기반 정규화\n",
    "    capacity_kw = plant_meta['capacity_kw']\n",
    "    if isinstance(capacity_kw, str):\n",
    "        try:\n",
    "            capacity_kw = float(capacity_kw)\n",
    "        except:\n",
    "            capacity_kw = 0\n",
    "    \n",
    "    if capacity_kw > 0:\n",
    "        return y_sample / capacity_kw, 'capacity_based', capacity_kw\n",
    "\n",
    "    # 통계 기반 정규화는 개별 샘플로는 불가능하므로 원본 반환\n",
    "    return y_sample, 'no_normalization', None    \n",
    "\n",
    "\n",
    "def normalize_data(input_dir, output_dir, scenario, split_names=['train', 'valid', 'test', 'external_test']):\n",
    "    \"\"\"\n",
    "    데이터 정규화 및 저장\n",
    "    \n",
    "    Args:\n",
    "        input_dir: 입력 데이터 디렉토리\n",
    "        output_dir: 출력 데이터 디렉토리\n",
    "        scenario: 시나리오 이름 ('14시' 또는 '20시')\n",
    "        split_names: 분할 데이터 이름 목록\n",
    "    \"\"\"\n",
    "    print(f\"===== {scenario} 시나리오 데이터 정규화 =====\")\n",
    "    \n",
    "    # 컬럼 정보 로드\n",
    "    column_info = load_column_info(input_dir, scenario)\n",
    "    historical_cols = column_info['historical_cols']\n",
    "    forecast_cols = column_info['forecast_cols']\n",
    "    meta_cols = column_info.get('meta_cols', [])\n",
    "    \n",
    "    # 특성 유형 식별\n",
    "    historical_numerical, historical_categorical, historical_cyclical, historical_special, historical_plant_output = identify_column_types(historical_cols)\n",
    "    forecast_numerical, forecast_categorical, forecast_cyclical, forecast_special, forecast_plant_output = identify_column_types(forecast_cols)\n",
    "    \n",
    "    print(f\"Historical columns ({len(historical_cols)}):\")\n",
    "    print(f\"  Numerical: {len(historical_numerical)}\")\n",
    "    print(f\"  Categorical: {len(historical_categorical)}\")\n",
    "    print(f\"  Cyclical: {len(historical_cyclical)}\")\n",
    "    print(f\"  Special: {len(historical_special)}\")\n",
    "    print(f\"  Plant Output: {len(historical_plant_output)}\")\n",
    "    \n",
    "    print(f\"Forecast columns ({len(forecast_cols)}):\")\n",
    "    print(f\"  Numerical: {len(forecast_numerical)}\")\n",
    "    print(f\"  Categorical: {len(forecast_categorical)}\")\n",
    "    print(f\"  Cyclical: {len(forecast_cyclical)}\")\n",
    "    print(f\"  Special: {len(forecast_special)}\")\n",
    "    print(f\"  Plant Output: {len(forecast_plant_output)}\")\n",
    "    \n",
    "    print(f\"Meta columns ({len(meta_cols)}): {meta_cols}\")\n",
    "    \n",
    "    # 시나리오 디렉토리 생성\n",
    "    scenario_dir = os.path.join(input_dir, scenario)\n",
    "    output_scenario_dir = os.path.join(output_dir, scenario)\n",
    "    os.makedirs(output_scenario_dir, exist_ok=True)\n",
    "    \n",
    "    # 전처리 파이프라인 생성 (Historical 데이터용)\n",
    "    historical_preprocessor = create_preprocessor(\n",
    "        historical_numerical, \n",
    "        historical_categorical, \n",
    "        historical_cyclical, \n",
    "        historical_special,\n",
    "        historical_plant_output\n",
    "    )\n",
    "    \n",
    "    # 전처리 파이프라인 생성 (Forecast 데이터용)\n",
    "    forecast_preprocessor = create_preprocessor(\n",
    "        forecast_numerical, \n",
    "        forecast_categorical, \n",
    "        forecast_cyclical, \n",
    "        forecast_special,\n",
    "        forecast_plant_output\n",
    "    )\n",
    "    \n",
    "    # 레이블 인코더 생성 (범주형 변수용)\n",
    "    historical_label_encoders = {}\n",
    "    forecast_label_encoders = {}\n",
    "\n",
    "    # ✅ 1단계: 모든 split에서 지역 정보 수집하여 OneHotEncoder 생성\n",
    "    print(\"\\n모든 split에서 지역 정보 수집 중...\")\n",
    "    region_encoder, all_regions = create_region_encoder_from_all_splits(scenario_dir, split_names)\n",
    "    \n",
    "    \n",
    "    # 학습 데이터 로드\n",
    "    train_dir = os.path.join(scenario_dir, 'train')\n",
    "    X_historical_train = np.load(os.path.join(train_dir, 'X_historical.npy'), allow_pickle=True)\n",
    "    X_forecast_train = np.load(os.path.join(train_dir, 'X_forecast.npy'), allow_pickle=True)\n",
    "    y_train = np.load(os.path.join(train_dir, 'y.npy'), allow_pickle=True)\n",
    "    plants_train = np.load(os.path.join(train_dir, 'plants.npy'), allow_pickle=True)\n",
    "    \n",
    "    # 메타데이터 로드 (있는 경우)\n",
    "    X_meta_train = None\n",
    "    try:\n",
    "        X_meta_train = np.load(os.path.join(train_dir, 'X_meta.npy'), allow_pickle=True)\n",
    "        print(\"메타데이터 로드 성공\")\n",
    "    except:\n",
    "        print(\"메타데이터 파일이 없거나 로드할 수 없습니다.\")\n",
    "\n",
    "    # ✅ 2단계: Train 데이터로 수치형 스케일러만 학습 (지역은 미리 만든 encoder 사용)\n",
    "    processed_meta, meta_feature_names, numeric_scaler = preprocess_meta_data_train_only(\n",
    "        X_meta_train, region_encoder\n",
    "    )        \n",
    "    \n",
    "    # 발전소별 메타데이터 사전 구성\n",
    "    # 발전소별 메타데이터 사전 구성 - 모든 데이터 세트에서 수집\n",
    "    plant_meta_dict = {}\n",
    "\n",
    "    # 모든 분할 데이터 세트에서 메타데이터 수집\n",
    "    for split_name in ['train', 'valid', 'test', 'external_test']:\n",
    "        split_dir = os.path.join(scenario_dir, split_name)\n",
    "        try:\n",
    "            # 해당 세트의 발전소 및 메타데이터 로드\n",
    "            plants_split = np.load(os.path.join(split_dir, 'plants.npy'), allow_pickle=True)\n",
    "            X_meta_split = np.load(os.path.join(split_dir, 'X_meta.npy'), allow_pickle=True)\n",
    "            \n",
    "            # 메타데이터 사전에 추가\n",
    "            unique_plants = np.unique(plants_split)\n",
    "            for plant_id in unique_plants:\n",
    "                if plant_id not in plant_meta_dict:  # 아직 사전에 없는 발전소만 추가\n",
    "                    # 현재 발전소의 첫 번째 샘플 인덱스 찾기\n",
    "                    plant_idx = np.where(plants_split == plant_id)[0][0]\n",
    "                    \n",
    "                    # 해당 발전소의 메타데이터 저장\n",
    "                    plant_meta_dict[plant_id] = {\n",
    "                        'capacity_kw': X_meta_split[plant_idx][0] if len(X_meta_split[plant_idx]) > 0 else 0,\n",
    "                        'plant_age': X_meta_split[plant_idx][1] if len(X_meta_split[plant_idx]) > 1 else 0,\n",
    "                        'region': X_meta_split[plant_idx][2] if len(X_meta_split[plant_idx]) > 2 else \"Unknown\"\n",
    "                    }\n",
    "        except:\n",
    "            print(f\"  {split_name} 세트에서 메타데이터를 로드할 수 없습니다.\")\n",
    "\n",
    "    print(f\"발전소 메타데이터 사전 구성 완료: {len(plant_meta_dict)}개 발전소\")\n",
    "    \n",
    "    # 데이터 형태 확인\n",
    "    print(f\"Train data shapes:\")\n",
    "    print(f\"  X_historical_train: {X_historical_train.shape}\")\n",
    "    print(f\"  X_forecast_train: {X_forecast_train.shape}\")\n",
    "    print(f\"  y_train: {y_train.shape}\")\n",
    "    if X_meta_train is not None:\n",
    "        print(f\"  X_meta_train: {X_meta_train.shape}\")\n",
    "    \n",
    "    # 시나리오에 따른 패딩 식별\n",
    "    cutoff_hour = 14 if scenario == '14시' else 20\n",
    "    padding_size = 24 - cutoff_hour\n",
    "    \n",
    "    # 3D 데이터를 2D로 변환 (시간 차원 펼치기)\n",
    "    n_samples_train, n_timesteps_hist, n_features_hist = X_historical_train.shape\n",
    "    n_samples_train, n_timesteps_forecast, n_features_forecast = X_forecast_train.shape\n",
    "    \n",
    "    # 패딩 마스크 생성 (패딩이 아닌 실제 데이터 위치: True)\n",
    "    # 형태: [n_samples, n_timesteps]\n",
    "    mask_hist = np.ones((n_samples_train, n_timesteps_hist), dtype=bool)\n",
    "    \n",
    "    # history_days를 추정 (전체 시간 단계에서 하루 24시간을 뺀 값)\n",
    "    history_days = (n_timesteps_hist - 24) // 24\n",
    "    \n",
    "    # 패딩 위치 계산: D-1일의 cutoff_hour 이후 시간\n",
    "    # 패딩 시작 인덱스: history_days*24 + cutoff_hour\n",
    "    padding_start_idx = history_days * 24 + cutoff_hour\n",
    "    \n",
    "    # 패딩 위치 마스킹\n",
    "    mask_hist[:, padding_start_idx:] = False\n",
    "    \n",
    "    # 마스크를 2D로 변환\n",
    "    mask_hist_2d = mask_hist.reshape(-1)\n",
    "    \n",
    "    # 실제 데이터만 추출하여 2D로 변환\n",
    "    X_historical_train_2d = X_historical_train.reshape(-1, n_features_hist)\n",
    "    X_forecast_train_2d = X_forecast_train.reshape(-1, n_features_forecast)\n",
    "    \n",
    "    # 패딩을 제외한 실제 데이터만 선택\n",
    "    X_historical_train_real = X_historical_train_2d[mask_hist_2d]\n",
    "    \n",
    "    # 2D 마스크에 대응하는 샘플 인덱스\n",
    "    sample_indices_2d = np.repeat(np.arange(n_samples_train), n_timesteps_hist)[mask_hist_2d]\n",
    "    \n",
    "    # 2D 마스크에 대응하는 발전소 이름\n",
    "    plants_2d = plants_train[sample_indices_2d]\n",
    "    \n",
    "    # 데이터프레임으로 변환\n",
    "    historical_df_train = pd.DataFrame(X_historical_train_real, columns=historical_cols)\n",
    "    forecast_df_train = pd.DataFrame(X_forecast_train_2d, columns=forecast_cols)\n",
    "    \n",
    "    # 범주형 변수에 대한 LabelEncoder 학습 및 변환 매핑 저장\n",
    "    label_mapping = {}\n",
    "            \n",
    "    # Historical 데이터의 범주형 변수 처리\n",
    "    for col in historical_categorical:\n",
    "        print(f\"\\n범주형 특성 '{col}' 레이블 인코딩:\")\n",
    "        \n",
    "        if col == '미세먼지' or col == '초미세먼지':\n",
    "            # 미세먼지/초미세먼지 순서 정의 (좋음 > 보통 > 나쁨 > 매우나쁨)\n",
    "            ordered_categories = ['좋음', '보통', '나쁨', '매우나쁨']\n",
    "            \n",
    "            # 데이터에 있는 추가 카테고리 확인 (예: nan)\n",
    "            data_categories = set(historical_df_train[col].astype(str).unique())\n",
    "            for category in data_categories:\n",
    "                if category not in ordered_categories and category != 'nan':\n",
    "                    ordered_categories.append(category)\n",
    "            \n",
    "            # nan 값이 있으면 마지막에 추가\n",
    "            if 'nan' in data_categories:\n",
    "                ordered_categories.append('nan')\n",
    "            \n",
    "            # 직접 매핑 생성\n",
    "            mapping = {cat: idx for idx, cat in enumerate(ordered_categories)}\n",
    "            \n",
    "            # 직접 변환 함수 생성\n",
    "            def transform_func(x, mapping=mapping):\n",
    "                return np.array([mapping.get(str(val), len(mapping) - 1) for val in x])\n",
    "            \n",
    "            # 변환 함수 저장\n",
    "            historical_label_encoders[col] = transform_func\n",
    "            \n",
    "        elif col == 'sc_14' or col == 'sc_20':\n",
    "            # 하늘상태(sc) 순서 정의 (1=맑음, 2=구름조금, 3=구름많음, 4=흐림)\n",
    "            ordered_categories = ['1.0', '2.0', '3.0', '4.0']\n",
    "            \n",
    "            # 데이터에 있는 추가 카테고리 확인\n",
    "            data_categories = set(historical_df_train[col].astype(str).unique())\n",
    "            for category in data_categories:\n",
    "                if category not in ordered_categories and category != 'nan':\n",
    "                    ordered_categories.append(category)\n",
    "            \n",
    "            # nan 값이 있으면 마지막에 추가\n",
    "            if 'nan' in data_categories:\n",
    "                ordered_categories.append('nan')\n",
    "            \n",
    "            # 직접 매핑 생성\n",
    "            mapping = {cat: idx for idx, cat in enumerate(ordered_categories)}\n",
    "            \n",
    "            # 직접 변환 함수 생성\n",
    "            def transform_func(x, mapping=mapping):\n",
    "                return np.array([mapping.get(str(val), len(mapping) - 1) for val in x])\n",
    "            \n",
    "            # 변환 함수 저장\n",
    "            historical_label_encoders[col] = transform_func\n",
    "            \n",
    "        else:\n",
    "            # 기존 방식 사용 (다른 범주형 변수)\n",
    "            encoder = LabelEncoder()\n",
    "            values = historical_df_train[col].astype(str).unique()\n",
    "            encoder.fit(values)\n",
    "            historical_label_encoders[col] = encoder\n",
    "            \n",
    "            # 기존 매핑 정보 저장\n",
    "            mapping = {val: int(idx) for idx, val in enumerate(encoder.classes_)}\n",
    "        \n",
    "        # 매핑 정보 저장 및 출력\n",
    "        label_mapping[col] = mapping\n",
    "        print(f\"  원본 카테고리: {list(mapping.keys())}\")\n",
    "        print(f\"  변환 매핑: {mapping}\")\n",
    "\n",
    "    # Forecast 데이터의 범주형 변수 처리\n",
    "    for col in forecast_categorical:\n",
    "        # 이미 처리된 컬럼은 건너뛰기\n",
    "        if col in historical_label_encoders:\n",
    "            forecast_label_encoders[col] = historical_label_encoders[col]\n",
    "            continue\n",
    "        \n",
    "        print(f\"\\n범주형 특성 '{col}' 레이블 인코딩:\")\n",
    "        \n",
    "        if col == '미세먼지' or col == '초미세먼지':\n",
    "            # 미세먼지/초미세먼지 순서 정의 (좋음 > 보통 > 나쁨 > 매우나쁨)\n",
    "            ordered_categories = ['좋음', '보통', '나쁨', '매우나쁨']\n",
    "            \n",
    "            # 데이터에 있는 추가 카테고리 확인 (예: nan)\n",
    "            data_categories = set(forecast_df[col].astype(str).unique())\n",
    "            for category in data_categories:\n",
    "                if category not in ordered_categories and category != 'nan':\n",
    "                    ordered_categories.append(category)\n",
    "            \n",
    "            # nan 값이 있으면 마지막에 추가\n",
    "            if 'nan' in data_categories:\n",
    "                ordered_categories.append('nan')\n",
    "            \n",
    "            # 직접 매핑 생성\n",
    "            mapping = {cat: idx for idx, cat in enumerate(ordered_categories)}\n",
    "            \n",
    "            # 직접 변환 함수 생성\n",
    "            def transform_func(x, mapping=mapping):\n",
    "                return np.array([mapping.get(str(val), len(mapping) - 1) for val in x])\n",
    "            \n",
    "            # 변환 함수 저장\n",
    "            forecast_label_encoders[col] = transform_func\n",
    "            \n",
    "        elif col == 'sc_14' or col == 'sc_20':\n",
    "            # 하늘상태(sc) 순서 정의 (1=맑음, 2=구름조금, 3=구름많음, 4=흐림)\n",
    "            ordered_categories = ['1.0', '2.0', '3.0', '4.0']\n",
    "            \n",
    "            # 데이터에 있는 추가 카테고리 확인\n",
    "            data_categories = set(forecast_df[col].astype(str).unique())\n",
    "            for category in data_categories:\n",
    "                if category not in ordered_categories and category != 'nan':\n",
    "                    ordered_categories.append(category)\n",
    "            \n",
    "            # nan 값이 있으면 마지막에 추가\n",
    "            if 'nan' in data_categories:\n",
    "                ordered_categories.append('nan')\n",
    "            \n",
    "            # 직접 매핑 생성\n",
    "            mapping = {cat: idx for idx, cat in enumerate(ordered_categories)}\n",
    "            \n",
    "            # 직접 변환 함수 생성\n",
    "            def transform_func(x, mapping=mapping):\n",
    "                return np.array([mapping.get(str(val), len(mapping) - 1) for val in x])\n",
    "            \n",
    "            # 변환 함수 저장\n",
    "            forecast_label_encoders[col] = transform_func\n",
    "            \n",
    "        else:\n",
    "            # 기존 방식 사용 (다른 범주형 변수)\n",
    "            encoder = LabelEncoder()\n",
    "            values = forecast_df[col].astype(str).unique()\n",
    "            encoder.fit(values)\n",
    "            forecast_label_encoders[col] = encoder\n",
    "            \n",
    "            # 기존 매핑 정보 저장\n",
    "            mapping = {val: int(idx) for idx, val in enumerate(encoder.classes_)}\n",
    "        \n",
    "        # 매핑 정보 저장 및 출력\n",
    "        label_mapping[col] = mapping\n",
    "        print(f\"  원본 카테고리: {list(mapping.keys())}\")\n",
    "        print(f\"  변환 매핑: {mapping}\")\n",
    "        \n",
    "    # 발전소별 데이터 처리 및 파생 피처 추가를 위한 준비\n",
    "    enhanced_historical_df_train = pd.DataFrame()\n",
    "    all_added_features = []\n",
    "    \n",
    "    # 발전소별 데이터 정규화 및 파생 피처 추가\n",
    "    print(\"\\n발전소별 발전량 데이터 정규화 및 파생 피처 추가:\")\n",
    "    unique_plants = np.unique(plants_2d)\n",
    "    for plant_id in unique_plants:\n",
    "        # 현재 발전소 데이터 추출\n",
    "        plant_mask = plants_2d == plant_id\n",
    "        plant_data = historical_df_train[plant_mask].copy()\n",
    "        \n",
    "        # 발전소 메타데이터 가져오기\n",
    "        plant_meta = plant_meta_dict.get(plant_id, None)\n",
    "        \n",
    "        # 발전량 데이터 정규화 및 파생 피처 추가\n",
    "        normalized_plant_data, added_features = normalize_plant_output(\n",
    "            plant_data, plant_meta, plant_id, historical_plant_output\n",
    "        )\n",
    "        \n",
    "        # 모든 파생 피처 목록 업데이트\n",
    "        for feature in added_features:\n",
    "            if feature not in all_added_features:\n",
    "                all_added_features.append(feature)\n",
    "        \n",
    "        # 결과 합치기\n",
    "        enhanced_historical_df_train = pd.concat([enhanced_historical_df_train, normalized_plant_data])\n",
    "    \n",
    "    # 데이터 정렬 (원래 순서로)\n",
    "    enhanced_historical_df_train = enhanced_historical_df_train.sort_index()\n",
    "    \n",
    "    \n",
    "    # 전처리 파이프라인 학습 (패딩 제외한 실제 데이터로만)\n",
    "    print(\"\\nHistorical preprocessor fitting (excluding padding)...\")\n",
    "    historical_preprocessor.fit(enhanced_historical_df_train[[col for col in enhanced_historical_df_train.columns \n",
    "                                                             if col not in all_added_features]])\n",
    "    \n",
    "    print(\"Forecast preprocessor fitting...\")\n",
    "    forecast_preprocessor.fit(forecast_df_train)\n",
    "    \n",
    "    # 정규화된 특성 이름 가져오기\n",
    "    historical_feature_names = get_feature_names(\n",
    "        historical_preprocessor, \n",
    "        historical_numerical, \n",
    "        historical_categorical, \n",
    "        historical_cyclical, \n",
    "        historical_special,\n",
    "        historical_plant_output,\n",
    "        all_added_features\n",
    "    )\n",
    "    \n",
    "    forecast_feature_names = get_feature_names(\n",
    "        forecast_preprocessor, \n",
    "        forecast_numerical, \n",
    "        forecast_categorical, \n",
    "        forecast_cyclical, \n",
    "        forecast_special,\n",
    "        forecast_plant_output\n",
    "    )\n",
    "    \n",
    "    # 변환된 특성 정보 저장\n",
    "    feature_info = {\n",
    "        'historical_feature_names': historical_feature_names,\n",
    "        'forecast_feature_names': forecast_feature_names,\n",
    "        'meta_feature_names': meta_feature_names,\n",
    "        'historical_cols': historical_cols,\n",
    "        'forecast_cols': forecast_cols,\n",
    "        'meta_cols': meta_cols,\n",
    "        'historical_numerical': historical_numerical,\n",
    "        'historical_categorical': historical_categorical,\n",
    "        'historical_cyclical': historical_cyclical,\n",
    "        'historical_special': historical_special,\n",
    "        'historical_plant_output': historical_plant_output,\n",
    "        'forecast_numerical': forecast_numerical,\n",
    "        'forecast_categorical': forecast_categorical,\n",
    "        'forecast_cyclical': forecast_cyclical,\n",
    "        'forecast_special': forecast_special,\n",
    "        'forecast_plant_output': forecast_plant_output,\n",
    "        'added_features': all_added_features,\n",
    "        'label_mapping': label_mapping,\n",
    "        'padding_info': {\n",
    "                    'cutoff_hour': cutoff_hour,\n",
    "                    'padding_size': padding_size,\n",
    "                    'padding_start_idx': padding_start_idx,\n",
    "                    'history_days': history_days\n",
    "                }\n",
    "    }\n",
    "\n",
    "    # 컬럼 순서 정보 추가\n",
    "    column_order_info = {\n",
    "        'historical_feature_order': historical_feature_names,\n",
    "        'forecast_feature_order': forecast_feature_names,\n",
    "        'meta_feature_order': meta_feature_names\n",
    "    }\n",
    "\n",
    "    # 기존 feature_info 딕셔너리에 column_order_info 추가\n",
    "    feature_info.update(column_order_info)    \n",
    "            \n",
    "    # 변환된 특성 정보를 JSON 파일로 저장\n",
    "    with open(os.path.join(output_scenario_dir, 'feature_info.json'), 'w') as f:\n",
    "        json.dump(feature_info, f, indent=2)\n",
    "    \n",
    "    print(f\"변환된 특성 수:\")\n",
    "    print(f\"  Historical: {len(historical_feature_names)}\")\n",
    "    print(f\"  Forecast: {len(forecast_feature_names)}\")\n",
    "    print(f\"  Meta: {len(meta_feature_names)}\")\n",
    "    \n",
    "    # 각 분할 데이터 세트 처리\n",
    "    for split_name in split_names:\n",
    "        print(f\"\\nProcessing {split_name} data...\")\n",
    "        \n",
    "        # 데이터 로드\n",
    "        split_dir = os.path.join(scenario_dir, split_name)\n",
    "        X_historical = np.load(os.path.join(split_dir, 'X_historical.npy'), allow_pickle=True)\n",
    "        X_forecast = np.load(os.path.join(split_dir, 'X_forecast.npy'), allow_pickle=True)\n",
    "        y = np.load(os.path.join(split_dir, 'y.npy'), allow_pickle=True)\n",
    "        dates = np.load(os.path.join(split_dir, 'dates.npy'), allow_pickle=True)\n",
    "        plants = np.load(os.path.join(split_dir, 'plants.npy'), allow_pickle=True)\n",
    "        \n",
    "        # 메타데이터 로드 (있는 경우)\n",
    "        X_meta = None\n",
    "        try:\n",
    "            X_meta = np.load(os.path.join(split_dir, 'X_meta.npy'), allow_pickle=True)\n",
    "        except:\n",
    "            print(f\"  {split_name} 세트에 대한 메타데이터 파일 없음\")\n",
    "        \n",
    "        # 데이터 형태 확인\n",
    "        print(f\"  {split_name} shapes:\")\n",
    "        print(f\"    X_historical: {X_historical.shape}\")\n",
    "        print(f\"    X_forecast: {X_forecast.shape}\")\n",
    "        print(f\"    y: {y.shape}\")\n",
    "        if X_meta is not None:\n",
    "            print(f\"    X_meta: {X_meta.shape}\")\n",
    "        \n",
    "        # 3D 데이터를 2D로 변환 (시간 차원 펼치기)\n",
    "        n_samples, n_timesteps_hist, n_features_hist = X_historical.shape\n",
    "        n_samples, n_timesteps_forecast, n_features_forecast = X_forecast.shape\n",
    "        \n",
    "        # 패딩 마스크 생성\n",
    "        mask_hist = np.ones((n_samples, n_timesteps_hist), dtype=bool)\n",
    "        mask_hist[:, padding_start_idx:] = False\n",
    "        \n",
    "        # 결과 저장을 위한 정규화된 데이터 배열 초기화\n",
    "        # 기존 특성 + 추가된 파생 피처 수를 고려한 크기\n",
    "        historical_output_dim = len(historical_feature_names)\n",
    "        X_historical_normalized = np.full((n_samples, n_timesteps_hist, historical_output_dim), -1.0, dtype=np.float32)\n",
    "        \n",
    "        # 각 샘플별로 처리\n",
    "        for i in range(n_samples):\n",
    "            # 현재 발전소 이름\n",
    "            plant_id = plants[i]\n",
    "            \n",
    "            # 현재 발전소 메타데이터\n",
    "            plant_meta = plant_meta_dict.get(plant_id, None)\n",
    "            \n",
    "            # 현재 샘플에서 패딩이 아닌 실제 데이터 추출\n",
    "            real_data_indices = np.where(mask_hist[i])[0]\n",
    "            real_data = X_historical[i, real_data_indices]\n",
    "            \n",
    "            # 실제 데이터를 DataFrame으로 변환\n",
    "            real_df = pd.DataFrame(real_data, columns=historical_cols)\n",
    "            \n",
    "            # 범주형 변수 레이블 인코딩 적용\n",
    "            for col in historical_categorical:\n",
    "                # NaN 값 처리 (문자열로 변환)\n",
    "                real_df[col] = real_df[col].astype(str)\n",
    "                \n",
    "                # 레이블 인코더 적용\n",
    "                encoder = historical_label_encoders[col]\n",
    "                \n",
    "                if isinstance(encoder, LabelEncoder):\n",
    "                    # 기존 LabelEncoder 방식\n",
    "                    # 알 수 없는 범주 처리 (학습 데이터에 없는 값)\n",
    "                    for val in real_df[col].unique():\n",
    "                        if val not in encoder.classes_:\n",
    "                            print(f\"    경고: '{col}'에서 학습 데이터에 없는 값 '{val}' 발견됨. '0'으로 대체\")\n",
    "                            real_df.loc[real_df[col] == val, col] = encoder.classes_[0]\n",
    "                    \n",
    "                    real_df[col] = encoder.transform(real_df[col])\n",
    "                else:\n",
    "                    # 직접 생성한 변환 함수 사용\n",
    "                    real_df[col] = encoder(real_df[col])\n",
    "            \n",
    "            # 발전량 데이터 정규화 및 파생 피처 추가\n",
    "            normalized_real_df, _ = normalize_plant_output(\n",
    "                real_df, plant_meta, plant_id, historical_plant_output\n",
    "            )\n",
    "            \n",
    "            # 누락된 파생 피처 추가 (모든 발전소에 동일한 피처 세트 유지)\n",
    "            for feature in all_added_features:\n",
    "                if feature not in normalized_real_df.columns:\n",
    "                    normalized_real_df[feature] = 0.0\n",
    "            \n",
    "            # 기본 피처에 대한 전처리 파이프라인 적용\n",
    "            preprocessor_input = normalized_real_df[[col for col in normalized_real_df.columns \n",
    "                                                    if col not in all_added_features]]\n",
    "            preprocessed_data = historical_preprocessor.transform(preprocessor_input)\n",
    "            \n",
    "            # 추가 파생 피처 데이터 추출\n",
    "            additional_data = normalized_real_df[all_added_features].values\n",
    "            \n",
    "            # 전처리된 데이터와 추가 파생 피처 결합\n",
    "            combined_data = np.hstack([preprocessed_data, additional_data])\n",
    "            combined_data = combined_data.astype(np.float32) \n",
    "            \n",
    "            # 결합된 데이터 형태 확인 및 필요시 조정\n",
    "            if combined_data.shape[1] != historical_output_dim:\n",
    "                print(f\"    경고: 데이터 차원 불일치 - 예상: {historical_output_dim}, 실제: {combined_data.shape[1]}\")\n",
    "                # 필요시 차원 조정 (예: 0으로 패딩)\n",
    "                if combined_data.shape[1] < historical_output_dim:\n",
    "                    padding_dim = historical_output_dim - combined_data.shape[1]\n",
    "                    combined_data = np.hstack([combined_data, np.zeros((combined_data.shape[0], padding_dim))])\n",
    "                else:\n",
    "                    combined_data = combined_data[:, :historical_output_dim]\n",
    "\n",
    "            # 정규화된 데이터를 원래 위치에 삽입\n",
    "            X_historical_normalized[i, real_data_indices] = combined_data\n",
    "\n",
    "\n",
    "        \n",
    "        # 예보 데이터 처리 (패딩 없음)\n",
    "        X_forecast_2d = X_forecast.reshape(n_samples * n_timesteps_forecast, n_features_forecast)\n",
    "        forecast_df = pd.DataFrame(X_forecast_2d, columns=forecast_cols)\n",
    "        \n",
    "        # 범주형 변수 레이블 인코딩 적용\n",
    "        for col in forecast_categorical:\n",
    "            # NaN 값 처리 (문자열로 변환)\n",
    "            forecast_df[col] = forecast_df[col].astype(str)\n",
    "            \n",
    "            # 레이블 인코더 적용\n",
    "            encoder = forecast_label_encoders[col]\n",
    "            \n",
    "            if isinstance(encoder, LabelEncoder):\n",
    "                # 기존 LabelEncoder 방식\n",
    "                # 알 수 없는 범주 처리 (학습 데이터에 없는 값)\n",
    "                for val in forecast_df[col].unique():\n",
    "                    if val not in encoder.classes_:\n",
    "                        print(f\"    경고: '{col}'에서 학습 데이터에 없는 값 '{val}' 발견됨. '0'으로 대체\")\n",
    "                        forecast_df.loc[forecast_df[col] == val, col] = encoder.classes_[0]\n",
    "                \n",
    "                forecast_df[col] = encoder.transform(forecast_df[col])\n",
    "            else:\n",
    "                # 직접 생성한 변환 함수 사용 (알 수 없는 값도 함수 내에서 처리됨)\n",
    "                forecast_df[col] = encoder(forecast_df[col])\n",
    "                \n",
    "        X_forecast_transformed = forecast_preprocessor.transform(forecast_df)\n",
    "        X_forecast_normalized = X_forecast_transformed.reshape(n_samples, n_timesteps_forecast, -1)\n",
    "        \n",
    "        # 메타데이터 정규화 (있는 경우)\n",
    "        # ✅ 학습된 전처리기로 메타데이터 변환 (지역은 모든 split 기반, 수치형은 train 기반)\n",
    "        X_meta_normalized = apply_meta_preprocessing(\n",
    "            X_meta, meta_cols, region_encoder, numeric_scaler\n",
    "        )\n",
    "        \n",
    "        \n",
    "        # 정규화된 데이터 형태 출력\n",
    "        print(f\"    Normalized X_historical: {X_historical_normalized.shape}\")\n",
    "        print(f\"    Normalized X_forecast: {X_forecast_normalized.shape}\")\n",
    "        if X_meta_normalized is not None:\n",
    "            print(f\"    Normalized X_meta: {X_meta_normalized.shape}\")\n",
    "\n",
    "        # Y값 정규화 적용 (X와 동일한 방식으로 개별 처리)\n",
    "        y_normalized = np.zeros_like(y, dtype=np.float32)\n",
    "        for i in range(n_samples):\n",
    "            plant_id = plants[i]\n",
    "\n",
    "            # 현재 발전소 메타 데이터\n",
    "            plant_meta = plant_meta_dict.get(plant_id, None)\n",
    "            y_normalized[i], _, _ = normalize_single_target_value(\n",
    "                y[i], plant_id, plant_meta\n",
    "            )            \n",
    "        \n",
    "        # 출력 디렉토리 생성\n",
    "        output_split_dir = os.path.join(output_scenario_dir, split_name)\n",
    "        os.makedirs(output_split_dir, exist_ok=True)\n",
    "        \n",
    "        # 정규화된 데이터와 패딩 마스크 저장\n",
    "        np.save(os.path.join(output_split_dir, 'X_historical.npy'), X_historical_normalized)\n",
    "        np.save(os.path.join(output_split_dir, 'X_forecast.npy'), X_forecast_normalized)\n",
    "        np.save(os.path.join(output_split_dir, 'padding_mask.npy'), mask_hist)\n",
    "        #np.save(os.path.join(output_split_dir, 'y.npy'), y)\n",
    "        np.save(os.path.join(output_split_dir, 'y.npy'), y_normalized)  # 정규화된 Y 저장\n",
    "        np.save(os.path.join(output_split_dir, 'y_original.npy'), y)   # 원본 Y도 저장        \n",
    "        np.save(os.path.join(output_split_dir, 'dates.npy'), dates)\n",
    "        np.save(os.path.join(output_split_dir, 'plants.npy'), plants)\n",
    "        \n",
    "        # 메타데이터 저장 (있는 경우)\n",
    "        if X_meta_normalized is not None:\n",
    "            np.save(os.path.join(output_split_dir, 'X_meta.npy'), X_meta_normalized)\n",
    "    \n",
    "    # 전처리 파이프라인 및 인코더 저장\n",
    "    import pickle\n",
    "    \n",
    "    # 전처리 파이프라인 저장 디렉토리 생성\n",
    "    models_dir = os.path.join(output_scenario_dir, 'preprocessors')\n",
    "    os.makedirs(models_dir, exist_ok=True)\n",
    "    \n",
    "    # 전처리 파이프라인 저장\n",
    "    with open(os.path.join(models_dir, 'historical_preprocessor.pkl'), 'wb') as f:\n",
    "        pickle.dump(historical_preprocessor, f)\n",
    "    \n",
    "    with open(os.path.join(models_dir, 'forecast_preprocessor.pkl'), 'wb') as f:\n",
    "        pickle.dump(forecast_preprocessor, f)\n",
    "\n",
    "    # 지역 인코더 저장 (모든 split 기반)\n",
    "    if region_encoder is not None:\n",
    "        with open(os.path.join(models_dir, 'region_encoder.pkl'), 'wb') as f:\n",
    "            pickle.dump(region_encoder, f)\n",
    "        print(f\"지역 인코더 저장 완료 - 총 {len(all_regions)}개 지역\")\n",
    "    \n",
    "    # 수치형 스케일러 저장 (train 기반)\n",
    "    if numeric_scaler is not None:\n",
    "        with open(os.path.join(models_dir, 'numeric_meta_scaler.pkl'), 'wb') as f:\n",
    "            pickle.dump(numeric_scaler, f)\n",
    "        print(\"수치형 메타데이터 스케일러 저장 완료\")        \n",
    "    \n",
    "    # 레이블 인코더 저장\n",
    "    #with open(os.path.join(models_dir, 'historical_label_encoders.pkl'), 'wb') as f:\n",
    "    #    pickle.dump(historical_label_encoders, f)\n",
    "    \n",
    "    #with open(os.path.join(models_dir, 'forecast_label_encoders.pkl'), 'wb') as f:\n",
    "    #    pickle.dump(forecast_label_encoders, f)\n",
    "    \n",
    "    # 지역 인코더 저장 (있는 경우)\n",
    "    if region_encoder is not None:\n",
    "        with open(os.path.join(models_dir, 'region_encoder.pkl'), 'wb') as f:\n",
    "            pickle.dump(region_encoder, f)\n",
    "    \n",
    "    print(f\"{scenario} 시나리오 데이터 정규화 완료!\")\n",
    "\n",
    "\n",
    "# 경로 설정\n",
    "input_dir = '../data/forecast_modeling_data'\n",
    "output_dir = '../data/normalized_forecast_modeling_data'\n",
    "\n",
    "# 출력 디렉토리 생성\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# 시나리오별 데이터 정규화\n",
    "for scenario in ['14시','20시']: # '20시'\n",
    "    normalize_data(input_dir, output_dir, scenario)\n",
    "\n",
    "print(\"모든 데이터 정규화 완료!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "8db6246d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2017, 48, 54)\n",
      "[-1.232 -0.848 -0.12  -0.476 -0.567 -1.067  1.267  1.687 -0.743 -0.514 -0.473  0.016 -0.333  0.3   -1.3    0.043  0.45\n",
      " -1.402  2.125  0.259  0.265 -0.874  1.356 -1.259 -0.835 -0.908  0.257  1.     1.     2.     0.     1.    -0.417  0.909\n",
      " -0.     1.     1.     0.     0.    -0.704 -0.704 -0.699  0.    -0.699 -0.503 -0.43   2.153  0.09   0.425  0.     0.425\n",
      "  0.     0.     0.   ]\n"
     ]
    }
   ],
   "source": [
    "a = np.load('/Users/soomin/Desktop/공공데이터/renewable-power-prediction/data/normalized_forecast_modeling_data/14시/external_test/X_historical.npy'    , allow_pickle=True )\n",
    "print(a.shape)\n",
    "\n",
    "np.set_printoptions(precision=3, suppress=True, linewidth=120)\n",
    "print(a[0, 0, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "1735eec3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.]\n"
     ]
    }
   ],
   "source": [
    "print(a[0, 46, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "4308e71d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2017, 48, 46)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([637.6, 26.567, 125.7, 0.0, 125.7, 0.1, 0.0, 1.3, 48.0, 0.0, 1.2, 90.0, 3.2, 1021.3, 1030.1, 0.0, 0.0, 3.0, 3.0,\n",
       "       18.0, 1962.0, -0.9, 0.004, 0.5, 0.007, 0.0411, 43.0, 24.0, '보통', '보통', 0.0, 1.0, -0.41719360261231697,\n",
       "       0.9088176373395028, -2.4492935982947064e-16, 1.0, 0, 0.0, 17.25, 1.0, 6.123233995736766e-17, 2.0, 108.0, '3.0',\n",
       "       1.0, 23.0], dtype=object)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.load('/Users/soomin/Desktop/공공데이터/renewable-power-prediction/data/forecast_modeling_data/14시/external_test/X_historical.npy',allow_pickle=True)\n",
    "print(a.shape)\n",
    "a[0,0,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "377e1407",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2017, 3)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(['296.09999999999997', '5.0', '인천광역시'], dtype='<U32')"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.load('/Users/soomin/Desktop/공공데이터/renewable-power-prediction/data/forecast_modeling_data/14시/external_test/X_meta.npy',allow_pickle=True)\n",
    "print(a.shape)\n",
    "a[0,]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "60e61c31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2017, 10)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-0.727, -1.231,  0.   ,  0.   ,  0.   ,  0.   ,  0.   ,  1.   ,  0.   ,  0.   ])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.load('/Users/soomin/Desktop/공공데이터/renewable-power-prediction/data/normalized_forecast_modeling_data/14시/external_test/X_meta.npy',allow_pickle=True)\n",
    "print(a.shape)\n",
    "a[0,]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "b8dca40e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2017, 24)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.002, 0.019, 0.105, 0.12 , 0.131, 0.081, 0.114, 0.138,\n",
       "       0.093, 0.019, 0.   , 0.   , 0.   , 0.   , 0.   , 0.   ], dtype=float32)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "a = np.load('/Users/soomin/Desktop/공공데이터/renewable-power-prediction/data/normalized_forecast_modeling_data/14시/external_test/y.npy',allow_pickle=True)\n",
    "print(a.shape)\n",
    "a[450]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "53449ae9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2017, 24)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.24,  2.16, 12.12, 13.8 , 15.12,  9.36, 13.08, 15.84,\n",
       "       10.68,  2.16,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.load('/Users/soomin/Desktop/공공데이터/renewable-power-prediction/data/normalized_forecast_modeling_data/14시/external_test/y_original.npy',allow_pickle=True)\n",
    "print(a.shape)\n",
    "a[450]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
